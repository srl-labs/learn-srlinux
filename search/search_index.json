{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"alwayson/","text":"<p>It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab.</p> <p>But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out.</p>","title":"Always-ON SR Linux Instance"},{"location":"alwayson/#what-is-always-on-sr-linux-for","text":"<p>The Always-ON SR Linux instance is an Internet reachable SR Linux container running in the cloud. Although running in the read-only mode, the Always-ON instance can unlock some interesting use cases, which won't require anything but Internet connection from a curious user.</p> <ul> <li> <p>getting to know SR Linux CLI     SR Linux offers a modern, extensible CLI with unique features aimed to make Ops teams life easier.     New users can make their first steps by looking at the <code>show</code> commands, exploring the datastores, running <code>info from</code> commands and getting the grips of configuration basics by entering into the configuration mode.</p> </li> <li> <p>YANG browsing     By being a YANG-first Network OS, SR Linux is fully modelled with YANG. This means that by traversing the CLI users are inherently investigating the underlying YANG models that serve the base for all the programmable interfaces SR Linux offers.</p> </li> <li> <p>gNMI exploration     The de-facto king of the Streaming Telemetry - gNMI - is one of the programmable interfaces of SR Linux.     gNMI is enabled on the Always-ON instance, so anyone can stream the data out of the SR Linux and see how it works for themselves.</p> </li> </ul>","title":"What is Always-ON SR Linux for?"},{"location":"alwayson/#connection-details","text":"<p>Always-ON SR Linux instance comes up with the SSH and gNMI management interfaces exposed. The following table summarizes the connection details for each of those interfaces:</p>    Method Details     SSH address: <code>ssh guest@on.srlinux.dev -p 44268</code>password: <code>n0k1asrlinux</code>for key-based authentication use this key to authenticate the <code>guest</code> user   gNMI1 <pre><code>gnmic -a on.srlinux.dev:39010 -u guest -p n0k1asrlinux --skip-verify \\      capabilities</code></pre>   JSON-RPC2 http://http.on.srlinux.dev","title":"Connection details"},{"location":"alwayson/#gnmi","text":"<p>SR Linux runs a TLS-enabled gNMI server with a certificate already present on the system. The users of the gNMI interface can either skip verification of the node certificate, or they can use this CA.pem file to authenticate the node's TLS certificate.</p>","title":"gNMI"},{"location":"alwayson/#guest-user","text":"<p>The <code>guest</code> user has the following settings applied to it:</p> <ol> <li>Read-only mode</li> <li><code>bash</code> and <code>file</code> commands are disabled</li> </ol> <p>Although the read-only mode is enforced, the guest user can still enter in the configuration mode and perform configuration actions, it is just that <code>guest</code> can't commit them.</p>","title":"Guest user"},{"location":"alwayson/#always-on-sandbox-setup","text":"<p>The Always-ON sandbox consists of SR Linux node connected with a LAG interface towards an Nokia SR OS node.</p>","title":"Always-ON sandbox setup"},{"location":"alwayson/#protocols-and-services","text":"<p>We pre-created a few services on the SR Linux node so that you would see a \"real deal\" configuration- and state-wise.</p> <p>The underlay configuration consists of the two L3 links between the nodes with eBGP peering built on link addresses. The system/loopback interfaces are advertised via eBGP to enable overlay services.</p> <p>In the overlay the following services are configured:</p> <ol> <li>Layer 2 EVPN with VXLAN dataplane1 with <code>mac-vrf-100</code> network instance created on SR Linux</li> <li>Layer 3 EVPN with VXLAN dataplane with <code>ip-vrf-200</code> network instance created on SR Linux</li> </ol>   <ol> <li> <p>check this tutorial to understand how this service is configured\u00a0\u21a9\u21a9</p> </li> <li> <p>HTTP service running over port 80\u00a0\u21a9</p> </li> </ol>","title":"Protocols and Services"},{"location":"community/","text":"","title":"Community"},{"location":"community/#discord-server","text":"<p>SR Linux has lots to offer to various groups of engineers...</p> <p>Those with a strong networking background will find themselves at home with proven routing stack SR Linux inherited from Nokia SR OS.</p> <p>Automation engineers will appreciate the vast automation and programmability options thanks to SR Linux NetOps Development Kit and customizable CLI.</p> <p>Monitoring-obsessed networkers would be pleased with SR Linux 100% YANG coverage and thus through-and-through gNMI-based telemetry support.</p> <p>We are happy to chat with you all! And the chosen venue for our new-forming SR Linux Community1 is the SR Linux Discord Server which everyone can join!</p> <p> Join SR Linux Discord Server</p>","title":"Discord server"},{"location":"community/#always-on-sr-linux","text":"<p>It is extremely easy and hassle free to run SR Linux, thanks to the public container image and topology builder tool - containerlab.</p> <p>But wouldn't it be nice to have an SR Linux instance running in the cloud open for everyone to tinker with? We think it would, so we created an Always-ON SR Linux instance that we invite you to try out.</p>   <ol> <li> <p>this is an unofficial community. Engineers to engineers.\u00a0\u21a9</p> </li> </ol>","title":"Always-ON SR Linux"},{"location":"get-started/","text":"<p>SR Linux packs a lot of unique features that the data center networking teams can leverage. Some of the features being truly new to the networking domain. The goal of this portal is to introduce SR Linux to the visitors through the interactive tutorials centered around SR Linux services and capabilities.</p> <p>We believe that learning by doing yields the best results. With that in mind we made SR Linux container image available to everybody without any registration or licensing requirements </p> <p>The public SR Linux container image when powered by containerlab allows us to create easily deployable labs that everyone can launch at their convenience. All that to let you not only read about the features we offer, but to try them live!</p>","title":"Get SR Linux"},{"location":"get-started/#sr-linux-container-image","text":"<p>A single container image that hosts management, control and data plane functions is all you need to get started.</p>","title":"SR Linux container image"},{"location":"get-started/#getting-the-image","text":"<p>To make our SR Linux image available to everyone, we pushed it to a publicly accessible GitHub container registry. This means that you can pull SR Linux container image exactly the same way as you would pull any other image:</p> <pre><code>docker pull ghcr.io/nokia/srlinux\n</code></pre> <p>When image is referenced without a tag, the latest container image version will be pulled. To obtain a specific version of a containerized SR Linux, refer to the list of tags the <code>nokia/srlinux</code> image has and change the <code>docker pull</code> command accordingly.</p>","title":"Getting the image"},{"location":"get-started/#running-sr-linux","text":"<p>When the image is pulled to a local image store, you can start exploring SR Linux by either running a full-fledged lab topology, or by starting a single container to explore SR Linux CLI and its management interfaces.</p> <p>A system on which you can run SR Linux containers should conform to the following requirements:</p> <ol> <li>Linux OS with a kernel v4+1.</li> <li>Docker container runtime.</li> <li>At least 2 vCPU and 4GB RAM.</li> <li>A user with administrative privileges.</li> </ol> <p>Let's explore the different ways you can launch SR Linux container.</p>","title":"Running SR Linux"},{"location":"get-started/#docker-cli","text":"<p><code>docker</code> CLI offers a quick way to run a standalone SR Linux container:</p> Docker runTopology file   <p>Note the presence of topology file mount in the <code>docker run</code> command, it is used to drive a selection of the emulated chassis type. <pre><code>docker run -t -d --rm --privileged \\\n  -v $(pwd)/topology.yml:/tmp/topology.yml \\\n  -u $(id -u):$(id -g) \\\n  --name srlinux ghcr.io/nokia/srlinux \\\n  sudo bash /opt/srlinux/bin/sr_linux\n</code></pre></p>   <p>The following topology file is for IXR-D3L chassis.</p> <pre><code>chassis_configuration:\n  \"chassis_type\": 72\n  \"base_mac\": \"1a:b0:00:00:00:00\"\n  \"cpm_card_type\": 187\n\nslot_configuration:\n  1:\n    \"card_type\": 187\n    \"mda_type\": 200\n</code></pre>    <p>The above command will start the container named <code>srlinux</code> on the host system with a single management interface attached to the default docker network.</p>  <p>Warning</p> <p>To get SSH access for a deployed container, make sure to disable TX Offload on a default docker bridge, otherwise, CRC checksums will be fake and SR Linux will discard those packets.</p> <pre><code>sudo ethtool --offload docker0 tx off\n</code></pre>  <p>This approach is viable when all you need is to run a standalone container to explore SR Linux CLI or to interrogate its management interfaces. But it is not particularly suitable to run multiple SR Linux containers with links between them, as this requires some extra work.</p> <p>For multi-node SR Linux deployments containerlab3 offers a better way.</p>","title":"Docker CLI"},{"location":"get-started/#containerlab","text":"<p>Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them and manages labs lifecycle.</p> <p>A quickstart guide is a perfect place to get started with containerlab. For the sake of completeness, let's have a look at the containerlab file that defines a lab with two SR Linux nodes connected back to back together:</p> <pre><code># file: srlinux.clab.yml\nname: srlinux\n\ntopology:\n  nodes:\n    srl1:\n      kind: srl\n      image: ghcr.io/nokia/srlinux\n    srl2:\n      kind: srl\n      image: ghcr.io/nokia/srlinux\n\n  links:\n    - endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n</code></pre> <p>By copying this file over to your system you can immediately deploy it with containerlab:</p> <p><pre><code>containerlab deploy -t srlinux.clab.yml\n</code></pre> <pre><code>INFO[0000] Parsing &amp; checking topology file: srlinux.clab.yml \nINFO[0000] Creating lab directory: /root/demo/clab-srlinux \nINFO[0000] Creating container: srl1                     \nINFO[0000] Creating container: srl2                     \nINFO[0001] Creating virtual wire: srl1:e1-1 &lt;--&gt; srl2:e1-1 \nINFO[0001] Writing /etc/hosts file                      \n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| # |        Name        | Container ID |         Image         | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srlinux-srl1  | 50826b3e3703 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n| 2 | clab-srlinux-srl2  | 4d4494aba320 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n+---+--------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n</code></pre></p>","title":"Containerlab"},{"location":"get-started/#deployment-verification","text":"<p>Regardless of the way you spin up SR Linux container it will be visible in the output of the <code>docker ps</code> command. If the deployment process went well and the container did not exit, a user can see it with <code>docker ps</code> command:</p> <pre><code>$ docker ps\nCONTAINER ID   IMAGE                      COMMAND                  CREATED             STATUS             PORTS                    NAMES\n4d4494aba320   ghcr.io/nokia/srlinux      \"/tini -- fixuid -q \u2026\"   32 minutes ago      Up 32 minutes                               clab-learn-01-srl2\n</code></pre> <p>The logs of the running container can be displayed with <code>docker logs &lt;container-name&gt;</code>.</p> <p>In case of the misconfiguration or runtime errors, container may exit abruptly. In that case it won't appear in the <code>docker ps</code> output as this command only shows running containers. Containers which are in the exited status will be part of the <code>docker ps -a</code> output. In case your container exits abruptly, check the logs as they typically reveal the cause of termination.</p>","title":"Deployment verification"},{"location":"get-started/#connecting-to-sr-linux","text":"<p>When SR Linux container is up and running, users can connect to it over different interfaces.</p>","title":"Connecting to SR Linux"},{"location":"get-started/#cli","text":"<p>One of the ways to manage SR Linux is via its advanced and extensible Command Line Interface.</p> <p>To invoke the CLI application inside the SR Linux container get container name/ID first, and then execute the <code>sr_cli</code> process inside of it:</p> <p><pre><code># get SR Linux container name -&gt; clab-srl01-srl\n$ docker ps\nCONTAINER ID   IMAGE                             COMMAND                  CREATED          STATUS         PORTS                    NAMES\n17a47c58ad59   ghcr.io/nokia/srlinux             \"/tini -- fixuid -q \u2026\"   10 seconds ago   Up 6 seconds                            clab-learn-01-srl1\n</code></pre> <pre><code># start the sr_cli process inside this container to get access to CLI\ndocker exec -it clab-learn-01-srl1 sr_cli\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                                                                                                           \nA:srl1#\n</code></pre></p> <p>The CLI can also be accessed via an SSH service the SR Linux container runs. Using the default credentials <code>admin:admin</code> you can connect to the CLI over the network:</p> <pre><code># containerlab creates local /etc/hosts entries\n# for container names to resolve to their IP\nssh admin@clab-learn-01-srl1\n\nadmin@clab-learn-01-srl1's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                                                                                                           \nA:srl1#\n</code></pre>","title":"CLI"},{"location":"get-started/#gnmi","text":"<p>SR Linux containers deployed with containerlab come up with gNMI interface up and running over port 57400.</p> <p>Using the gNMI client2 users can explore SR Linux' gNMI interface:</p> <pre><code>gnmic -a clab-srlinux-srl1 --skip-verify -u admin -p admin capabilities\ngNMI version: 0.7.0\nsupported models:\n  - urn:srl_nokia/aaa:srl_nokia-aaa, Nokia, 2021-03-31\n  - urn:srl_nokia/aaa-types:srl_nokia-aaa-types, Nokia, 2019-11-30\n  - urn:srl_nokia/acl:srl_nokia-acl, Nokia, 2021-03-31\n&lt;SNIP&gt;\n</code></pre>   <ol> <li> <p>Centos 7.3+ although having a 3.x kernel is still capable of running SR Linux container\u00a0\u21a9</p> </li> <li> <p>for example gnmic \u21a9</p> </li> <li> <p>The labs referenced on this site are deployed with containerlab unless stated otherwise\u00a0\u21a9</p> </li> </ol>","title":"gNMI"},{"location":"kb/cfgmgmt/","text":"<p>SR Linux employs a transaction-based configuration management system. That allows for a number of changes to be made to the configuration with an explicit <code>commit</code> required to apply the changes as a single transaction.</p>","title":"Configuration management"},{"location":"kb/cfgmgmt/#configuration-file","text":"<p>The default location for the configuration file is <code>/etc/opt/srlinux/config.json</code>.</p> <p>If there is no configuration file present, a basic configuration file is auto-generated with the following defaults:</p> <ul> <li>Creation of a management network instance</li> <li>Management interface is added to the mgmt network instance</li> <li>DHCP v4/v6 is enabled on mgmt interface</li> <li>A set of default of logs are created</li> <li>SSH server is enabled</li> <li>Some default IPv4/v6 CPM filters</li> </ul>","title":"Configuration file"},{"location":"kb/cfgmgmt/#configuration-modes","text":"<p>Configuration modes define how the system is running when transactions are performed. Supported modes are the following:</p> <ul> <li>Running: the default mode when logging in and displays displays the currently running or active configuration.</li> <li>State: the running configuration plus the addition of any dynamically added data.  Some examples of state specific data are operational state of various elements, counters and statistics, BGP auto-discovered peer, LLDP peer information, etc.</li> <li>Candidate: this mode is used to modify configuration. Modifications are not applied until the <code>commit</code> is performed. When committed, the changes are copied to the running configuration and become active. The candidate configuration configuration can itself be edited in the following modes:<ul> <li>Shared: this is the default mode when entering the candidate mode with <code>enter candidate</code> command.  This allows multiple users to modify the candidate configuration concurrently. When the configuration is committed, the changes from all of the users are applied.</li> <li>Exclusive Candidate: When entering candidate mode with <code>enter candidate exclusive</code>, it locks out other users from making changes to the candidate configuration.   You can enter candidate exclusive mode only under the following conditions:  <ul> <li>The current shared candidate configuration has not been modified.</li> <li>There are no other users in candidate shared mode.</li> <li>No other users have entered candidate exclusive mode.</li> </ul> </li> <li>Private: A private candidate allows multiple users to modify a configuration; however when a user commits their changes, only the changes from that user are committed.   When a private candidate is created, private datastores are created and a snapshot is taken from the running database to create a baseline. When starting a private candidate, a default candidate is defined per user with the name <code>private-&lt;username&gt;</code> unless a unique name is defined.</li> </ul> </li> </ul>  <p>Note</p> <p>gNMI &amp; JSON-RPC both use an exclusive candidate and an implicit commit when making a configuration change on the device.</p>","title":"Configuration modes"},{"location":"kb/cfgmgmt/#setting-the-configuration-mode","text":"<p>After logging in to the CLI, you are initially placed in <code>running</code> mode. The following table provides commands to enter in a specific mode:</p>    Candidate mode Command to enter     Candidate shared <code>enter candidate</code>   Candidate mode for named shared candidate <code>enter candidate name &lt;name&gt;</code>   Candidate private <code>enter candidate private</code>   Candidate mode for named private candidate <code>enter candidate private name &lt;name&gt;</code>   Candidate exclusive <code>enter candidate exclusive</code>   Exclusive mode for named candidate <code>enter candidate exclusive name &lt;name&gt;</code>   Running <code>enter running</code>   State <code>enter state</code>   Show <code>enter show</code>","title":"Setting the configuration mode"},{"location":"kb/cfgmgmt/#committing-configuration","text":"<p>Changes made during a configuration modification session do not take effect until a <code>commit</code> command is issued. Several options are available for <code>commit</code> command, below are the most notable ones:</p>    Option Action     <code>commit now</code> Apply the changes, exit candidate mode, and enter running mode   <code>commit stay</code> Apply the changes and then remain in candidate mode   <code>commit save</code> Apply the changes and automatically save the commit to the startup configuration   <code>commit confirmed</code> Apply the changes, but requires an explicit confirmation to become permanent. If the explicit confirmation is not issued within a specified time period, all changes are automatically reverted","title":"Committing configuration"},{"location":"kb/cfgmgmt/#deleting-configuration","text":"<p>Use the <code>delete</code> command to delete configurations while in candidate mode.</p> <p>The following example displays the system banner configuration, deletes the configured banner, then displays the resulting system banner configuration: <pre><code>--{ candidate shared default}--[ ]--\nA:leaf1# info system banner\n    system {\n        banner {\n            login-banner \"Welcome to SRLinux!\"\n        }\n    }\n\n--{ candidate shared default}--[ ]--\nA:leaf1# delete system banner\n\n--{ candidate shared default}--[ ]--\nA:leaf1# info system banner\n    system {\n        banner {\n        }\n    }\n</code></pre></p>","title":"Deleting configuration"},{"location":"kb/cfgmgmt/#discarding-configuration","text":"<p>You can discard previously applied configurations with the <code>discard</code> command.</p> <ul> <li>To discard the changes and remain in candidate mode with a new candidate session, enter <code>discard stay</code>.</li> <li>To discard the changes, exit candidate mode, and enter running mode, enter <code>discard now</code>.</li> </ul>","title":"Discarding configuration"},{"location":"kb/cfgmgmt/#displaying-configuration-diff","text":"<p>Use the <code>diff</code> command to get a comparison of configuration changes. Optional arguments can be used to indicate the source and destination datastore.</p> <p>The following use rules apply: * If no arguments are specified, the diff is performed from the candidate to the baseline of the candidate. * If a single argument is specified, the diff is performed from the current candidate to the specified candidate. * If two arguments are specified, the first is treated as the source, and the second as the destination.</p> <p>Global arguments include: <code>baseline</code>, <code>candidate</code>, <code>checkpoint</code>, <code>factory</code>, <code>file</code>, <code>from</code>, <code>rescue</code>, <code>running</code>, and <code>startup</code>.</p> <p>The diff command can be used outside of candidate mode, but only if used with arguments.</p> <p>The following shows a basic <code>diff</code> command without arguments. In this example, the description and admin state of an interface are changed and the differences shown: <pre><code>--{ candidate shared default }--[ ]--\n# interface ethernet-1/1 admin-state disable\n\n--{ * candidate shared default }--[ ]--\n# interface ethernet-1/2 description \"updated\"\n\n--{ * candidate shared default }--[ ]--\n# diff\n    interface ethernet-1/1 {\n+       admin-state disable\n    }\n+   interface ethernet-1/2 {\n+       description updated\n+   }\n</code></pre></p>","title":"Displaying configuration diff"},{"location":"kb/cfgmgmt/#displaying-configuration-details","text":"<p>Use the <code>info</code> command to display the configuration. Entering the info command from the root context displays the entire configuration, or the configuration for a specified context. Entering the command from within a context limits the display to the configuration under that context.</p> <p>To display the entire configuration, enter <code>info</code> from the root context: <pre><code>--{ candidate shared default}--[ ]--\n# info\n&lt;all the configuration is displayed&gt;\n--{ candidate }--[ ]--\n</code></pre></p> <p>To display the configuration for a specific context, enter info and specify the context: <pre><code>--{ candidate shared default}--[ ]--\n# info system lldp\n    system {\n        lldp {\n            admin-state enable\n            hello-timer 600\n            management-address mgmt0.0 {\n                type [\n                    IPv4\n                ]\n            }\n            interface mgmt0 {\n                admin-state disable\n            }\n        }\n    }\n</code></pre></p> <p>The following <code>info</code> command options are rather useful:</p> <ul> <li><code>as-json</code> - to display JSON-formatted output</li> <li><code>detail</code> - to display values for all parameters, including those not specifically configured</li> <li><code>flat</code> -  to display the output as a series of set statements, omitting indentation for any sub-contexts</li> </ul>","title":"Displaying configuration details"},{"location":"kb/event-handler/","text":"Official doc section Will be updated with 22.6.1 release     <p>Warning</p> <p>Always consult with the official documentation, as knowledge base articles provide only the gist of a certain feature/functionality.</p>  <p>Event-driven architecture is a software architecture paradigm promoting the production, detection, consumption of, and reaction to events. In network automation, an event-driven approach is often seen as a holy grail of closed-loop systems as a reaction to events makes the loop close in highly automated networks.</p> <p>Keeping ourselves in the network boundaries, we can define the following main components of an event-driven system:</p> <ol> <li>Event     Most commonly produced by the network element itself. A link goes down, session transitions to <code>oper-state=up</code> - are the events that the network element produces.</li> <li>Event transport system     Events sourced at a certain point in the network or a specific subsystem of a node need to be transferred to an Event processor for evaluation. This can be a local message bus or a management protocol (gNMI, etc).</li> <li>Event processor     A system that consumes the events and produces an output</li> </ol> <p>The event processor can be deployed centrally in the network, distributed over several nodes, or even run on each network element. Starting with the Nokia SR Linux 22.6.11 software release, a new application called \"Event Handler\" has been introduced to give users a way to make SR Linux react to local events.</p> <p>The event handling concept is being able to react to certain system events, with programmable logic on what actions to take as a result. Event Handler allows users to write custom Python1 scripts and has these scripts be called in the event state paths change the value, thus introducing programmable logic to handle events.</p> <p>The most common use case that leverages Event Handler capability is known as Oper Group, where specific ports are put operationally up/down based on the oper status of another group of ports. Of course, many other use cases can benefit from local events that can be programmatically handled whenever users wish to.</p> <p>Interactions between Event Handler, SR Linux' Management Server and user-provided script are outlined within the following sequence diagram and are explained in detail further.</p> <pre><code>sequenceDiagram\n  autonumber\n  participant M as Mgmt Server\n  participant EH as Event Handler\n  participant S as Script\n  EH-&gt;&gt;M: Subscribe to YANG paths\n  Note over M: State change occurs for any&lt;br/&gt;of the subscribed paths\n  M-&gt;&gt;EH: Current state for all subscribed paths\n  EH-&gt;&gt;S: Invoke a script with passing&lt;br/&gt; state changes as input parameter\n  S-&gt;&gt;S: Process input parameters\n  S-&gt;&gt;EH: Output as a list of actions\n  EH-&gt;&gt;EH: Process actions list\n  EH-&gt;&gt;M: Execute action(s)&lt;br/&gt;(or run another script)</code></pre>","title":"Event Handler"},{"location":"kb/event-handler/#configuration","text":"<p>Event Handler is supported in SR Linux by the <code>event_mgr</code> process which exposes configuration and state via a container at <code>.system.event-handler</code>. Within this container, a list of event handling instances can be configured at <code>.system.event-handler.instance{}</code> with a user-defined name.</p> <p>Here is the annotated config of <code>event-handler</code> container that highlights the most important knobs of this feature.</p> <p>An event handler instance config consists of:</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info\n    admin-state enable # (1)!\n    upython-script opergroup.py # (2)!\n    paths [\n        \"interface ethernet-1/55 oper-state\" # (3)!\n        \"interface ethernet-1/56 oper-state\"\n    ]\n    options { # (4)!\n        object down-links {\n            values [ # (5)!\n                ethernet-1/1\n                ethernet-1/2\n            ]\n        }\n        object required-up-uplinks {\n            value 1 # (6)!\n        }\n    }\n</code></pre> <ol> <li>A toggle to enable or disable the instance</li> <li>A reference to a MicroPython script to run. This script must live in <code>/etc/opt/srlinux/eventmgr/</code>, or <code>/opt/srlinux/eventmgr/</code> for Nokia-provided scripts</li> <li>A list of paths in a CLI notation that Event Handler will receive state change events for.</li> <li>A set of options in the form of objects and their value or values. This is useful for passing configuration to the function and is described in the options chapter.</li> <li>An example of an option's object with multiple values. Values are passed as json strings and need not follow any particular schema.</li> <li>An example of an option's object with a single value passed as a string.</li> </ol> <p>The config above is created for Oper Group use case that monitors the operation state of uplinks <code>ethernet-1/55</code>/<code>ethernet-1/56</code>, and if any of those uplinks change their state to <code>down</code>, then downstream links from the options list <code>down-link</code> will be set to down operationally. The logic to counting the amount of oper-up uplinks and putting down down-links is kept within the <code>opergroup.py</code> script referenced in the config.</p>","title":"Configuration"},{"location":"kb/event-handler/#paths","text":"<p>Event Handler monitors objects referenced by their path. Paths must be given in a CLI notation and refer to a leaf or leaf-list[^2]. A few examples:</p> <ul> <li><code>interface ethernet-1/1 oper-state</code></li> <li><code>interface ethernet-1/{1..12} oper-state</code></li> <li><code>interface ethernet-1/* oper-state</code></li> <li><code>interface * oper-state</code></li> </ul> <p>In our example, we configure Event Handler to subscribe to state changes of the <code>oper-state</code> leaf of the two uplink interfaces.</p>","title":"Paths"},{"location":"kb/event-handler/#options","text":"<p>The options field lets a user define a set of arbitrarily named objects, and associate either a value or values to it. This allows users to provide configuration options to the script's main function.</p> <p>In the example above, two options are configured:</p> <ol> <li><code>down-links</code> - an option with multiple values, in that case, two values <code>ethernet-1/1</code> and <code>ethernet-1/2</code>.     The <code>down-links</code> option thus keeps a list of downstream links we want to pass to the script. Script's logic then may use those values when it is being run.</li> <li><code>required-up-uplinks</code> - an option with a single value that conveys the number of uplinks we want to always have in oper-up state before bringing down links.</li> </ol>  <p>Note</p> <ol> <li>Option' values passed via CLI are encoded as strings</li> <li>Option' name is a free-formed string that users define as they see fit.</li> </ol>","title":"Options"},{"location":"kb/event-handler/#upython-script","text":"<p>A path to a MicroPython script is provided with <code>upython-script</code> config parameter. Read more about it in the subsequent sections.</p>","title":"uPython script"},{"location":"kb/event-handler/#script","text":"<p>An Event Handler script is the core component of the framework. It contains the logic that operates on the input JSON string passed by the <code>event_mgr</code> process to it each time there is a state change detected for the paths used in the event handler instance configuration.</p>","title":"Script"},{"location":"kb/event-handler/#micropython","text":"<p>Scripts are executed by MicroPython1 interpreter and thus have a limited set of modules available in the standard library. For the most part, users may use a regular Python interpreter to write the scripts for the Event Handler, granted they use standard libraries available for MicroPython.</p> <p>Check the Dev environment section for various ways of developing for MicroPython.</p>","title":"MicroPython"},{"location":"kb/event-handler/#location","text":"<p>Event Handler scripts may exist in two locations:</p> <ol> <li><code>/etc/opt/srlinux/eventmgr/</code> for user-provided scripts</li> <li><code>/opt/srlinux/eventmgr</code> for Nokia-provided scripts.</li> </ol> <p>No other directory hierarchy can be used.</p>","title":"Location"},{"location":"kb/event-handler/#input","text":"<p>Whenever a state change is detected for any of the monitored paths, the Event Handler calls a referenced MicroPython script. Event Handler calls a specific function <code>event_handler_main()</code> in the provided script, passing it a JSON string indicating the current values of the monitored paths, and any other options configured</p> <p>Using the configuration example given at the beginning of this page, in the event of a state change for any of the two links' operational status, the following JSON string would have been generated by the Event Handler and passed over to a script as input.</p> <pre><code>{\n    \"paths\": [\n        {\n            \"path\": \"interface ethernet-1/55 oper-state\", // (1)!\n            \"value\": \"down\"\n        },\n        {\n            \"path\": \"interface ethernet-1/56 oper-state\",\n            \"value\": \"down\"\n        }\n    ],\n    \"options\": { // (2)!\n        \"required-up-uplinks\": \"2\",\n        \"down-links\": [\n            \"ethernet-1/1\",\n            \"ethernet-1/2\"\n        ]\n    }\n}\n</code></pre> <ol> <li>the current state of each monitored path is provided in the <code>paths</code> list which contains <code>path:value</code> objects.</li> <li>user-provided options are passed in the <code>options</code> JSON object.</li> </ol>","title":"Input"},{"location":"kb/event-handler/#output","text":"<p>A MicroPython script must return a single parameter, which is a JSON string with a structure expected by the Event Handler.</p> <p>The structure of the output JSON string adheres to the following schema:</p>  Output JSON format <pre><code>{\n    \"actions\": [\n        {\n            \"set-ephemeral-path\": {\n                \"path\": \"\",\n                \"value\": \"\",\n                \"always-execute\": false\n            }\n        },\n        {\n            \"set-cfg-path\": {\n                \"path\": \"\",\n                \"value\": \"\",\n                \"always-execute\": false  \n            }\n        },\n        {\n            \"set-cfg-path\": {\n                \"path\": \"\",\n                \"json-value\": {},\n                \"always-execute\": false\n            }\n        },\n        {\n            \"delete-cfg-path\": {\n                \"path\": \"\",\n                \"always-execute\": false\n            }\n        },\n        {\n            \"set-tools-path\": {\n                \"path\": \"\",\n                \"value\": \"\",\n                \"always-execute\": false\n            }\n        },\n        {\n            \"set-tools-path\": {\n                \"path\": \"\",\n                \"json-value\": {},\n                \"always-execute\": false\n            }\n        },\n        {\n            \"run-script\": {\n                \"cmdline\": \"\",\n                \"always-execute\": false\n            }\n        },\n        {\n            \"reinvoke-with-delay\": 5000\n        }\n    ],\n    \"persistent-data\": {\n        \"last-state-up\": false\n    }\n}\n</code></pre>  <p>As seen from the output example above, the script output mostly contains a list of various actions. These actions are passed to the Event Handler for processing.</p>","title":"Output"},{"location":"kb/event-handler/#actions","text":"<p>An incomplete list of actions is provided below for reference.</p>","title":"Actions"},{"location":"kb/event-handler/#set-ephemeral-cfg","text":"<p>Allows a user to ephemerally change a state leaf. Each <code>set-ephemeral-path</code> is a <code>path:value</code>. Paths are provided in a CLI notation with the possibility to use ranges.</p> <p>The most common use case for this action is setting an interface oper-state based on some other criteria like in the oper-group use case.</p> <p>In release 21.6.1 a single path is supported by this action - <code>interface * oper-state</code> - with the values of <code>up</code>/<code>down</code>.</p>","title":"set-ephemeral-cfg"},{"location":"kb/event-handler/#dev-environment","text":"<p>Writing MicroPython scripts for the Event Handler is very much like writing regular Python scripts; a developer just needs to keep in mind a limited set of standard library modules available to them.</p> <p>For testing purposes, users may leverage <code>ghcr.io/srl-labs/upy:1.18</code> container image to execute their scripts against a MicroPython interpreter used in SR Linux. Granted, they add a <code>main()</code> function to their script in addition to the <code>event_hander_main()</code> function required by the framework.</p> <p>VS Code users can create a dev container with the above image to develop inside the container with MicroPython interpreter as demonstrated in opergroup-lab repo.</p>   <ol> <li> <p>a trimmed-down python engine - MicroPython - is used to run Event Handler scripts.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>","title":"Dev environment"},{"location":"kb/hwtypes/","text":"<p>The SR Linux software supports the following Nokia hardware platforms1:</p> <ul> <li>7250 IXR-6</li> <li>7250 IXR-10</li> <li>7220 IXR-D1</li> <li>7220 IXR-D2</li> <li>7220 IXR-D2L</li> <li>7220 IXR-D3</li> <li>7220 IXR-D3L</li> <li>7220 IXR-H2</li> <li>7220 IXR-H3</li> </ul> <p>The <code>type</code> field under the node configuration sets the emulated hardware type in the containerlab file:</p> <pre><code># part of the evpn01.clab.yml file\n  nodes:\n    leaf1:\n      kind: srl\n      type: ixrd3 # &lt;- hardware type this node will emulate\n</code></pre> <p>The <code>type</code> field defines the hardware variant that this SR Linux node will emulate. The available <code>type</code> values are:</p>    type value HW platform     ixr6 7250 IXR-6   ixr10 7250 IXR-10   ixrd1 7220 IXR-D1   ixrd2 7220 IXR-D2   ixrd2l 7220 IXR-D2L   ixrd3 7220 IXR-D3   ixrd3l 7220 IXR-D3L   ixrh2 7220 IXR-H2   ixrh3 7220 IXR-H3     <p>Tip</p> <p>Containerlab-launched nodes are started as <code>ixrd2</code> hardware type unless set to a different type in the clab file.</p>    <ol> <li> <p>SR Linux can also run on the whitebox/3rd party switches.\u00a0\u21a9</p> </li> </ol>","title":"Hardware types"},{"location":"kb/ifaces/","text":"<p>On the SR Linux, an interface is any physical or logical port through which packets can be sent to or received from other devices.</p>","title":"Interfaces"},{"location":"kb/ifaces/#loopback","text":"<p>Loopback interfaces are virtual interfaces that are always up, providing a stable source or destination from which packets can always be originated or received. The SR Linux supports up to 256 loopback interfaces system-wide, across all network instances. Loopback interfaces are named <code>loN</code>, where N is 0 to 255.</p>","title":"Loopback"},{"location":"kb/ifaces/#system","text":"<p>The system interface is a type of loopback interface that has characteristics that do not apply to regular loopback interfaces:</p> <ul> <li>The system interface can be bound to the default network-instance only.</li> <li>The system interface does not support multiple IPv4 addresses or multiple IPv6 addresses.</li> <li>The system interface cannot be administratively disabled. Once configured, it is always up.</li> </ul> <p>The SR Linux supports a single system interface named <code>system0</code>. When the system interface is bound to the default network-instance, and an IPv4 address is configured for it, the IPv4 address is the default local address for multi-hop BGP sessions to IPv4 neighbors established by the default network-instance, and it is the default IPv4 source address for IPv4 VXLAN tunnels established by the default network-instance. The same functionality applies with respect to IPv6 addresses / IPv6 BGP neighbors / IPv6 VXLAN tunnels.</p>","title":"System"},{"location":"kb/ifaces/#network","text":"<p>Network interfaces carry transit traffic, as well as originate and terminate control plane traffic and in-band management traffic.</p> <p>The physical ports in line cards installed in the SR Linux are network interfaces. A typical line card has a number of front-panel cages, each accepting a pluggable transceiver. Each transceiver may support a single channel or multiple channels, supporting one Ethernet port or multiple Ethernet ports, depending on the transceiver type and its breakout options.</p> <p>In the SR Linux CLI, each network interface has a name that indicates its type and its location in the chassis. The location is specified with a combination of slot number and port number, using the following formats: <code>ethernet-slot/port</code>. For example, interface <code>ethernet-2/1</code> refers to the line card in slot 2 of the SR Linux chassis, and port 1 on that line card.</p> <p>On 7220 IXR-D3 systems, the QSFP28 connector ports (ports <code>1/3-1/33</code>) can operate in breakout mode. Each QSFP28 connector port operating in breakout mode can have four breakout ports configured, each operating at 25G. Breakout ports are named using the following format: <code>ethernet-slot/port/breakout-port</code>.</p> <p>For example, if interface <code>ethernet 1/3</code> is enabled for breakout mode, its breakout ports are named as follows:</p> <ul> <li><code>ethernet 1/3/1</code></li> <li><code>ethernet 1/3/2</code></li> <li><code>ethernet 1/3/3</code></li> <li><code>ethernet 1/3/4</code></li> </ul>","title":"Network"},{"location":"kb/ifaces/#management","text":"<p>Management interfaces are used for out-of-band management traffic. The SR Linux supports a single management interface named <code>mgmt0</code>. The <code>mgmt0</code> interface supports the same functionality and defaults as a network interface, except for the following:</p> <ul> <li>Packets sent and received on the mgmt0 interface are processed completely in software.</li> <li>The mgmt0 interface does not support multiple output queues, so there is no output traffic differentiation based on forwarding class.</li> <li>The mgmt0 interface does not support pluggable optics. It is a fixed 10/100/ 1000-BaseT copper port.</li> </ul>","title":"Management"},{"location":"kb/ifaces/#integrated-routing-and-bridging-irb","text":"<p>IRB interfaces enable inter-subnet forwarding. Network instances of type mac-vrf are associated with a network instance of type ip-vrf via an IRB interface.</p>","title":"Integrated Routing and Bridging (IRB)"},{"location":"kb/ifaces/#subinterfaces","text":"<p>On the SR Linux, each type of interface can be subdivided into one or more subinterfaces. A subinterface is a logical channel within its parent interface.</p> <p>Traffic belonging to one subinterface can be distinguished from traffic belonging to other subinterfaces of the same port using encapsulation methods such as 802.1Q VLAN tags.</p> <p>While each port can be considered a shared resource of the router that is usable by all network instances, a subinterface can only be associated with one network instance at a time. To move a subinterface from one network instance to another, you must disassociate it from the first network instance before associating it with the second network instance.</p> <p>You can configure ACL policies to filter IPv4 and/or IPv6 packets entering or leaving a subinterface.</p> <p>The SR Linux supports policies for assigning traffic on a subinterface to forwarding classes or remarking traffic at egress before it leaves the router. DSCP classifier policies map incoming packets to the appropriate forwarding classes, and DSCP rewrite-rule policies mark outgoing packets with an appropriate DSCP value based on the forwarding class</p> <p>SR Linux subinterfaces can be specified as type routed or bridged:</p> <ul> <li>Routed subinterfaces can be assigned to a network-instance of type mgmt, default, or ip-vrf.</li> <li>Bridged subinterfaces can be assigned to a network-instance of type mac-vrf.</li> </ul> <p>Routed subinterfaces allow for configuration of IPv4 and IPv6 settings, and bridged subinterfaces allow for configuration of bridge table and VLAN ingress/egress mapping.</p>","title":"Subinterfaces"},{"location":"kb/mgmt/","text":"<p>Nokia SR Linux is equipped with 100% YANG modelled management interfaces. The supported management interfaces (CLI, JSON-RPC, and gNMI) access the common management API layer via a gRPC interface. Since all interfaces act as a client towards a common management API, SR Linux provides complete consistency across all the management interfaces with regards to the capabilities available to each of them.</p>","title":"Management interfaces"},{"location":"kb/mgmt/#sr-linux-cli","text":"<p>The SR Linux CLI is an interactive interface for configuring, monitoring, and maintaining the SR Linux via an SSH or console session.</p> <p>Throughout the course of this quickstart we will use CLI as our main configuration interface and leave the gNMI and JSON interfaces for the more advanced scenarios. For that reason, we describe CLI interface here in a bit more details than the other interfaces.</p>","title":"SR Linux CLI"},{"location":"kb/mgmt/#features","text":"<ul> <li>Output Modifiers.   Advanced Linux output modifiers <code>grep</code>, <code>more</code>, <code>wc</code>, <code>head</code>, and <code>tail</code> are exposed directly through the SR Linux CLI.</li> <li>Suggestions &amp; List Completions.   As commands are typed suggestions are provided.  Tab can be used to list options available.</li> <li>Output Format.   When displaying info from a given datastore, the output can be formatted in one of three ways:<ul> <li>Text: this is the default out, it is JSON-like but not quite JSON.</li> <li>JSON: the output will be in JSON format.</li> <li>Table: The CLI will try to format the output in a table, this doesn\u2019t work for all data but can be very useful.</li> </ul> </li> <li>Aliases.   An alias is used to map a CLI command to a shorter easier to remember command.  For example, if a command is built to retrieve specific information from the state datastore and filter on specific fields while formatting the output as a table the CLI command could get quite long.   An alias could be configured so that a shorter string of text could be used to execute that long CLI command.  Alias can be further enhanced to be dynamic which makes them extremely powerful because they are not limited to static CLI commands.</li> </ul>","title":"Features"},{"location":"kb/mgmt/#accessing-the-cli","text":"<p>After the SR Linux device is initialized, you can access the CLI using a console or SSH connection.</p> <p>Using the connection details provided by containerlab when we deployed the quickstart lab we can connect to any of the nodes via SSH protocol. For example, to connect to <code>leaf1</code>:</p> <p><pre><code>ssh admin@clab-quickstart-leaf1\n</code></pre> <pre><code>Warning: Permanently added 'clab-quickstart-leaf1,2001:172:20:20::8' (ECDSA) to the list of known hosts.\nadmin@clab-quickstart-leaf1's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:leaf1#\n</code></pre></p>","title":"Accessing the CLI"},{"location":"kb/mgmt/#prompt","text":"<p>By default, the SR Linux CLI prompt consists of two lines of text, indicating with an asterisk whether the configuration has been modified, the current mode and session type, the current CLI context, and the host name of the SR Linux device, in the following format: <pre><code>--{ modified? mode_and_session_type }--[ context ]--\nhostname#\n</code></pre></p> <p>Example: <pre><code>--{ * candidate shared }--[ acl ]--\n3-node-srlinux-A#\n</code></pre></p> <p>The CLI prompt is configurable and can be changed within the <code>environment prompt</code> configuration context.</p> <p>In addition to the prompt, SR Linux CLI has a bottom toolbar. It appears at the bottom of the terminal window and displays:</p> <ul> <li>the current mode and session type</li> <li>whether the configuration has been modified</li> <li>the user name and session ID of the current AAA session</li> <li>and the local time</li> </ul> <p>For example: <pre><code>Current mode: * candidate shared     root (36)   Wed 09:52PM\n</code></pre></p>","title":"Prompt"},{"location":"kb/mgmt/#gnmi","text":"<p>The gRPC-based gNMI protocol is used for the modification and retrieval of configuration from a target device, as well as the control and generation of telemetry streams from a target device to a data collection system.</p> <p>SR Linux can enable a gNMI server that allows external gNMI clients to connect to the device and modify the configuration and collect state information.</p> <p>Supported gNMI RPCs are:</p> <ul> <li>Get</li> <li>Set</li> <li>Subscribe</li> <li>Capabilities</li> </ul>","title":"gNMI"},{"location":"kb/mgmt/#json-rpc","text":"<p>The SR Linux provides a JSON-based Remote Procedure Call (RPC) for both CLI commands and configuration. The JSON API allows the operator to retrieve and set the configuration and state, and provide a response in JSON format. This JSON-RPC API models the CLI implemented on the system.</p> <p>If output from a command cannot be displayed in JSON, the text output is wrapped in JSON to allow the application calling the API to retrieve the output. During configuration, if a TCP port is in use when the JSON-RPC server attempts to bind to it, the commit fails. The JSON-RPC supports both normal paths, as well as XPATHs.</p>","title":"JSON-RPC"},{"location":"kb/netwinstance/","text":"<p>On the SR Linux, you can configure one or more virtual routing instances, known as network instances. Each network instance has its own interfaces, its own protocol instances, its own route table, and its own FIB.</p> <p>When a packet arrives on a subinterface associated with a network instance, it is forwarded according to the FIB of that network instance. Transit packets are normally forwarded out another subinterface of the network instance.</p> <p>SR Linux supports the following types of network instances:</p> <ul> <li>default</li> <li>ip-vrf</li> <li>mac-vrf</li> </ul> <p>The initial startup configuration for SR Linux has a single <code>default</code> network instance.</p> <p>By default, there are no ip-vrf or mac-vrf network instances; these must be created by explicit configuration. The ip-vrf network instances are the building blocks of Layer 3 IP VPN services, and mac-vrf network instances are the building blocks of EVPN services.</p> <p>Within a network instance, you can configure BGP, OSPF, and IS-IS protocol options that apply only to that network instance.</p>","title":"Network instances"},{"location":"ndk/intro/","text":"<p>Nokia SR Linux enables its users to create high-performance applications which run alongside native apps on SR Linux Network OS. These \"on-box custom applications\" can be deeply integrated with the rest of the SR Linux system and thus can perform tasks that are not possible with traditional management interfaces standard for the typical network operating systems.</p>  <p> </p> Custom applications run natively on SR Linux NOS  <p>The on-box applications (which we also refer to as \"agents\") leverage the SR Linux SDK called NetOps Development Kit or NDK for short.</p> <p>Applications developed with SR Linux NDK have a set of unique characteristics which set them aside from the traditional off-box automation solutions:</p> <ol> <li>Native integration with SR Linux system     SR Linux architecture is built so that NDK agents look and feel like any other regular application such as bgp or acl. This seamless integration is achieved on several levels:<ol> <li>System integration: when deployed on SR Linux system, an NDK agent renders itself like any other \"standard\" application. That makes lifecycle management unified between Nokia-provided system apps and custom agents.</li> <li>CLI integration: every NDK agent automatically becomes a part of the global CLI tree, making it possible to configure the agent and query its state the same way as for any other configuration region.</li> <li>Telemetry integration: an NDK agent configuration and state data will automatically become available for Streaming Telemetry consumption.</li> </ol> </li> <li>Programming language-neutral     With SR Linux NDK, the developers are not forced to use any particular language when writing their apps. As NDK is a gRPC service defined with Protocol Buffers, it is possible to use any1 programming language for which protobuf compiler is available. </li> <li>Deep integration with system components     NDK apps are not constrained to only configuration and state management, as often happens with traditional north-bound interfaces. On the contrary, the NDK service exposes additional services that enable deep integration with the SR Linux system, such as listening to RIB/FIB updates or having direct access to the datapath.</li> </ol> <p>With the information outlined in the NDK Developers Guide, you will learn about NDK architecture and how to develop apps with this kit.</p> <p>Please navigate to the Apps Catalog to browse our growing list of NDK apps that Nokia or 3rd parties wrote.</p>   <ol> <li> <p>This in practice covers all popular programming languages: Python, Go, C#, C, C++, Java, JS, etc.\u00a0\u21a9</p> </li> </ol>","title":"NetOps Development Kit"},{"location":"ndk/apps/catalog/","text":"<p>SR Linux NetOps Development Kit (NDK) enables its users to write apps to solve many automation tasks, operational hurdles, or optimization problems.</p> <p>gRPC based service that provides deep integration with Network OS is quite a novel thing for a networking domain, making NDK application examples the second most valuable asset after the NDK documentation. Sometimes the best applications are born after getting inspired by others' work or ideas implemented in different projects.</p> <p>With the App Catalog, we intend to collect references to the noteworthy NDK applications that Nokia engineers or 3rd parties have open-sourced. With that growing catalog of examples, we hope that both new and seasoned NDK users will find something that can inspire them to create their next app.</p>  <p>Disclaimer</p> <p>The examples listed in the App Catalog are not of production quality and should not be used \"as is.\" Visitors of App Catalog should treat those applications/agents as demo examples of what can be achieved with NDK.</p> <p>The applications kept under <code>srl-labs</code> or <code>nokia</code> GitHub organizations are not official Nokia products unless explicitly mentioned.</p>","title":"App Catalog"},{"location":"ndk/apps/catalog/#ndk-agents","text":"","title":"NDK agents"},{"location":"ndk/apps/catalog/#evpn-proxy","text":"<p> \u00b7 <code>jbemmel/srl-evpn-proxy</code></p> <p>SR Linux EVPN Proxy agent that allows bridging EVPN domains with domains that only employ static VXLAN.  Read more</p>","title":"EVPN Proxy"},{"location":"ndk/apps/catalog/#kbutler","text":"<p> \u00b7 <code>brwallis/srlinux-kbutler</code></p> <p>kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service's external IP with a next-hop of the worker node.  Read more</p>","title":"kButler"},{"location":"ndk/apps/catalog/#prometheus-exporter","text":"<p> \u00b7 <code>karimra/srl-prometheus-exporter</code></p> <p>SR Linux Prometheus Exporter agent creates Prometheus scrape-able endpoints on individual switches. This horizontally-scaled telemetry collection model has additional operational enhancements over traditional setups with a central telemetry collector.  Read more</p>","title":"Prometheus Exporter"},{"location":"ndk/apps/evpn-proxy/","text":"Description SR Linux EVPN Proxy agent that allows to bridge EVPN domains with domains that only employ static VXLAN   Components Nokia SR Linux, Cumulus VX   Programming Language Python   Source Code <code>jbemmel/srl-evpn-proxy</code>   Authors Jeroen van Bemmel","title":"SR Linux EVPN Proxy"},{"location":"ndk/apps/evpn-proxy/#introduction","text":"<p>Most data center designs start small before they evolve. At small scale, it may make sense to manually configure static VXLAN tunnels between leaf switches, as implemented on the 2 virtual lab nodes on the left side. </p> <p></p> <p>There is nothing wrong with such an initial design, but as the fabric grows and the number of leaves reaches a certain threshold, having to touch every switch each time a device is added can get cumbersome and error prone.</p> <p>The internet and most modern large scale data center designs use dynamic control plane protocols and volatile in-memory configuration to configure packet forwarding. BGP is a popular choice, and the Ethernet VPN address family (EVPN RFC8365) can support both L2 and L3 overlay services. However, legacy fabrics continue to support business critical applications, and there is a desire to keep doing so without service interruptions, and with minimal changes.</p> <p>So how can we move to the new dynamic world of EVPN based data center fabrics, while transitioning gradually and smoothly from these static configurations?</p>","title":"Introduction"},{"location":"ndk/apps/evpn-proxy/#evpn-proxy-agent","text":"<p>The <code>evpn-proxy</code> agent developed with NDK can answer the need of gradually transitioning from the static VXLAN dataplane to the EVPN based service. It has a lot of embedded functionality, we will cover the core feature here which is the Static VXLAN &lt;-&gt; EVPN Proxy functionality for point to point tunnels.</p> <p>The agent gets installed on SR Linux NOS and enables the control plane stitching between static VXLAN VTEP and EVPN-enabled service by generating EVPN routes on behalf of a legacy VTEP device.</p> <p></p>","title":"EVPN Proxy Agent"},{"location":"ndk/apps/kbutler/","text":"Description kButler agent ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node   Components Nokia SR Linux, Kubernetes, MetalLB   Programming Language Go   Source Code <code>brwallis/srlinux-kbutler</code>   Additional resources This agent was demonstrated at NFD 25   Authors Bruce Wallis","title":"kButler - k8s aware agent"},{"location":"ndk/apps/kbutler/#introduction","text":"<p>In the datacenter fabrics where applications run in Kubernetes clusters it is common to see Metallb to be used as a mean to advertise k8s services external IP addresses towards the fabric switches over BGP.</p>  <p> </p> kButler agent demo setup  <p>From the application owner standpoint as long as all the nodes advertise IP addresses of the application-related services things are considered to work as expected. But applications users do not get connected to the apps directly, there is always a network in-between which needs to play in unison with the applications.</p> <p>How can we make sure, that the network state matches the expectations of the applications? The networking folks may have little to no visibility into the application land, thus they may not have the necessary information to say if a network state reflects the applications configuration.</p> <p>Consider the diagram above, and the following state of affairs:</p> <ul> <li>application App1 is scaled to run on all three nodes of a cluster</li> <li>a service is created to make this application available from the outside of the k8s cluster</li> <li>all three nodes advertise the virtual IP of the App1 with its own nexthop via BGP</li> </ul> <p>If all goes well, the Data Center leaf switch will install three routes in its forwarding and will ECMP load balance requests towards the nodes running application pods.</p> <p>But what if the leaf switch has installed only two routes in its FIB? This can be a result of a fat fingering during the BGP configuration, or a less likely event of a resources congestion. In any case, the disparity between the network state and the application can arise.</p> <p>The questions becomes, how can we make the network to be aware of the applications configuration and make sure that those deviations can be easily spotted by the NetOps teams?</p>","title":"Introduction"},{"location":"ndk/apps/kbutler/#kbutler","text":"<p>The kButler NDK agent is designed to demonstrate how data center switches can tap into the application land and correlated the network state with the application configuration.</p> <p>At a high level, the agent does the following:</p> <ul> <li>subscribes to the K8S service API and is specifically interested in any new services being exposed or changes to existing exposed services. Objective is to gain view of which worker nodes host an application which has an associated exposed service</li> <li>subscribes to the SR Linux NDK API listening for any changes to the FIB</li> <li>ensures that for every worker node which hosts an application with an exposed service, there is a corresponding FIB entry for the service external IP with a next-hop of the worker node</li> <li>reports the operational status within SR Linux allowing quick alerts of any K8S service degradation</li> <li>provides contextualized monitoring, alerting and troubleshooting, exposing its data model through all available SR Linux management interfaces</li> </ul>","title":"kButler"},{"location":"ndk/apps/srl-prom-exporter/","text":"Description SR Linux Prometheus Exporter agent creates prometheus scrape-able endpoints on individual switches. This telemetry horizontally scaled telemetry collection model comes with additional operational enhancements over traditional setups with a central telemetry collector.   Components Nokia SR Linux, Prometheus   Programming Language Go   Source Code <code>karimra/srl-prometheus-exporter</code>   Authors Karim Radhouani","title":"SR Linux Prometheus Exporter"},{"location":"ndk/apps/srl-prom-exporter/#introduction","text":"<p>Most Streaming Telemetry stacks are built with a telemetry collector1 playing a key part in getting data out of the network elements via gNMI subscriptions. While this deployment model is valid and common it is not the only model that can be used.</p> <p>With SR Linux Prometheus Exporter agent we offer SR Linux users another way to consume Streaming Telemetry in a scaled out fashion.</p>  <p> </p> Classic and agent-enabled telemetry stacks  <p>With Prometheus Exporter agent deployed on SR Linux switches the telemetry deployment model changes from a \"single collector - many targets\" to a \"many collectors - single target\" mode. The collection role is now distributed across the network with Prometheus TSDB scraping metrics endpoints exposed by the agents.</p> <p>Adopting this model has some interesting benefits beyond load sharing the collection task across the network fleet:</p> <ol> <li>\"Removing\" gNMI complexity     As gNMI based collection now happens \"inside\" the switch, the monitoring teams do not need to be exposed to gNMI subscription internals or to worry about managing collectors. This streamlines the telemetry scraping workflows, as now the switches practically behave the same way as any other system that provides telemetry metrics.    </li> <li>Easy way to add/remove subscription     Since SR Linux NDK agents provide seamless integration with all the management interfaces, the subscription handling can be done via CLI/gNMI/JSON-RPC. Users will add them the same way they do any configuration on their switches.     Most common subscriptions come pre-baked into the agent, removing the need to do anything for getting basic statistics out of the switches.  </li> <li>Auto discovery of nodes     Agents can register the prometheus endpoints they expose in Consul, which will enable Prometheus server to auto-discover the new nodes as they come This is your self-organizing telemetry fleet.</li> </ol>","title":"Introduction"},{"location":"ndk/apps/srl-prom-exporter/#agents-operations","text":"<p> </p> Agent's core components and interactions map  <p>The high level operations model of the <code>srl-prometheus-exported</code> consists of the following steps:</p> <ol> <li>Agent maps metric names to gNMI XPATHs.</li> <li>A user can disable/enable metrics via any mgmt interface (CLI, gNMI, JSON-RPC)</li> <li>On each scrape request, agent performs a gNMI subscription with mode <code>ONCE</code> for all paths mapped to metrics with state enable (one subscription per metric).</li> <li>The agent will then transform the subscribe responses into prometheus metrics and send them back in the HTTP GET response body.</li> </ol> <p>The following diagram outlines the core components of the agent.</p> <p>Consult with the repository's readme on how to install and configure this agent.</p>   <ol> <li> <p>collectors such as gnmic and others.\u00a0\u21a9</p> </li> </ol>","title":"Agent's operations"},{"location":"ndk/guide/agent-install-and-ops/","text":"","title":"Agent Installation & Operations"},{"location":"ndk/guide/agent-install-and-ops/#installing-the-agent","text":"<p>The onboarding of an NDK agent onto the SR Linux system is simply a task of copying the agent and its files over to the SR Linux filesystem and placing them in the relevant directories.</p> <p>This table summarizes an agent's components and the recommended locations to use.</p>    Component Filesystem location     Executable file <code>/usr/local/bin/</code>   YANG modules <code>/opt/$agentName/yang</code>   Config file <code>/etc/opt/srlinux/appmgr/$agentName.yml</code>   Other files <code>/opt/$agentName/</code>    <p>The agent installation procedure can be carried out in different ways:</p> <ol> <li>manual copy of files via <code>scp</code> or similar tools</li> <li>automated files delivery via configuration management tools (Ansible, etc.)</li> <li>creating an <code>rpm</code> package for the agent and its files and installing the package on SR Linux</li> </ol> <p>The first two options are easy to execute, but they are a bit more involved as the installers need to maintain the remote paths for the copy commands. When using the <code>rpm</code> option, though, it becomes less cumbersome to install the package. All the installers deal with is a single <code>.rpm</code> file and a copy command. Of course, the build process of the <code>rpm</code> package is still required, and we would like to explain this process in detail.</p>","title":"Installing the agent"},{"location":"ndk/guide/agent-install-and-ops/#rpm-package","text":"<p>One of the easiest ways to create an rpm, deb, or apk package is to use the nFPM tool - a simple, 0-dependencies packager.</p> <p>The only thing that nFPM requires of a user is to create a configuration file with the general instructions on how to build a package, and the rest will be taken care of.</p>","title":"RPM package"},{"location":"ndk/guide/agent-install-and-ops/#nfpm-installation","text":"<p>nFPM offers many installation options for all kinds of operating systems and environments. In the course of this guide, we will use the universal nFPM docker image.</p>","title":"nFPM installation"},{"location":"ndk/guide/agent-install-and-ops/#nfpm-configuration-file","text":"<p>nFPM configuration file is the way of letting nFPM know how to build a package for the software artifacts that users created.</p> <p>The complete list of options the <code>nfpm.yml</code> file can have is documented on the project's site. Here we will have a look at the configuration file that is suitable for a typical NDK application written in Go.</p> <p>The file named <code>ndkDemo.yml</code> with the following contents will instruct nFPM how to build a package:</p> <pre><code>name: \"ndkDemo\"       # name of the go package\narch: \"amd64\"         # architecture you are using \nversion: \"v1.0.0\"     # version of this rpm package\nmaintainer: \"John Doe &lt;john@doe.com&gt;\"\ndescription: Sample NDK agent # description of a package\nvendor: \"JD Corp\"     # optional information about the creator of the package\nlicense: \"BSD 2\"\ncontents:                              # contents to add to the package\n  - src: ./ndkDemo                     # local path of agent binary\n    dst: /usr/local/bin/ndkDemo        # destination path of agent binary\n\n  - src: ./yang                        # local path of agent's YANG directory\n    dst: /opt/ndkDemo/yang             # destination path of agent YANG\n\n  - src: ./ndkDemo.yml                 # local path of agent yml\n    dst: /etc/opt/srlinux/appmgr/      # destination path of agent yml\n</code></pre>","title":"nFPM configuration file"},{"location":"ndk/guide/agent-install-and-ops/#running-nfpm","text":"<p>When nFPM configuration and NDK agent files are present, proceed with building an <code>rpm</code> package.</p> <p>Consider the following file layout:</p> <pre><code>.\n\u251c\u2500\u2500 ndkDemo          # agent binary file\n\u251c\u2500\u2500 ndkDemo.yml      # agent config file\n\u251c\u2500\u2500 nfpm.yml         # nFPM config file\n\u2514\u2500\u2500 yang             # directory with agent YANG modules\n    \u2514\u2500\u2500 ndkDemo.yang\n\n1 directory, 4 files\n</code></pre> <p>With these files present we can build an RPM package using the containerized nFPM image like that:</p> <pre><code>docker run --rm -v $PWD:/tmp -w /tmp goreleaser/nfpm package \\\n    --config /tmp/nfpm.yml \\\n    --target /tmp \\\n    --packager rpm\n</code></pre> <p>This command will create <code>ndkDemo-1.0.0.x86_64.rpm</code> file in the current directory that can be copied over to the SR Linux system for installation.</p>","title":"Running nFPM"},{"location":"ndk/guide/agent-install-and-ops/#installing-rpm","text":"<p>Delivering the available rpm package to a fleet of SR Linux boxes can be done with any configuration management tools. For demo purposes, we will utilize the <code>scp</code> utility:</p> <pre><code># this example copies the rpm via scp command to /tmp dir\nscp ndkDemo-1.0.0.x86_64.rpm admin@&lt;srlinux-mgmt-address&gt;:/tmp\n</code></pre> <p>Once the package has been delivered to the SR Linux system, it is ready to be installed. First, we login to SR Linux CLI and drill down to the Linux shell:</p> <pre><code>ssh admin@&lt;srlinux-address&gt;\n\nadmin@clab-srl-srl's password: \nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl# bash\n</code></pre> <p>Once in the bash shell, install the package with <code>yum install</code> or <code>rpm</code>:</p> <pre><code>sudo rpm -U /tmp/ndkDemo-1.0.0.x86_64.rpm\n</code></pre>  <p>Tip</p> <p>To check if the package was installed, issue <code>rpm -qa | grep ndkDemo</code></p> <pre><code>admin@srl ~]$ rpm -qa | grep ndkDemo\nndkDemo-1.0.0-1.x86_64\n</code></pre>  <p>During the package installation, the agent related files are copied over to the relevant paths as stated in the nFPM config file:</p> <pre><code># check the executable location\n[admin@srl ~]$ ls -la /usr/local/bin/ | grep ndkDemo\n-rw-r--r-- 1 root root    12312 Nov  4 11:28 ndkDemo\n\n# check YANG modules dir is present\n[admin@srl ~]$ ls -la /opt/ndkDemo/yang/\ntotal 8\ndrwxr-xr-x 2 root root 4096 Nov  4 12:58 .\ndrwxr-xr-x 3 root root 4096 Nov  4 12:53 ..\n-rw-r--r-- 1 root root    0 Nov  4 11:28 ndkDemo.yang\n\n# check ndkDemo config file is present\n[admin@srl ~]$ ls -la /etc/opt/srlinux/appmgr/\ntotal 16\ndrwxr-xr-x+  2 root    root    4096 Nov  4 12:58 .\ndrwxrwxrwx+ 10 srlinux srlinux 4096 Nov  4 12:53 ..\n-rw-r--r--+  1 root    root       0 Nov  4 11:28 ndkDemo.yml\n</code></pre> <p>All the agent components are available by the paths specified in the nFPM configuration file.</p>  <p>Note</p> <p>To update the SR Linux NDK app, the package has to be removed first <pre><code>sudo yum remove ndkDemo-1.0.0 # using yum\nsudo rpm -e ndkDemo-1.0.0     # using rpm\n</code></pre></p>  <p>Congratulations, the agent has been installed successfully.</p>","title":"Installing RPM"},{"location":"ndk/guide/agent-install-and-ops/#loading-the-agent","text":"<p>SR Linux's Application Manager is in charge of managing the applications lifecycle. App Manager controls both the native apps and customer-written agents.</p> <p>After a user installs the agent on the SR Linux system by copying the relevant files, they need to reload the <code>app_mgr</code> process to detect new applications. App Manager gets to know about the available apps by reading the app configuration files located at the following paths:</p>    Directory Description     <code>/opt/srlinux/appmgr/</code> SR Linux embedded applications   <code>/etc/opt/srlinux/appmgr/</code> User-provided applications    <p>To reload the App Manager:</p> <pre><code>/ tools system app-management application app_mgr reload\n</code></pre> <p>Once reloaded, App Manager will detect the new applications and load them according to their configuration. The users will be able to see their app in the list of applications:</p> <pre><code>/show system application &lt;app-name&gt;\n</code></pre>","title":"Loading the agent"},{"location":"ndk/guide/agent-install-and-ops/#managing-the-agents-lifecycle","text":"<p>An application's lifecycle can be managed via any management interface by using the following knobs from the <code>tools</code> schema.</p> <pre><code>/ tools system app-management application &lt;app-name&gt; &lt;start|stop|reload|restart&gt;\n</code></pre> <p>The commands that can be given to an application are translated to system signals as per the following table:</p>    Command Description     <code>start</code> Executes the application   <code>reload</code> Send <code>SIGHUP</code> signal to the app. This signal can be handled by the app and reload its config and change initialization values if necessary   <code>stop</code> Send <code>SIGTERM</code> signal to the app. The app should handle this signal and exit gracefully   <code>quit</code> Send <code>SIGQUIT</code> signal to the app. Default behavior is to terminate the process and dump core info   <code>kill</code> Send <code>SIGKILL</code> signal to the app. Kills the process without any cleanup","title":"Managing the agent's lifecycle"},{"location":"ndk/guide/agent/","text":"<p>As was explained in the NDK Architecture section, an agent1 is a custom software that can extend SR Linux capabilities by running alongside SR Linux native applications and performing some user-defined tasks.</p> <p>To deeply integrate with the rest of the SR Linux architecture, the agents have to be defined like an application that SR Linux's application manager can take control of. The structure of the agents is the main topic of this chapter.</p> <p>The main three components of an agent:</p> <ol> <li>Agent's executable file</li> <li>YANG module</li> <li>Agent configuration file</li> </ol>","title":"Agent Structure"},{"location":"ndk/guide/agent/#executable-file","text":"<p>An executable file is called when the agent starts running on SR Linux system. It contains the application logic and is typically an executable binary or a script.</p> <p>The application logic handles the agents' configuration that may be provided via any management interface (CLI, gNMI, etc.) and contains the core logic of interfacing with gRPC based NDK services.</p> <p>In the subsequent sections of the Developers Guide, we will cover how to write the logic of an agent and interact with various NDK services.</p> <p>An executable file can be placed at <code>/usr/local/bin</code> directory.</p>","title":"Executable file"},{"location":"ndk/guide/agent/#yang-module","text":"<p>SR Linux is a fully modeled Network OS - any native or custom application that can be configured or can have state is required to have a proper YANG model.</p> <p>The \"cost\" associated with requiring users to write YANG models for their apps pays off immensely as this</p> <ul> <li>enables seamless integration of an agent with all management interfaces: CLI, gNMI, JSON-RPC.     Any agent's configuration knobs that users expressed in YANG will be immediately available in the SR Linux CLI as if it was part of it from the beginning. Yes, with auto-suggestion of the fields as well.</li> <li>provides out-of-the-box Streaming Telemetry (gNMI) support for any config or state data that the agent maintains</li> </ul> <p>And secondly, the YANG modules for custom apps are not that hard to write as their data model is typically relatively small.</p>  <p>Note</p> <p>The YANG module is only needed if a developer wants their agent to be configurable via any management interfaces or keep state.</p>  <p>YANG files related to an agent are typically located by the <code>/opt/$agentName/yang</code> path.</p>","title":"YANG module"},{"location":"ndk/guide/agent/#configuration-file","text":"<p>Due to SR Linux modular architecture, each application, be it an internal app like <code>bgp</code> or a custom NDK agent, needs to have a configuration file. This file contains application parameters read by the Application Manager service to onboard the application onto the system.</p> <p>With an agent's config file, users define properties of an application, for example:</p> <ul> <li>application version</li> <li>location of the executable file</li> <li>YANG modules related to this app</li> <li>lifecycle management policy</li> <li>and others</li> </ul> <p>Custom agents must have their config file present by the <code>/etc/opt/srlinux/appmgr</code> directory. It is a good idea to name the agent's config file after the agent's name; if we have the agent called <code>myCoolAgent</code>, then its config file can be named <code>myCoolAgent.yml</code> and stored by the <code>/etc/opt/srlinux/appmgr</code> path.</p> <p>Through the subsequent chapters of the Developers Guide, we will cover the most important options, but here is a complete list of config file parameters:</p>  Complete list of config files parameters <pre><code># Example configuration file for the applications on sr_linux\n# All valid options are shown and explained\n# The name of the application.\n# This must be unique.\napplication-name:\n    # [Mandatory] The source path where the binary can be found\n    path: /usr/local/bin\n    # [Optional, default='./&lt;application-name&gt;'] The command to launch the application.\n    # Note these replacement rules:\n    #   {slot-num} will be replaced by the slot number the process is running on\n    #   {0}, {1}, ... can be replaced by parameters provided in the launch request (launch-by-request: Yes)\n    launch-command: \"VALUE=2 ./binary_name --log-level debug\"\n    # [Optional, default='&lt;launch-command&gt;'] The command to search for when checking if the application is running.\n    # This will be executed as a prefix search, so if the application was launched using './app-name -loglevel debug'\n    # a search-command './app-name' would work.\n    # Note: same replacement rules as launch-command\n    search-command: \"./binary_name\"\n    # [Optional, default=No] Indicates whether the application needs to be launched automatically\n    never-start: No\n    # [Optional, default=No] Indicates whether the application can be restarted automatically when it crashes.\n    # Applies only when never-start is No (if the app is not started by app_mgr it would not be restarted either).\n    # Applications are only restarted when running app_mgr in restart mode (e.g. sr_linux --restart)\n    never-restart: No\n    # [Optional, default=No] Indicates whether the application will be shown in app manager status\n    never-show: No\n    # [Optional, default=No] Indicates whether the launch of the application is delayed\n    # until any configuration is loaded in the application's YANG modules.\n    wait-for-config: No\n    # [Optional] Indicates the application is run as 'user' including 'root'\n    run-as-user: root\n    # [Optional, default=200] Indicates the order in which the application needs to be launched.\n    # The applications with the lowest value are launched first.\n    # Applications with the same value are launched in an undefined order.\n    # By convention, start-order &gt;= 100 require idb.  1 is reserved for device-mgr, which determines chassis type.\n    start-order: 123\n    # [Optional, default=No] Indicates whether this application is launched via an request (idb only at this point).\n    launch-by-request: No\n    # [Optional, default=No] Indicates whether this application is launched in a net namespace (launch-by-request\n    # must be set to Yes).\n    launch-in-net-namespace: No\n    # [Optional, default=3] Indicates the number of restarts within failure-window which will trigger the system restart\n    failure-threshold: 3\n    # [Optional, default=300] Indicates the window in seconds over which to count restarts towards failure-threshold\n    failure-window: 400\n    # [Optional, default=reboot] Indicates the action taken after 'failure-threshold' failures within 'failure-window'\n    failure-action: 'reboot'\n    # [Optional, default=Nokia] Indicates the author of the application\n    author: 'Nokia'\n    # [Optional, default=\u201d\u201d] The command for app_mgr to run to read the application version\n    version-command: 'snmpd --version'\n    # [Optional The operations that may not be manually performed on this application\n    restricted-operations: ['start', 'stop', 'restart', 'quit', 'kill']\n    # [Optional, default No] app-mgr will wait for app to acknowledge it via oob channel\n    oob-init: No\n    # [Optional] The list of launch restrictions - if of all of the restrictions of an element in the list are met,\n    # then the application is launched.  The restrictions are separated by a ':'.  Valid restrictions are:\n    #   'sim' - running in sim mode (like in container env.)\n    #   'hw' - running on real h/w\n    #   'chassis' - running on a chassis (cpm and imm are running on different processors)\n    #   'imm' - runs on the imm\n    #   'cpm' - runs on the cpm (default)\n    launch-restrictions: ['hw:cpm', 'hw:chassis:imm']\n    yang-modules:\n        # [Mandatory] The names of the YANG modules to load. This is usually the file-name without '.yang'\n        names: [module-name, other-module-name]\n        # [Optional] List of enabled YANG features. Each needs to be qualified (e.g. srl_nokia-common:foo)\n        enabled-features: ['module-name:foo', 'other-module-name:bar']\n        # [Optional] The names of the YANG validation plugins to load.\n        validation-plugins: [plugin-name, other-plugin-name]\n        # [Mandatory] All the source-directories where we should search for:\n        #    - The YANG modules listed here\n        #    - any YANG module included/imported in these modules\n        source-directories: [/path/one, /path/two]\n        # [Optional] The names of the not owned YANG modules to load for commit confirmation purposes.\n        not-owned-names: [module-name, other-module-name]\n# [Optional] Multiple applications can be defined in the same YAML file\nother-application-name:\n    command: \"./other-binary\"\n    path: /other/path\n</code></pre>","title":"Configuration file"},{"location":"ndk/guide/agent/#dependency-and-other-files","text":"<p>Quite often, an agent may require additional files for its operation. It can be a virtual environment for your Python agent or some JSON file that your agent consumes.</p> <p>All those auxiliary files can be saved by the <code>/opt/$agentName/</code> directory.</p>   <ol> <li> <p>terms NDK agent and NDK app are used interchangeably\u00a0\u21a9</p> </li> </ol>","title":"Dependency and other files"},{"location":"ndk/guide/architecture/","text":"<p>SR Linux provides a Software Development Kit (SDK) to assist operators with developing agents that run alongside SR Linux applications. This SDK is named NetOps Development Kit, or NDK for short.</p> <p>NDK allows operators to write applications (a.k.a agents) that deeply integrate with other native SR Linux applications. The deep integration is the courtesy of the NDK gRPC service that enables custom applications to interact with other SR Linux applications via Impart Database (IDB).</p> <p>In Fig. 1, custom NDK applications <code>app-1</code> and <code>app-2</code> interact with other SR Linux subsystems via gRPC-based NDK service.</p>   Fig 1. NDK applications integration  <p>In addition to the traditional tasks of reading and writing configuration, NDK-based applications gain low-level access to the SR Linux system. For example, these apps can install FIB routes or listen to LLDP events.</p>","title":"NDK Architecture"},{"location":"ndk/guide/architecture/#grpc-protocol-buffers","text":"<p>NDK uses gRPC - a high-performance, open-source framework for remote procedure calls.</p> <p>gRPC framework by default uses Protocol buffers as its Interface Definition Language as well as the underlying message exchange format.</p>  <p>Info</p> <p>Protocol buffers are Google's language-neutral, platform-neutral, extensible mechanism for serializing structured data \u2013 think XML, but smaller, faster, and simpler. You define how you want your data to be structured once, then you can use special generated source code to easily write and read your structured data to and from a variety of data streams and using a variety of languages.</p>  <p>In gRPC, a client application can directly call a method on a server application on a different machine as if it were a local object. As in many RPC systems, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types.</p> <p>On the server side, the server implements this interface and runs a gRPC server to handle client calls. On the client side, the client provides the same methods as the server.</p>   Fig 2. gRPC client-server interactions  <p>Leveraging gRPC and protobufs provides some substantial benefits for NDK users:</p> <ol> <li>Language neutrality: NDK apps can be written in any language for which protobuf compiler exists. Go, Python, C, Java, Ruby, and more languages are supported by Protocol buffers enabling SR Linux users to write apps in the language of their choice.</li> <li>High-performance: protobuf-encoded messaging is an efficient way to exchange data in a client-server environment. Applications that consume high-volume streams of data (for example, route updates) benefit from an efficient and fast message delivery enabled by protobuf.</li> <li>Backwards API compatibility: a protobuf design property of using IDs for data fields makes it possible to evolve API over time without ever breaking backward compatibility. Old clients will still be able to consume data stored in the original fields, whereas new clients will benefit from accessing data stored in the new fields.</li> </ol>","title":"gRPC &amp; Protocol buffers"},{"location":"ndk/guide/architecture/#ndk-service","text":"<p>NDK provides a collection of gRPC services, each of which enables custom applications to interact with a particular subsystem on an SR Linux OS, delivering a high level of integration and extensibility.</p> <p>With this architecture, NDK agents act as gRPC clients that execute remote procedure calls (RPC) on a system that implements a gRPC server.</p> <p>On SR Linux, <code>ndk_mgr</code> is the application that runs the NDK gRPC server. Fig 3. shows how custom agents interact via gRPC with NDK, and NDK executes the remote procedure and communicates with other system applications through IDB and pub/sub interface to return the result of the RPC to a client.</p>   Fig 3. gRPC as an Inter Process Communication (IPC) protocol  <p>As a result, custom applications are able to communicate with the native SR Linux apps as if they were shipped with SR Linux OS.</p>","title":"NDK Service"},{"location":"ndk/guide/architecture/#proto-files","text":"<p>NDK services, underlying RPCs, and messages are defined in <code>.proto</code> files. These files are used to generate language bindings essential for the NDK apps development process and serve as the data modeling language for the NDK itself.</p> <p>The source <code>.proto</code> files for NDK are open and published in <code>nokia/srlinux-ndk-protobufs</code> repository. Anyone can clone this repository and explore the NDK gRPC services or build language bindings for the programming language of their choice.</p>","title":"Proto files"},{"location":"ndk/guide/architecture/#documentation","text":"<p>Although the proto files are human-readable, it is easier to browse the NDK services using the generated documentation that we keep in the same <code>nokia/srlinux-ndk-protobufs</code> repo. The HTML document is provided in the readme file that appears when a user selects a tag that matches the NDK release version1.</p> <p>The generated documentation provides the developers with a human-readable reference of all the services, messages, and types that comprise the NDK service.</p>","title":"Documentation"},{"location":"ndk/guide/architecture/#operations-flow","text":"<p>Regardless of the language in which the agents are written, at a high level, the following flow of operations applies to all agents when interacting with the NDK service:</p>   Fig 4. NDK operations flow  <ol> <li>Establish gRPC channel with NDK manager and instantiate an NDK client</li> <li>Register the agent with the NDK manager</li> <li>Register notification streams for different types of NDK services (config, lldp, interface, etc.)</li> <li>Start streaming notifications</li> <li>Handle the streamed notifications</li> <li>Update agent's state data if needed</li> <li>Exit gracefully if required</li> </ol> <p>To better understand the steps each agent undergoes, we will explain them in a language-neutral manner. For language-specific implementations, read the \"Developing with NDK\" chapter.</p>","title":"Operations flow"},{"location":"ndk/guide/architecture/#grpc-channel-and-ndk-manager-client","text":"<p>NDK agents communicate with gRPC based NDK service by invoking RPCs and handling responses. An RPC generally takes in a client request message and returns a response message from the server.</p> <p>A gRPC channel must be established before communicating with the NDK manager application running on SR Linux2. NDK server runs on port <code>50053</code>; agents which are installed on SR Linux OS use <code>localhost:50053</code> socket to establish the gRPC channel.</p> <p>Once the gRPC channel is set up, a gRPC client (often called stub) needs to be created to perform RPCs. Each gRPC service needs to have its own client. In NDK, the <code>SdkMgrService</code> service is the first service that agents interact with, therefore, users first need to create the NDK Manager Client (Mgr Client on diagram) that will be able to call RPCs defined for <code>SdkMgrService</code>.</p>","title":"gRPC Channel and NDK Manager Client"},{"location":"ndk/guide/architecture/#agent-registration","text":"<p>Agent must be first registered with SRLinux NDK by calling <code>AgentRegister</code> RPC of <code>SdkMgrService</code>. Initial agent state is created during the registration process.</p> <p>An <code>AgentRegistrationResponse</code> is returned (omitted in Fig. 4) with the status of the registration process.</p>","title":"Agent registration"},{"location":"ndk/guide/architecture/#registering-notifications","text":"<p>Agents interact with other services like Network Instance, Config, LLDP, BFD by subscribing to notification updates from these services.</p> <p>Before subscribing to a notification stream of a certain service the subscription stream needs to be created. To create it, a client of <code>SdkMgrService</code> calls <code>NotificationRegister</code> RPC with <code>NotificationRegistrationRequest</code> field <code>Op</code> set to <code>Create</code> and other fields absent.</p>  <p>Info</p> <p><code>NotificationRegistrationRequest</code> message's field <code>Op</code> (for Operation) may have one of the following values:</p> <ul> <li><code>Create</code> creates a subscription stream and returns a <code>StreamId</code> that is used when adding subscriptions with the <code>AddSubscription</code> operation.</li> <li><code>Delete</code> deletes the existing subscription stream that has a particular <code>SubId</code>.</li> <li><code>AddSubscription</code> adds a subscription. The stream will now be able to stream notifications of that subscription type (e.g., Intf, NwInst, etc).</li> <li><code>DeleteSubscription</code> deletes the previously added subscription.</li> </ul>  <p>When <code>Op</code> field is set to <code>Create</code>, NDK Manager responds with <code>NotificationRegisterResponse</code> message with <code>stream_id</code> field set to some value. The stream has been created, and the subscriptions can be added to the created stream.</p> <p>To subscribe to a certain service notification updates another call of <code>NotificationRegister</code> RPC is made with the following fields set:</p> <ul> <li><code>stream_id</code> set to an obtained value from the <code>NotificationRegisterResponse</code></li> <li><code>Op</code> is set to <code>AddSubscription</code></li> <li>one of the <code>subscription_types</code> is set according to the desired service notifications. For example, if notifications from the <code>Config</code> service are of interest, then <code>config</code> field of type <code>ConfigSubscriptionRequest</code> is set.</li> </ul> <p><code>NotificationRegisterResponse</code> message follows the request and contains the same <code>stream_id</code> but now also the <code>sub_id</code> field - subscription identifier. At this point agent successfully indicated its desire to receive notifications from certain services, but the notification streams haven't been started yet.</p>","title":"Registering notifications"},{"location":"ndk/guide/architecture/#streaming-notifications","text":"<p>Requesting applications to send notifications is done by interfacing with <code>SdkNotificationService</code>. As this is another gRPC service, it requires its own client - Notification client.</p> <p>To initiate streaming of updates based on the agent subscriptions the Notification Client executes <code>NotificationStream</code> RPC which has <code>NotificationStreamRequest</code> message with <code>stream_id</code> field set to the ID of a stream to be used. This RPC returns a stream of <code>NotificationStreamResponse</code>, which makes this RPC of type \"server streaming RPC\".</p>  Server-streaming RPC <p>A server-streaming RPC is similar to a unary RPC, except that the server returns a stream of messages in response to a client's request. After sending all its messages, the server's status details (status code and optional status message) and optional trailing metadata are sent to the client. This completes processing on the server side. The client completes once it has all the server's messages.</p>  <p><code>NotificationStreamResponse</code> message represents a notification stream response that contains one or more notifications. The <code>Notification</code> message contains one of the <code>subscription_types</code> notifications, which will be set in accordance to what notifications were subscribed by the agent.</p> <p>In our example, we sent <code>ConfigSubscriptionRequest</code> inside the <code>NotificationRegisterRequest</code>, hence the notifications that we will get back for that <code>stream_id</code> will contain <code>ConfigNotification</code> messages inside <code>Notification</code> of a <code>NotificationStreamResponse</code>.</p>","title":"Streaming notifications"},{"location":"ndk/guide/architecture/#handling-notifications","text":"<p>The agent handles the stream of notifications by analyzing which concrete type of notification was read from the stream. The Server streaming RPC will provide notifications till the last available one; the agent then reads out the incoming notifications and handles the messages contained within them.</p> <p>The handling of notifications is done when the last notification is sent by the server. At this point, the agent may perform some work on the received data and, if needed, update the agent's state if it has one.</p>","title":"Handling notifications"},{"location":"ndk/guide/architecture/#updating-agents-state-data","text":"<p>Each agent may keep state and configuration data modeled in YANG. When an agent needs to set/update its own state data (for example, when it made some calculations based on received notifications), it needs to use <code>SdkMgrTelemetryService</code> and a corresponding client.</p>   Fig 5. Updating agent's state flow  <p>The state that an agent intends to have will be available for gNMI telemetry, CLI access, and JSON-RPC retrieval, as it essentially becomes part of the SR Linux state.</p> <p>Updating or initializing agent's state with data is done with <code>TelemetryAddOrUpdate</code> RPC that has a request of type <code>TelemetryUpdateRequest</code> that encloses a list of <code>TelemetryInfo</code> messages. Each <code>TelemetryInfo</code> message contains a <code>key</code> field that points to a subtree of agent's YANG model that needs to be updated with the JSON data contained within <code>data</code> field.</p>","title":"Updating agent's state data"},{"location":"ndk/guide/architecture/#exiting-gracefully","text":"<p>When an agent needs to stop its operation and be removed from the SR Linux system, it needs to be unregistered by invoking <code>AgentUnRegister</code> RPC of the <code>SdkMgrService</code>. The gRPC connection to the NDK server needs to be closed.</p> <p>When unregistered, the agent's state data will be removed from SR Linux system and will no longer be accessible to any of the management interfaces.</p>   <ol> <li> <p>For example, here you will find the auto-generated documentation for the latest NDK version at the moment of this writing.\u00a0\u21a9</p> </li> <li> <p><code>sdk_mgr</code> is the name of the application that implements NDK gRPC server and runs on SR Linux OS.\u00a0\u21a9</p> </li> </ol>","title":"Exiting gracefully"},{"location":"ndk/guide/dev/go/","text":"<p>This guide explains how to consume the NDK service when developers write the agents in a Go1 programming language.</p>  <p>Note</p> <p>This guide provides code snippets for several operations that a typical agent needs to perform according to the NDK Service Operations Flow chapter.</p> <p>Where applicable, the chapters on this page will refer to the NDK Architecture section to provide more context on the operations.</p>  <p>In addition to the publicly available protobuf files, which define the NDK Service, Nokia also provides generated Go bindings for data access classes of NDK in a <code>nokia/srlinux-ndk-go</code> repo.</p> <p>The <code>github.com/nokia/srlinux-ndk-go</code> package provided in that repository enables developers of NDK agents to immediately start writing NDK applications without the need to generate the Go package themselves.</p>","title":"Developing agents with NDK in Go"},{"location":"ndk/guide/dev/go/#establish-grpc-channel-with-ndk-manager-and-instantiate-an-ndk-client","text":"<p> Additional information</p> <p>To call service methods, a developer first needs to create a gRPC channel to communicate with the NDK manager application running on SR Linux.</p> <p>This is done by passing the NDK server address - <code>localhost:50053</code> - to <code>grpc.Dial()</code> as follows:</p> <pre><code>import (\n    \"google.golang.org/grpc\"\n)\n\nconn, err := grpc.Dial(\"localhost:50053\", grpc.WithInsecure())\nif err != nil {\n  ...\n}\ndefer conn.Close()\n</code></pre> <p>Once the gRPC channel is setup, we need to instantiate a client (often called stub) to perform RPCs. The client is obtained using the <code>NewSdkMgrServiceClient</code> method provided.</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\n\nclient := ndk.NewSdkMgrServiceClient(conn)\n</code></pre>","title":"Establish gRPC channel with NDK manager and instantiate an NDK client"},{"location":"ndk/guide/dev/go/#register-the-agent-with-the-ndk-manager","text":"<p> Additional information</p> <p>Agent must be first registered with SR Linux by calling the <code>AgentRegister</code> method available on the returned <code>SdkMgrServiceClient</code> interface. The initial agent state is created during the registration process.</p>","title":"Register the agent with the NDK manager"},{"location":"ndk/guide/dev/go/#agents-context","text":"<p>Go context is a required parameter for each RPC service method. Contexts provide the means of enforcing deadlines and cancellations as well as transmitting metadata within the request.</p> <p>During registration, SR Linux will be expecting a key-value pair with the <code>agent_name</code> key and a value of the agent's name passed in the context of an RPC. The agent name is defined in the agent's YAML file.</p>  <p>Warning</p> <p>Not including this metadata in the agent <code>ctx</code> would result in an agent registration failure. SR Linux would not be able to differentiate between two agents both connected to the same NDK manager.</p>  <pre><code>ctx, cancel := context.WithCancel(context.Background())\ndefer cancel()\n// appending agent's name to the context metadata\nctx = metadata.AppendToOutgoingContext(ctx, \"agent_name\", \"ndkDemo\")\n</code></pre>","title":"Agent's context"},{"location":"ndk/guide/dev/go/#agent-registration","text":"<p><code>AgentRegister</code> method takes in the context <code>ctx</code> that is by now has agent name as its metadata and an <code>AgentRegistrationRequest</code>.</p> <p><code>AgentRegistrationRequest</code> structure can be passed in with its default values for a basic registration request.</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\n\nr, err := client.AgentRegister(ctx, &amp;ndk.AgentRegistrationRequest{})\nif err != nil {\n    log.Fatalf(\"agent registration failed: %v\", err)\n}\n</code></pre> <p><code>AgentRegister</code> method returns <code>AgentRegistrationResponse</code> and an error. Response can be additionally checked for status and error description.</p>","title":"Agent registration"},{"location":"ndk/guide/dev/go/#register-notification-streams","text":"<p> Additional information</p>","title":"Register notification streams"},{"location":"ndk/guide/dev/go/#create-subscription-stream","text":"<p>A subscription stream needs to be created first before any of the subscription types can be added. <code>SdkMgrServiceClient</code> first creates the subscription stream by executing <code>NotificationRegister</code> method with a [<code>NotificationRegisterRequest</code>][notif_reg_req_godoc] only field <code>Op</code> set to a value of <code>const NotificationRegisterRequest_Create</code>. This effectively creates a stream which is identified with a <code>StreamID</code> returned inside the <code>NotificationRegisterResponse</code>.</p> <p><code>StreamId</code> must be associated when subscribing/unsubscribing to certain types of router notifications.</p> <pre><code>req := &amp;ndk.NotificationRegisterRequest{\n    Op: ndk.NotificationRegisterRequest_Create,\n}\n\nresp, err := client.NotificationRegister(ctx, req)\nif err != nil {\n    log.Fatalf(\"Notification Register failed with error: %v\", err)\n} else if resp.GetStatus() == ndk.SdkMgrStatus_kSdkMgrFailed {\n    r.log.Fatalf(\"Notification Register failed with status %d\", resp.GetStatus())\n}\n\nlog.Debugf(\"Notification Register was successful: StreamID: %d SubscriptionID: %d\", resp.GetStreamId(), resp.GetSubId())\n}\n</code></pre>","title":"Create subscription stream"},{"location":"ndk/guide/dev/go/#add-notification-subscriptions","text":"<p>Once the <code>StreamId</code> is acquired, a client can register notifications of a particular type to be delivered over that stream.</p> <p>Different types of notifications types can be subscribed to by calling the same <code>NotificationRegister</code> method with a <code>NotificationRegisterRequest</code> having <code>Op</code> field set to <code>NotificationRegisterRequest_AddSubscription</code> and certain <code>SubscriptionType</code> selected.</p> <p>In the example below we would like to receive notifications from the [<code>Config</code>][config_servi<code>ce_docs] service, hence we specify</code>NotificationRegisterRequest_Config` subscription type.</p> <pre><code>subType := &amp;ndk.NotificationRegisterRequest_Config{ // This is unique to each notification type (Config, Intf, etc.).\n    Config: &amp;ndk.ConfigSubscriptionRequest{},\n}\nreq := &amp;ndk.NotificationRegisterRequest{\n    StreamId:          resp.GetStreamId(), // StreamId is retrieved from the NotificationRegisterResponse\n    Op:                ndk.NotificationRegisterRequest_AddSubscription,\n    SubscriptionTypes: subType,\n}\nresp, err := r.mgrStub.NotificationRegister(r.ctx, req)\nif err != nil {\n    log.Fatalf(\"Agent could not subscribe for config notification\")\n} else if resp.GetStatus() == ndk.SdkMgrStatus_kSdkMgrFailed {\n    log.Fatalf(\"Agent could not subscribe for config notification with status  %d\", resp.GetStatus())\n}\nlog.Infof(\"Agent was able to subscribe for config notification with status %d\", resp.GetStatus())\n</code></pre>","title":"Add notification subscriptions"},{"location":"ndk/guide/dev/go/#streaming-notifications","text":"<p> Additional information</p> <p>Actual streaming of notifications is a task for another service - <code>SdkNotificationService</code>. This service requires developers to create its own client, which is done with <code>NewSdkNotificationServiceClient</code> function.</p> <p>The returned <code>SdkNotificationServiceClient</code> interface has a single method <code>NotificationStream</code> that is used to start streaming notifications.</p> <p><code>NotificationsStream</code> is a server-side streaming RPC which means that SR Linux (server) will send back multiple event notification responses after getting the agent's (client) request.</p> <p>To tell the server to start streaming notifications that were subscribed to before the <code>NewSdkNotificationServiceClient</code> executes <code>NotificationsStream</code> method where <code>NotificationStreamRequest</code> struct has its <code>StreamId</code> field set to the value that was obtained at subscription stage.</p> <pre><code>req := &amp;ndk.NotificationStreamRequest{\n    StreamId: resp.GetStreamId(),\n}\nstreamResp, err := notifClient.NotificationStream(ctx, req)\nif err != nil {\n    log.Fatal(\"Agent failed to create stream client with error: \", err)\n}\n</code></pre>","title":"Streaming notifications"},{"location":"ndk/guide/dev/go/#handle-the-streamed-notifications","text":"<p> Additional information</p> <p>Handling notifications starts with reading the incoming notification messages and detecting which type this notification is exactly. When the type is known the client reads the fields of a certain notification. Here is the pseudocode that illustrates the flow:</p> <pre><code>func HandleNotifications(stream ndk.SdkNotificationService_NotificationStreamClient) {\n    for { // loop until stream returns io.EoF\n        notification stream response (nsr) := stream.Recv()\n        for notif in nsr.Notification { // nsr.Notification is a slice of `Notification`\n            if notif.GetConfig() is not nil {\n                1. config notif = notif.GetConfig()\n                2. handle config notif\n            } else if notif.GetIntf() is not nil {\n                1. intf notif = notif.GetIntf()\n                2. handle intf notif\n            } ... // Do this if statement for every notification type the agent is subscribed to\n        }\n    }\n}\n</code></pre> <p><code>NotificationStream</code> method of the <code>SdkNotificationServiceClient</code> interface will return a stream client <code>SdkNotificationService_NotificationStreamClient</code>.</p> <p><code>SdkNotificationService_NotificationStreamClient</code> contains a <code>Recv()</code> to retrieve notifications one by one. At the end of a stream <code>Rev()</code> will return <code>io.EOF</code>.</p> <p><code>Recv()</code> returns a <code>*NotificationStreamResponse</code> which contains a slice of <code>Notification</code>.</p> <p><code>Notification</code> struct has <code>GetXXX()</code> methods defined which retrieve the notification of a specific type. For example: <code>GetConfig</code> returns <code>ConfigNotification</code>.</p>  <p>Note</p> <p><code>ConfigNotification</code> is returned only if <code>Notification</code> struct has a certain subscription type set for its <code>SubscriptionType</code> field. Otherwise, <code>GetConfig</code> returns <code>nil</code>.</p>  <p>Once the specific <code>XXXNotification</code> has been extracted using the <code>GetXXX()</code> method, users can access the fields of the notification and process the data contained within the notification using <code>GetKey()</code> and <code>GetData()</code> methods.</p>","title":"Handle the streamed notifications"},{"location":"ndk/guide/dev/go/#exiting-gracefully","text":"<p>Agent needs to handle SIGTERM signal that is sent when a user invokes <code>stop</code> command via SR Linux CLI. The following is the required steps to cleanly stop the agent:</p> <ol> <li>Remove any agent's state if it was set using <code>TelemetryDelete</code> method of a Telemetry client.</li> <li>Delete notification subscriptions stream <code>NotificationRegister</code> method with <code>Op</code> set to <code>NotificationRegisterRequest_Delete</code>.</li> <li>Invoke use <code>AgentUnRegister()</code> method of a <code>SdkMgrServiceClient</code> interface.</li> <li>Close gRPC channel with the <code>sdk_mgr</code>.</li> </ol>","title":"Exiting gracefully"},{"location":"ndk/guide/dev/go/#logging","text":"<p>To debug an agent, the developers can analyze the log messages that the agent produced. If the agent's logging facility used stdout/stderr to write log messages, then these messages will be found at <code>/var/log/srlinux/stdout/</code> directory.</p> <p>The default SR Linux debug messages are found in the messages directory <code>/var/log/srlinux/buffer/messages</code>; check them when something went wrong within the SR Linux system (agent registration failed, IDB server warning messages, etc.).</p> <p>Logrus is a popular structured logger for Go that can log messages of different levels of importance, but developers are free to choose whatever logging package they see fit.</p>   <ol> <li> <p>Make sure that you have set up the dev environment as explained on this page. Readers are also encouraged to first go through the gRPC basic tutorial to get familiar with the common gRPC workflows when using Go.\u00a0\u21a9</p> </li> </ol>","title":"Logging"},{"location":"ndk/guide/env/go/","text":"<p>Although every developer's environment is different and is subject to a personal preference, we will provide recommendations for a Go toolchain setup suitable for the development and build of NDK applications.</p>","title":"Go Development Environment"},{"location":"ndk/guide/env/go/#environment-components","text":"<p>The toolchain that can be used to develop and build Go-based NDK apps consists of the following components:</p> <ol> <li>Go programming language - Go compiler, toolchain, and standard library</li> <li>Go NDK bindings - generated data access classes for gRPC based NDK service.</li> <li>Goreleaser - Go-focused build &amp; release pipeline runner. Packages nFPM to produce rpm packages that can be used to install NDK agents.</li> </ol>","title":"Environment components"},{"location":"ndk/guide/env/go/#project-structure","text":"<p>It is recommended to use Go modules when developing applications with Go. Go modules allow for better dependency management and can be placed outside the <code>$GOPATH</code> directory.</p> <p>Here is an example project structure that you can use for the NDK agent development:</p> <pre><code>.                            # Root of a project\n\u251c\u2500\u2500 app                      # Contains agent core logic\n\u251c\u2500\u2500 yang                     # A directory with agent YANG modules\n\u251c\u2500\u2500 agent.yml                # Agent yml config file\n\u251c\u2500\u2500 .goreleaser.yml          # Goreleaser config file\n\u251c\u2500\u2500 main.go                  # Package main that calls agent logic\n\u251c\u2500\u2500 go.mod                   # Go mod file\n\u251c\u2500\u2500 go.sum                   # Go sum file\n</code></pre>","title":"Project structure"},{"location":"ndk/guide/env/go/#ndk-language-bindings","text":"<p>As explained in the NDK Architecture section, NDK is a gRPC based service. To be able to use gRPC services in a Go program the language bindings have to be generated from the source proto files.</p> <p>Nokia not only provides the proto files for the SR Linux NDK service but also NDK Go language bindings.</p> <p>With the provided Go bindings, the NDK can be imported in a Go project like that:</p> <pre><code>import \"github.com/nokia/srlinux-ndk-go/v21/ndk\"\n</code></pre>","title":"NDK language bindings"},{"location":"ndk/guide/env/python/","text":"<p>Although every developer's environment is different and is subject to a personal preference, we will provide some recommendations for a Python toolchain setup suitable for the development of NDK applications.</p>","title":"Python Development Environment"},{"location":"ndk/guide/env/python/#environment-components","text":"<p>The toolchain that can be used to develop Python-based NDK apps consists of the following components:</p> <ol> <li>Python programming language - Python interpreter, toolchain, and standard library. Python2 is not supported.</li> <li>Python NDK bindings - generated data access classes for gRPC based NDK service.</li> </ol>","title":"Environment components"},{"location":"ndk/guide/env/python/#project-structure","text":"<p>Here is an example project structure that you can use for the NDK agent development:</p> <pre><code>.                            # Root of a project\n\u251c\u2500\u2500 app                      # Contains agent core logic\n\u251c\u2500\u2500 yang                     # A directory with agent YANG modules\n\u251c\u2500\u2500 agent.yml                # Agent yml config file\n\u251c\u2500\u2500 main.py                  # Package main that calls agent logic\n\u251c\u2500\u2500 requirements.txt         # Python packages required by the app logic\n</code></pre>","title":"Project structure"},{"location":"ndk/guide/env/python/#ndk-language-bindings","text":"<p>As explained in the NDK Architecture section, NDK is a gRPC based service. The language bindings have to be generated from the source proto files to use gRPC services in a Python program.</p> <p>Nokia provides both the proto files for the SR Linux NDK service and also NDK Python language bindings.</p> <p>With the provided Python bindings, the NDK can be installed with <code>pip</code></p> <pre><code># it is a good practice to use virtual env\nsudo python3 -m venv /opt/myApp/venv\n\n# activate the newly created venv\nsource /opt/myApp/venv/bin/activate\n\n# update pip/setuptools in the venv\npip3 install -U pip setuptools\n\n# install the latest pip package of the NDK\npip install srlinux-ndk # (1)\n</code></pre> <ol> <li>To install a specific version of the NDK check the NDK install instructions on the NDK github repo.</li> </ol> <p>Once installed, NDK services are imported in a Python project like that:</p> <pre><code>from ndk import appid_service_pb2 # (1)\n</code></pre> <ol> <li>The example is provided for <code>appid_service_pb2</code> service but every service is imported the same way.</li> </ol>","title":"NDK language bindings"},{"location":"tutorials/about/","text":"<p>Learning by doing is not only the most effective method but also an extremely fun one.</p> <p>The hands-on tutorials we provide in this section are designed in such a way that anyone can launch them</p> <ul> <li>at absolutely no cost</li> <li>whenever they want it</li> <li>whatever machine they have</li> <li>and run it for as long as required</li> </ul> <p>The tutorials use the open-source containerlab project to deploy the lab environment with all the needed components. This ensures that both the tutorial authors and the readers work in the same environment. No more second-guessing why the tutorial's outputs differ from yours!</p>","title":"SR Linux tutorials"},{"location":"tutorials/l2evpn/evpn/","text":"<p>Ethernet Virtual Private Network (EVPN), along with Virtual eXtensible LAN (VXLAN), is a technology that allows Layer 2 and Layer 3 traffic to be tunneled across an IP network.</p> <p>The SR Linux EVPN-VXLAN solution enables Layer 2 Broadcast Domains (BDs) in multi-tenant data centers using EVPN for the control plane and VXLAN as the data plane. It includes the following features:</p> <ul> <li>EVPN for VXLAN tunnels (Layer 2), extending a BD in overlay multi-tenant DCs</li> <li>EVPN for VXLAN tunnels (Layer 3), allowing inter-subnet-forwarding for unicast traffic within the same tenant infrastructure</li> </ul> <p>This tutorial is focused on EVPN for VXLAN tunnels Layer 2.</p>","title":"EVPN configuration"},{"location":"tutorials/l2evpn/evpn/#overview","text":"<p>EVPN-VXLAN provides Layer-2 connectivity in multi-tenant DCs. EVPN-VXLAN Broadcast Domains (BD) can span several leaf routers connected to the same IP fabric, allowing hosts attached to the same BD to communicate as though they were connected to the same layer-2 switch.</p> <p>VXLAN tunnels bridge the layer-2 frames between leaf routers with EVPN providing the control plane to automatically setup tunnels and use them efficiently.</p> <p>The following figure demonstrates this concept where servers <code>srv1</code> and <code>srv2</code> are connected to the different switches of the routed fabric, but appear to be on the same broadcast domain.</p>  <p>Now that the DC fabric has a routed underlay, and the loopbacks of the leaf switches are mutually reachable1, we can proceed with the VXLAN based EVPN service configuration.</p> <p>While doing that we will cover the following topics:</p> <ul> <li>VXLAN tunnel interface configuration</li> <li>Network instances of type <code>mac-vrf</code></li> <li>Bridged subinterfaces</li> <li>and BGP EVPN control plane configuration</li> </ul>","title":"Overview"},{"location":"tutorials/l2evpn/evpn/#ibgp-for-evpn","text":"<p>Prior to configuring the overlay services we must enable the EVPN address family for the distribution of EVPN routes among leaf routers of the same tenant. </p> <p>EVPN is enabled using iBGP and typically a Route Reflector (RR), or eBGP. In our example we have only two leafs, so we won't take extra time configuring the iBGP with a spine acting as a Route Reflector, and instead will configure the iBGP between the two leaf switches.</p>  <p>For that iBGP configuration we will create a group called <code>iBGP-overlay</code> which will have the <code>peer-as</code> and <code>local-as</code> set to <code>100</code> to form an iBGP neighborship. The group will also host the same permissive <code>all</code> routing policy, enabled <code>evpn</code> and disabled ipv4-unicast address families.</p> <p>Then for each leaf we add a new BGP neighbor addressed by the remote <code>system0</code> interface address and local system address as the source. Below you will find the pastable snippets with the aforementioned config:</p> leaf1leaf2   <pre><code>enter candidate\n\n/network-instance default protocols bgp\n    group iBGP-overlay {\n        export-policy all\n        import-policy all\n        peer-as 100\n        ipv4-unicast {\n            admin-state disable\n        }\n        evpn {\n            admin-state enable\n        }\n        local-as 100 {\n        }\n        timers {\n            minimum-advertisement-interval 1\n        }\n    }\n\n    neighbor 10.0.0.2 {\n        peer-group iBGP-overlay\n        transport {\n            local-address 10.0.0.1\n        }\n    }\ncommit now\n</code></pre>   <pre><code>enter candidate\n\n/network-instance default protocols bgp\n    group iBGP-overlay {\n        export-policy all\n        import-policy all\n        peer-as 100\n        ipv4-unicast {\n            admin-state disable\n        }\n        evpn {\n            admin-state enable\n        }\n        local-as 100 {\n        }\n        timers {\n            minimum-advertisement-interval 1\n        }\n    }\n\n    neighbor 10.0.0.1 {\n        peer-group iBGP-overlay\n        transport {\n            local-address 10.0.0.2\n        }\n    }\ncommit now\n</code></pre>    <p>Ensure that the iBGP session is established before proceeding any further:</p> <p><pre><code>A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2\n----------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Net-Inst  |   Peer    |   Group   |   Flags   |  Peer-AS  |   State   |  Uptime   | AFI/SAFI  | [Rx/Activ |\n|           |           |           |           |           |           |           |           |   e/Tx]   |\n+===========+===========+===========+===========+===========+===========+===========+===========+===========+\n| default   | 10.0.0.2  | iBGP-     | S         | 100       | establish | 0d:0h:2m: | evpn      | [0/0/0]   |\n|           |           | overlay   |           |           | ed        | 9s        |           |           |\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n</code></pre> Right now, as we don't have any EVPN service created, there are no EVPN routes that are being sent/received, which is indicated in the last column of the table above.</p>","title":"IBGP for EVPN"},{"location":"tutorials/l2evpn/evpn/#access-interfaces","text":"<p>Next we are configuring the interfaces from the leaf switches to the corresponding servers. According to our lab's wiring diagram, interface 1 is connected to the server on both leaf switches:</p>  <p>Configuration of an access interface is nothing special, we already configured leaf-spine interfaces at the fabric configuration stage, so the steps are all familiar. The only detail worth mentioning here is that we have to indicate the type of the subinterface to be <code>bridged</code>, this makes the interfaces only attachable to a network instance of <code>mac-vrf</code> type with MAC learning and layer-2 forwarding enabled.</p> <p>The following config is applied to both leaf switches:</p> <pre><code>enter candidate\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\ncommit now\n</code></pre> <p>As the config snippet shows, we are not using any VLAN classification on the subinterface, our intention is to send untagged frames from the servers.</p>","title":"Access interfaces"},{"location":"tutorials/l2evpn/evpn/#tunnelvxlan-interface","text":"<p>After creating the access sub-interfaces we are proceeding with creation of the VXLAN/Tunnel interfaces. The VXLAN encapsulation in the dataplane allows MAC-VRFs of the same BD to be connected throughout the IP fabric.</p> <p>The SR Linux models VXLAN as a tunnel-interface which has a vxlan-interface within. The tunnel-interface for VXLAN is configured with a name <code>vxlan&lt;N&gt;</code> where <code>N = 0..255</code>.</p> <p>A vxlan-interface is configured under a tunnel-interface. At a minimum, a vxlan-interface must have an index, type, and ingress VXLAN Network Identifier (VNI).</p> <ul> <li>The index can be a number in the range 0-4294967295.</li> <li>The type can be bridged or routed and indicates whether the vxlan-interface can be linked to a mac-vrf (bridged) or ip-vrf (routed).</li> <li>The ingress VNI is the VXLAN Network Identifier that the system looks for in incoming VXLAN packets to classify them to this vxlan-interface and its network-instance. VNI can be in the range of <code>1..16777215</code>.   The VNI is used to find the MAC-VRF where the inner MAC lookup is performed. The egress VNI is not configured and is determined by the imported EVPN routes.   SR Linux requires that the egress VNI (discovered) matches the configured ingress VNI so that two leaf routers attached to the same BD can exchange packets.</li> </ul>  <p>Note</p> <p>The source IP used in the vxlan-interfaces is the IPv4 address of subinterface <code>system0.0</code> in the default network-instance.</p>  <p>The above information translates to a configuration snippet which is applicable both to <code>leaf1</code> and <code>leaf2</code> nodes.</p> <pre><code>enter candidate\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\ncommit now\n</code></pre> <p>To verify the tunnel interface configuration: <pre><code>A:leaf2# show tunnel-interface vxlan-interface brief\n---------------------------------------------------------------------------------\nShow report for vxlan-tunnels\n---------------------------------------------------------------------------------\n+------------------+-----------------+---------+-------------+------------------+\n| Tunnel Interface | VxLAN Interface |  Type   | Ingress VNI | Egress source-ip |\n+==================+=================+=========+=============+==================+\n| vxlan1           | vxlan1.1        | bridged | 1           | 10.0.0.2/32      |\n+------------------+-----------------+---------+-------------+------------------+\n---------------------------------------------------------------------------------\nSummary\n  1 tunnel-interfaces, 1 vxlan interfaces\n  0 vxlan-destinations, 0 unicast, 0 es, 0 multicast, 0 ip\n---------------------------------------------------------------------------------\n</code></pre></p>","title":"Tunnel/VXLAN interface"},{"location":"tutorials/l2evpn/evpn/#mac-vrf","text":"<p>Now it is a turn of MAC-VRF to get configured.</p> <p>The network-instance type <code>mac-vrf</code> functions as a broadcast domain. Each mac-vrf network-instance builds a bridge table composed of MAC addresses that can be learned via the data path on network-instance interfaces, learned via BGP EVPN or provided with static configuration.</p> <p>By associating the access and vxlan interfaces with the mac-vrf we bound them to this network-instance:</p> <pre><code>enter candidate\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n    }\ncommit now\n</code></pre>","title":"MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#server-interfaces","text":"<p>The servers in our fabric do not have any addresses on their <code>eth1</code> interfaces by default. It is time to configure IP addresses on both servers, so that they will be ready to communicate with each other once we complete the EVPN service configuration.</p> <p>By the end of this section, we will have the following addressing scheme complete:</p>  <p>To connect to a shell of a server execute <code>docker exec -it &lt;container-name&gt; bash</code>:</p> srv1srv2   <pre><code>docker exec -it clab-evpn01-srv1 bash\n</code></pre>   <pre><code>docker exec -it clab-evpn01-srv2 bash\n</code></pre>    <p>Within the shell, configure MAC address2 and IPv4 address for the <code>eth1</code> interface according to the diagram above, as with this interface the server is connected to the leaf switch.</p> srv1srv2   <pre><code>ip link set address 00:c1:ab:00:00:01 dev eth1\nip addr add 192.168.0.1/24 dev eth1\n</code></pre>   <pre><code>ip link set address 00:c1:ab:00:00:02 dev eth1\nip addr add 192.168.0.2/24 dev eth1\n</code></pre>    <p>Let's try to ping server2 from server1:</p> <pre><code>bash-5.0# ping 192.168.0.2\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n^C\n--- 192.168.0.2 ping statistics ---\n3 packets transmitted, 0 received, 100% packet loss, time 2028ms\n</code></pre> <p>That failed, expectedly, as our servers connected to different leafs, and those leafs do not yet have a shared broadcast domain. But by just trying to ping the remote party from server 1, we made the <code>srv1</code> interface MAC to get learned by the <code>leaf1</code> mac-vrf network instance:</p> <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n----------------------------------------------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n----------------------------------------------------------------------------------------------------------------------\n+-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+\n|      Address      |       Destination        |   Dest    |  Type  | Active | Aging |       Last Update        |\n|                   |                          |   Index   |        |        |       |                          |\n+===================+==========================+===========+========+========+=======+==========================+\n| 00:C1:AB:00:00:01 | ethernet-1/1.0           | 4         | learnt | true   | 242   | 2021-07-13T17:36:23.000Z |\n+-------------------+--------------------------+-----------+--------+--------+-------+--------------------------+\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    1 Total    1 Active\nTotal Evpn Macs           :    0 Total    0 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    1 Total    1 Active\n----------------------------------------------------------------------------------------------------------------------\n</code></pre>","title":"Server interfaces"},{"location":"tutorials/l2evpn/evpn/#evpn-in-mac-vrf","text":"<p>To advertise the locally learned MACs to the remote leafs we have to configure EVPN in our <code>vrf-1</code> network-instance.</p> <p>EVPN configuration under the mac-vrf network instance will require two configuration containers:</p> <ul> <li><code>bgp-vpn</code> - provides the configuration of the bgp-instances where the route-distinguisher and the import/export route-targets used for the EVPN routes exist.</li> <li><code>bgp-evpn</code> - hosts all the commands required to enable EVPN in the network-instance. At a minimum, a reference to <code>bgp-instance 1</code> is configured, along with the reference to the vxlan-interface and the EVPN Virtual Identifier (EVI).</li> </ul> <p>The following configuration is entered on both leafs:</p> <pre><code>enter candidate\n    /network-instance vrf-1\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\ncommit now\n</code></pre> <p>Once configured, the <code>bgp-vpn</code> instance can be checked to have the RT/RD values set:</p> <pre><code>A:leaf1# show network-instance vrf-1 protocols bgp-vpn bgp-instance 1\n=====================================================================\nNet Instance   : vrf-1\n    bgp Instance 1\n---------------------------------------------------------------------\n        route-distinguisher: 10.0.0.1:111, auto-derived-from-evi\n        export-route-target: target:100:111, manual\n        import-route-target: target:100:111, manual\n=====================================================================\n</code></pre>  <p>VNI to EVI mapping</p> <p>Prior to release 21.11, SR Linux used only VLAN-based Service type of mapping between the VNI and EVI. In this option, a single Ethernet broadcast domain (e.g., subnet) represented by a VNI is mapped to a unique EVI.3</p> <p>Starting from release 21.11 SR Linux supports an interoperability mode in which SR Linux leaf nodes can be attached to VLAN-aware bundle broadcast domains along with other third-party routers.</p>","title":"EVPN in MAC-VRF"},{"location":"tutorials/l2evpn/evpn/#final-configurations","text":"<p>For your convenience, in case you want to jump over the config routines and start with control/data plane verification we provide the resulting configuration4 for all the lab nodes. You can copy paste those snippets to the relevant nodes and proceed with verification tasks.</p>  pastable snippets leaf1leaf2spine1srv1srv2   <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                accept {\n                }\n            }\n        }\n    }\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\n    /network-instance default {\n        interface ethernet-1/49.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 101\n                router-id 10.0.0.1\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 201\n                    ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                group iBGP-overlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 100\n                    ipv4-unicast {\n                        admin-state disable\n                    }\n                    evpn {\n                        admin-state enable\n                    }\n                    local-as 100 {\n                    }\n                    timers {\n                        minimum-advertisement-interval 1\n                    }\n                }\n                neighbor 10.0.0.2 {\n                    admin-state enable\n                    peer-group iBGP-overlay\n                    transport {\n                        local-address 10.0.0.1\n                    }\n                }\n                neighbor 192.168.11.2 {\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\n    }\n\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\n    /interface ethernet-1/49 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.11.1/30 {\n                }\n            }\n        }\n    }\n    /interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.0.1/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre>   <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                accept {\n                }\n            }\n        }\n    }\n    /tunnel-interface vxlan1 {\n        vxlan-interface 1 {\n            type bridged\n            ingress {\n                vni 1\n            }\n        }\n    }\n    /network-instance default {\n        interface ethernet-1/49.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 102\n                router-id 10.0.0.2\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 201\n                    ipv4-unicast {\n                        admin-state enable\n                    }\n                }\n                group iBGP-overlay {\n                    export-policy all\n                    import-policy all\n                    peer-as 100\n                    ipv4-unicast {\n                        admin-state disable\n                    }\n                    evpn {\n                        admin-state enable\n                    }\n                    local-as 100 {\n                    }\n                    timers {\n                        minimum-advertisement-interval 1\n                    }\n                }\n                neighbor 10.0.0.1 {\n                    admin-state enable\n                    peer-group iBGP-overlay\n                    transport {\n                        local-address 10.0.0.2\n                    }\n                }\n                neighbor 192.168.12.2 {\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n    /network-instance vrf-1 {\n        type mac-vrf\n        admin-state enable\n        interface ethernet-1/1.0 {\n        }\n        vxlan-interface vxlan1.1 {\n        }\n        protocols {\n            bgp-evpn {\n                bgp-instance 1 {\n                    admin-state enable\n                    vxlan-interface vxlan1.1\n                    evi 111\n                }\n            }\n            bgp-vpn {\n                bgp-instance 1 {\n                    route-target {\n                        export-rt target:100:111\n                        import-rt target:100:111\n                    }\n                }\n            }\n        }\n    }\n\n    /interface ethernet-1/1 {\n        vlan-tagging true\n        subinterface 0 {\n            type bridged\n            admin-state enable\n            vlan {\n                encap {\n                    untagged {\n                    }\n                }\n            }\n        }\n    }\n    interface ethernet-1/49 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.12.1/30 {\n                }\n            }\n        }\n    }\n    interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.0.2/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre>   <pre><code>enter candidate\n    /routing-policy {\n        policy all {\n            default-action {\n                accept {\n                }\n            }\n        }\n    }\n\n    /network-instance default {\n        interface ethernet-1/1.0 {\n        }\n        interface ethernet-1/2.0 {\n        }\n        interface system0.0 {\n        }\n        protocols {\n            bgp {\n                autonomous-system 201\n                router-id 10.0.1.1\n                group eBGP-underlay {\n                    export-policy all\n                    import-policy all\n                }\n                ipv4-unicast {\n                    admin-state enable\n                }\n                neighbor 192.168.11.1 {\n                    peer-as 101\n                    peer-group eBGP-underlay\n                }\n                neighbor 192.168.12.1 {\n                    peer-as 102\n                    peer-group eBGP-underlay\n                }\n            }\n        }\n    }\n\n    /interface ethernet-1/1 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.11.2/30 {\n                }\n            }\n        }\n    }\n    interface ethernet-1/2 {\n        subinterface 0 {\n            ipv4 {\n                address 192.168.12.2/30 {\n                }\n            }\n        }\n    }\n    interface system0 {\n        admin-state enable\n        subinterface 0 {\n            ipv4 {\n                address 10.0.1.1/32 {\n                }\n            }\n        }\n    }\ncommit now\n</code></pre>   <p>configuring static MAC and IP on the single interface of a server <pre><code>docker exec -it clab-evpn01-srv1 bash\n\nip link set address 00:c1:ab:00:00:01 dev eth1\nip addr add 192.168.0.1/24 dev eth1\n</code></pre></p>   <p>configuring static MAC and IP on the single interface of a server <pre><code>docker exec -it clab-evpn01-srv2 bash\n\nip link set address 00:c1:ab:00:00:02 dev eth1\nip addr add 192.168.0.2/24 dev eth1\n</code></pre></p>","title":"Final configurations"},{"location":"tutorials/l2evpn/evpn/#verification","text":"","title":"Verification"},{"location":"tutorials/l2evpn/evpn/#evpn-imet-routes","text":"<p>When the BGP-EVPN is configured in the mac-vrf instance, the leafs start to exchange EVPN routes, which we can verify with the following commands:</p> <pre><code>A:leaf1# /show network-instance default protocols bgp neighbor 10.0.0.2\n----------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n| Net-Inst  |   Peer    |   Group   |   Flags   |  Peer-AS  |   State   |  Uptime   | AFI/SAFI  | [Rx/Activ |\n|           |           |           |           |           |           |           |           |   e/Tx]   |\n+===========+===========+===========+===========+===========+===========+===========+===========+===========+\n| default   | 10.0.0.2  | iBGP-     | S         | 100       | establish | 0d:0h:2m: | evpn      | [1/1/1]   |\n|           |           | overlay   |           |           | ed        | 9s        |           |           |\n+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+-----------+\n</code></pre> <p>The single route that the leaf1 received/sent is an EVPN Inclusive Multicast Ethernet Tag route (IMET or type 3, RT3).</p> <p>The IMET route is advertised as soon as bgp-evpn is enabled in the MAC-VRF; it has the following purpose:</p> <ul> <li>Auto-discovery of the remote VTEPs attached to the same EVI</li> <li>Creation of a default flooding list in the MAC-VRF so that BUM frames are replicated</li> </ul> <p>The IMET/RT3 routes can be viewed in summary and detailed modes:</p> RT3 summaryRT3 detailed   <pre><code>A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 summary\n----------------------------------------------------------------------------------------------------------------\nShow report for the BGP route table of network-instance \"default\"\n----------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n----------------------------------------------------------------------------------------------------------------\nBGP Router ID: 10.0.0.1      AS: 101      Local AS: 101\n----------------------------------------------------------------------------------------------------------------\nType 3 Inclusive Multicast Ethernet Tag Routes\n+--------+---------------------+------------+---------------------+---------------------+---------------------+\n| Status | Route-distinguisher |   Tag-ID   |    Originator-IP    |      neighbor       |      Next-Hop       |\n+========+=====================+============+=====================+=====================+=====================+\n| u*&gt;    | 10.0.0.2:111        | 0          | 10.0.0.2            | 10.0.0.2            | 10.0.0.2            |\n+--------+---------------------+------------+---------------------+---------------------+---------------------+\n----------------------------------------------------------------------------------------------------------------\n1 Inclusive Multicast Ethernet Tag routes 0 used, 1 valid\n----------------------------------------------------------------------------------------------------------------\n</code></pre>   <pre><code>A:leaf1# /show network-instance default protocols bgp routes evpn route-type 3 detail\n-------------------------------------------------------------------------------------\nShow report for the EVPN routes in network-instance  \"default\"\n-------------------------------------------------------------------------------------\nRoute Distinguisher: 10.0.0.2:111\nTag-ID             : 0\nOriginating router : 10.0.0.2\nneighbor           : 10.0.0.2\nReceived paths     : 1\nPath 1: &lt;Best,Valid,Used,&gt;\n    VNI             : 1\n    Route source    : neighbor 10.0.0.2 (last modified 2m3s ago)\n    Route preference: No MED, LocalPref is 100\n    Atomic Aggr     : false\n    BGP next-hop    : 10.0.0.2\n    AS Path         :  i\n    Communities     : [target:100:111, bgp-tunnel-encap:VXLAN]\n    RR Attributes   : No Originator-ID, Cluster-List is []\n    Aggregation     : None\n    Unknown Attr    : None\n    Invalid Reason  : None\n    Tie Break Reason: none\n--------------------------------------------------------------------------------------\n</code></pre>     Lets capture those routes? <p>Since our lab is launched with containerlab, we can leverage the transparent sniffing of packets that it offers.</p> <p>By capturing on the <code>e1-49</code> interface of the <code>clab-evpn01-leaf1</code> container, we are able to collect all the packets that are flowing between the nodes. Then we simply flap the EVPN instance in the <code>vrf-1</code> network instance to trigger the BGP updates to flow and see them in the live capture.</p> <p>Here is the pcap file with the IMET routes advertisements between <code>leaf1</code> and <code>leaf2</code>.</p>  <p>When the IMET routes from <code>leaf2</code> are imported for <code>vrf-1</code> network-instance, the corresponding multicast VXLAN destinations are added and can be checked with the following command:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table multicast-destinations destination *\n-------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 multicast destinations (flooding-list)\n-------------------------------------------------------------------------------\n+--------------+------------+-------------------+----------------------+\n| VTEP Address | Egress VNI | Destination-index | Multicast-forwarding |\n+==============+============+===================+======================+\n| 10.0.0.2     | 1          | 160078821962      | BUM                  |\n+--------------+------------+-------------------+----------------------+\n-------------------------------------------------------------------------------\nSummary\n  1 multicast-destinations\n-------------------------------------------------------------------------------\n</code></pre> <p>This multicast destination means that BUM frames received on a bridged sub-interface are ingress-replicated to the VTEPs for that EVI as per the table above. For example any ARP traffic will be distributed (ingress-replicated) to the VTEPs from multicast destinations table.</p> <p>As to the unicast destinations there are none so far, and this is because we haven't yet received any MAC/IP RT2 EVPN routes. But before looking into the RT2 EVPN routes, let's zoom into VXLAN tunnels that got built right after we receive the first IMET RT3 routes.</p>","title":"EVPN IMET routes"},{"location":"tutorials/l2evpn/evpn/#vxlan-tunnels","text":"<p>After receiving EVPN routes from the remote leafs with VXLAN encapsulation5, SR Linux creates VXLAN tunnels towards remote VTEP, whose address is received in EVPN IMET routes. The state of a single remote VTEP we have in our lab is shown below from the <code>leaf1</code> switch.</p> <pre><code>A:leaf1# /show tunnel vxlan-tunnel all\n----------------------------------------------------------\nShow report for vxlan-tunnels\n----------------------------------------------------------\n+--------------+--------------+--------------------------+\n| VTEP Address |    Index     |       Last Change        |\n+==============+==============+==========================+\n| 10.0.0.2     | 160078821947 | 2021-07-13T21:13:50.000Z |\n+--------------+--------------+--------------------------+\n1 VXLAN tunnels, 1 active, 0 inactive\n----------------------------------------------------------\n</code></pre> <p>The VXLAN tunnel is built between the <code>vxlan</code> interfaces in the MAC-VRF network instances, which internally use <code>system</code> interfaces of the <code>default</code> network instance as a VTEP:</p>  <p>Once a VTEP is created in the vxlan-tunnel table with a non-zero allocated index6, an entry in the tunnel-table is also created for the tunnel.</p> <pre><code>A:leaf1# /show network-instance default tunnel-table all\n-------------------------------------------------------------------------------------------------------\nShow report for network instance \"default\" tunnel table\n-------------------------------------------------------------------------------------------------------\n+-------------+-----------+-------+-------+--------+------------+----------+--------------------------+\n| IPv4 Prefix |   Owner   | Type  | Index | Metric | Preference | Fib-prog |       Last Update        |\n+=============+===========+=======+=======+========+============+==========+==========================+\n| 10.0.0.2/32 | vxlan_mgr | vxlan | 1     | 0      | 0          | Y        | 2021-07-13T21:13:43.424Z |\n+-------------+-----------+-------+-------+--------+------------+----------+--------------------------+\n-------------------------------------------------------------------------------------------------------\n1 VXLAN tunnels, 1 active, 0 inactive\n</code></pre>","title":"VXLAN tunnels"},{"location":"tutorials/l2evpn/evpn/#evpn-macip-routes","text":"<p>As was mentioned, when the leafs exchanged only EVPN IMET routes they build the BUM flooding tree (aka multicast destinations), but unicast destinations are yet unknown, which is seen in the below output:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination *\n-------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 unicast destinations\n-------------------------------------------------------------------------------\nDestinations\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nEthernet Segment Destinations\n-------------------------------------------------------------------------------\n-------------------------------------------------------------------------------\nSummary\n  0 unicast-destinations, 0 non-es, 0 es\n  0 MAC addresses, 0 active, 0 non-active\n</code></pre> <p>This is due to the fact that no MAC/IP EVPN routes are being advertised yet. If we take a look at the MAC table of the <code>vrf-1</code>, we will see that no local MAC addresses are there, and this is because the servers haven't yet sent any frames towards the leafs7. <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n-------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n-------------------------------------------------------------------------------\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    0 Total    0 Active\nTotal Evpn Macs           :    0 Total    0 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    0 Total    0 Active\n-------------------------------------------------------------------------------\n</code></pre></p> <p>Let's try that ping from <code>srv1</code> towards <code>srv2</code> once again and see what happens:</p> <pre><code>bash-5.0# ping 192.168.0.2\nPING 192.168.0.2 (192.168.0.2) 56(84) bytes of data.\n64 bytes from 192.168.0.2: icmp_seq=1 ttl=64 time=1.28 ms\n64 bytes from 192.168.0.2: icmp_seq=2 ttl=64 time=0.784 ms\n64 bytes from 192.168.0.2: icmp_seq=3 ttl=64 time=0.901 ms\n^C\n--- 192.168.0.2 ping statistics ---\n3 packets transmitted, 3 received, 0% packet loss, time 2013ms\nrtt min/avg/max/mdev = 0.784/0.986/1.275/0.209 ms\n</code></pre> <p>Much better! The dataplane works and we can check that the MAC table in the <code>vrf-1</code> network-instance has been populated with local and EVPN-learned MACs:</p> <pre><code>A:leaf1# show network-instance vrf-1 bridge-table mac-table all\n---------------------------------------------------------------------------------------------------------------------------------------------\nMac-table of network instance vrf-1\n---------------------------------------------------------------------------------------------------------------------------------------------\n+-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+\n|      Address      |            Destination             |   Dest    |   Type    | Active | Aging |            Last Update             |\n|                   |                                    |   Index   |           |        |       |                                    |\n+===================+====================================+===========+===========+========+=======+====================================+\n| 00:C1:AB:00:00:01 | ethernet-1/1.0                     | 4         | learnt    | true   | 240   | 2021-07-18T14:22:55.000Z           |\n| 00:C1:AB:00:00:02 | vxlan-interface:vxlan1.1           | 160078821 | evpn      | true   | N/A   | 2021-07-18T14:22:56.000Z           |\n|                   | vtep:10.0.0.2 vni:1                | 962       |           |        |       |                                    |\n+-------------------+------------------------------------+-----------+-----------+--------+-------+------------------------------------+\nTotal Irb Macs            :    0 Total    0 Active\nTotal Static Macs         :    0 Total    0 Active\nTotal Duplicate Macs      :    0 Total    0 Active\nTotal Learnt Macs         :    1 Total    1 Active\nTotal Evpn Macs           :    1 Total    1 Active\nTotal Evpn static Macs    :    0 Total    0 Active\nTotal Irb anycast Macs    :    0 Total    0 Active\nTotal Macs                :    2 Total    2 Active\n---------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>When traffic is exchanged between <code>srv1</code> and <code>srv2</code>, the MACs are learned on the access bridged sub-interfaces and advertised in EVPN MAC/IP routes (type 2, RT2). The MAC/IP routes are imported, and the MACs programmed in the mac-table.</p> <p>The below output shows the MAC/IP EVPN route that <code>leaf1</code> received from its neighbor. The NLRI information contains the MAC of the <code>srv2</code>:</p> <pre><code>A:leaf1# show network-instance default protocols bgp routes evpn route-type 2 summary\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nShow report for the BGP route table of network-instance \"default\"\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nBGP Router ID: 10.0.0.1      AS: 101      Local AS: 101\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\nType 2 MAC-IP Advertisement Routes\n+-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+\n| Statu |     Route-     |  Tag-ID   |   MAC-address    |   IP-address   |    neighbor    |    Next-Hop    |      VNI       |              ESI              |  MAC Mobility  |\n|   s   | distinguisher  |           |                  |                |                |                |                |                               |                |\n+=======+================+===========+==================+================+================+================+================+===============================+================+\n| u*&gt;   | 10.0.0.2:111   | 0         | 00:C1:AB:00:00:0 | 0.0.0.0        | 10.0.0.2       | 10.0.0.2       | 1              | 00:00:00:00:00:00:00:00:00:00 | -              |\n|       |                |           | 2                |                |                |                |                |                               |                |\n+-------+----------------+-----------+------------------+----------------+----------------+----------------+----------------+-------------------------------+----------------+\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n1 MAC-IP Advertisement routes 1 used, 1 valid\n----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The MAC/IP EVPN routes also triggers the creation of the unicast tunnel destinations which were empty before:</p> <pre><code>A:leaf1# show tunnel-interface vxlan1 vxlan-interface 1 bridge-table unicast-destinations destination *\n---------------------------------------------------------------------------------------------------------------------------------------------\nShow report for vxlan-interface vxlan1.1 unicast destinations\n---------------------------------------------------------------------------------------------------------------------------------------------\nDestinations\n---------------------------------------------------------------------------------------------------------------------------------------------\n+--------------+------------+-------------------+-----------------------------+\n| VTEP Address | Egress VNI | Destination-index | Number MACs (Active/Failed) |\n+==============+============+===================+=============================+\n| 10.0.0.2     | 1          | 160078821962      | 1(1/0)                      |\n+--------------+------------+-------------------+-----------------------------+\n---------------------------------------------------------------------------------------------------------------------------------------------\nEthernet Segment Destinations\n---------------------------------------------------------------------------------------------------------------------------------------------\n---------------------------------------------------------------------------------------------------------------------------------------------\nSummary\n  1 unicast-destinations, 1 non-es, 0 es\n  1 MAC addresses, 1 active, 0 non-active\n-------------------------------------------------------------------------------\n</code></pre>  <p>packet capture</p> <p>The following pcap was captured a moment before <code>srv1</code> started to ping <code>srv2</code> on <code>leaf1</code> interface <code>e1-49</code>.</p> <p>It shows how:</p> <ol> <li>ARP frames were first exchanged using the multicast destination, </li> <li>next the first ICMP request was sent out by <code>leaf1</code> again using the BUM destination, since RT2 routes were not received yet </li> <li>and then the MAC/IP EVPN routes were exchanged triggered by the MACs being learned in the dataplane.</li> <li>after that event, the ICMP Requests and replies were using the unicast destinations, which were created after receiving the MAC/IP EVPN routes.</li> </ol>  <p>This concludes the verification steps, as we have a working data plane connectivity between the servers.</p>   <ol> <li> <p>as was verified before \u21a9</p> </li> <li> <p>containerlab assigns mac addresses to the interfaces with OUI <code>00:C1:AB</code>. We are changing the generated MAC with a more recognizable address, since we want to easily identify MACs in the bridge tables.\u00a0\u21a9</p> </li> <li> <p>Per section 5.1.2 of RFC 8365 \u21a9</p> </li> <li> <p>Easily extracted with doing <code>info &lt;container&gt;</code> where <code>container</code> is <code>routing-policy</code>, <code>network-instance *</code>, <code>interface *</code>, <code>tunnel-interface *</code> \u21a9</p> </li> <li> <p>IMET routes have extended community that conveys the encapsulation type. And for VXLAN EVPN it states VXLAN encap. Check pcap for reference.\u00a0\u21a9</p> </li> <li> <p>If the next hop is not resolved to a route in the default network-instance route-table, the index in the vxlan-tunnel table shows as \u201c0\u201d for the VTEP and no tunnel-table is created.\u00a0\u21a9</p> </li> <li> <p>We did try to ping from <code>srv1</code> to <code>srv2</code> in server interfaces section which triggered MAC-VRF to insert a locally learned MAC into its MAC table, but since then this mac has aged out, and thus the table is empty again.\u00a0\u21a9</p> </li> </ol>","title":"EVPN MAC/IP routes"},{"location":"tutorials/l2evpn/fabric/","text":"<p>Prior to configuring EVPN based overlay, a routing protocol needs to be deployed in the fabric to advertise the reachability of all the leaf VXLAN Termination End Point (VTEP) addresses throughout the IP fabric.</p> <p>With SR Linux, the following routing protocols can be used in the underlay:</p> <ul> <li>ISIS</li> <li>OSPF</li> <li>EBGP</li> </ul> <p>We will use a BGP based fabric design as described in RFC7938 due to its simplicity, scalability, and ease of multi-vendor interoperability.</p>","title":"Fabric configuration"},{"location":"tutorials/l2evpn/fabric/#leaf-spine-interfaces","text":"<p>Let's start with configuring the IP interfaces on the inter-switch links to ensure L3 connectivity is established. According to our lab topology configuration, and using the <code>192.168.xx.0/30</code> network to address the links, we will implement the following underlay addressing design:</p>  <p>On each leaf and spine we will bring up the relevant interface and address its routed subinterface to achieve L3 connectivity.</p> <p>We begin with connecting to the CLI of our nodes via SSH1:</p> <pre><code># connecting to leaf1\nssh admin@clab-evpn01-leaf1\n</code></pre> <p>Then on each node we enter into candidate configuration mode and proceed with the relevant interfaces configuration.</p> <p>Let's witness the step by step process of an interface configuration on a <code>leaf1</code> switch with providing the paste-ables snippets for the rest of the nodes</p> <ol> <li>Enter the <code>candidate</code> configuration mode to make edits to the configuration     <pre><code>Welcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n\n\n--{ running }--[  ]--\nA:leaf1# enter candidate\n</code></pre></li> <li>The prompt will indicate the changed active mode     <pre><code>--{ candidate shared default }--[  ]--\nA:leaf1#                              \n</code></pre></li> <li>Enter into the interface configuration context     <pre><code>--{ candidate shared default }--[  ]--\nA:leaf1# interface ethernet-1/49      \n</code></pre></li> <li>Create a subinterface under the parent interface to configure IPv4 address on it     <pre><code>--{ * candidate shared default }--[ interface ethernet-1/49 ]--\nA:leaf1# subinterface 0                                        \n--{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ]--\nA:leaf1# ipv4 address 192.168.11.1/30                                         \n</code></pre></li> <li>Apply the configuration changes by issuing a <code>commit now</code> command. The changes will be written to the running configuration.     <pre><code>--{ * candidate shared default }--[ interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30 ]--\nA:leaf1# commit now                                                                                        \nAll changes have been committed. Leaving candidate mode.\n</code></pre></li> </ol> <p>Below you will find the relevant configuration snippets2 for leafs and spine of our fabric which you can paste in the terminal while being in candidate mode.</p> leaf1leaf2spine1   <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.1/30 {\n            }\n        }\n    }\n}\n</code></pre>   <pre><code>interface ethernet-1/49 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.12.1/30 {\n            }\n        }\n    }\n}\n</code></pre>   <pre><code>interface ethernet-1/1 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.11.2/30 {\n            }\n        }\n    }\n}\ninterface ethernet-1/2 {\n    subinterface 0 {\n        ipv4 {\n            address 192.168.12.2/30 {\n            }\n        }\n    }\n}\n</code></pre>    <p>Once those snippets are committed to the running configuration with <code>commit now</code> command, we can ensure that the changes have been applied by showing the interface status:</p> <pre><code>--{ + running }--[  ]--                             \nA:spine1# show interface ethernet-1/1               \n====================================================\nethernet-1/1 is up, speed 10G, type None\n  ethernet-1/1.0 is up\n    Network-instance: \n    Encapsulation   : null\n    Type            : routed\n    IPv4 addr    : 192.168.11.2/30 (static, None)\n----------------------------------------------------\n====================================================\n</code></pre> <p>At this moment, the configured interfaces can not be used as they are not yet associated with any network instance. Below we are placing the interfaces to the network-instance <code>default</code> that is created automatically by SR Linux.</p> leaf1 &amp; leaf2spine1   <pre><code>--{ + candidate shared default }--[  ]--                                                   \nA:leaf1# network-instance default interface ethernet-1/49.0                                \n\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/49.0 ]--\nA:leaf1# commit now                                                                        \nAll changes have been committed. Leaving candidate mode.\n</code></pre>   <pre><code>--{ + candidate shared default }--[  ]--                                                  \nA:spine1# network-instance default interface ethernet-1/1.0                               \n\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/1.0 ]--\nA:spine1# /network-instance default interface ethernet-1/2.0                              \n\n--{ +* candidate shared default }--[ network-instance default interface ethernet-1/2.0 ]--\nA:spine2# commit now                                                                      \nAll changes have been committed. Leaving candidate mode.\n</code></pre>    <p>When interfaces are owned by the network-instance <code>default</code>, we can ensure that the basic IP connectivity is working by issuing a ping between the pair of interfaces. For example from <code>spine1</code> to <code>leaf2</code>:</p> <pre><code>--{ + running }--[  ]--                                     \nA:spine1# ping 192.168.12.1 network-instance default        \nUsing network instance default\nPING 192.168.12.1 (192.168.12.1) 56(84) bytes of data.\n64 bytes from 192.168.12.1: icmp_seq=1 ttl=64 time=31.4 ms\n64 bytes from 192.168.12.1: icmp_seq=2 ttl=64 time=10.0 ms\n64 bytes from 192.168.12.1: icmp_seq=3 ttl=64 time=13.1 ms\n64 bytes from 192.168.12.1: icmp_seq=4 ttl=64 time=16.5 ms\n^C\n--- 192.168.12.1 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3003ms\nrtt min/avg/max/mdev = 10.034/17.786/31.409/8.199 ms\n</code></pre>","title":"Leaf-Spine interfaces"},{"location":"tutorials/l2evpn/fabric/#ebgp","text":"<p>Since in this exercise the design decision was to use BGP in the data center, we need to configure BGP peering between the leaf-spine pairs. For that purpose we will use EBGP protocol.</p> <p>The EBGP will make sure of advertising the VTEP IP addresses (loopbacks) across the fabric. The VXLAN VTEPs themselves will be configured later, in this step we will take care of adding the EBGP peering.</p> <p>Let's turn this diagram with the ASN/Router ID allocation into a working configuration:</p>  <p>Here is a breakdown of the steps that are needed to configure EBGP on <code>leaf1</code> towards <code>spine1</code>:</p> <ol> <li> <p>Add BGP protocol to network-instance     Routing protocols are configured under a network-instance context. By adding BGP protocol to the default network-instance we implicitly enable this protocol. <pre><code>--{ + candidate shared default }--[  ]--       \nA:leaf1# network-instance default protocols bgp\n</code></pre></p> </li> <li> <p>Assign Autonomous System Number     The ASN is reported to peers when BGP speaker opens a session towards another router.     According to the diagram above, <code>leaf1</code> has ASN 101. <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# autonomous-system 101\n</code></pre></p> </li> <li> <p>Assign Router ID     This is the BGP identifier reported to peers when this network-instance opens a BGP session towards another router.     Leaf1 has a router-id of 10.0.0.1.     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# router-id 10.0.0.1\n</code></pre></p> </li> <li> <p>Enable AF     Enable all address families that should be enabled globally as a default for all peers of the BGP instance.     When you later configure individual neighbors or groups, you can override the enabled families at those levels.     For the sake of IPv4 loopbacks advertisement, we only need to enable <code>ipv4-unicast</code> address family:     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# ipv4-unicast admin-state enable\n</code></pre></p> </li> <li> <p>Create export/import policies     The export/import policy is required for an EBGP peer to advertise and install routes.     The policy named <code>all</code> that we create below will be used both as an import and export policy, effectively allowing all routes to be advertised and received4.  </p> <p>The routing policies are configured at <code>/routing-policy</code> context, so first, we switch to it from the current <code>bgp</code> context: <pre><code>--{ * candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# /routing-policy                                                      \n\n--{ * candidate shared default }--[ routing-policy ]--                        \nA:leaf1#\n</code></pre> Now that we are in the right context, we can paste the policy definition: <pre><code>--{ +* candidate shared default }--[ routing-policy ]--\nA:leaf1# info\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n</code></pre></p> </li> <li> <p>Create peer-group config     A peer group should include sessions that have a similar or almost identical configuration.     In this example, the peer group is named <code>eBGP-underlay</code> since it will be used to enable underlay routing between the leafs and spines.     New groups are administratively enabled by default.</p> <p>First, we come back to the bgp context from the routing-policy context: <pre><code>--{ * candidate shared default }--[ routing-policy ]--\nA:leaf1# /network-instance default protocols bgp      \n\n--{ * candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1#\n</code></pre> Now create the peer group. The common group configuration includes the <code>peer-as</code> and <code>export-policy</code> statements. <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# group eBGP-underlay\n\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# peer-as 201                                                                               \n\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# export-policy all\n\n--{ +* candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# import-policy all\n</code></pre></p> </li> <li> <p>Configure neighbor     Configure the BGP session with <code>spine1</code>. In this example, <code>spine1</code> is reachable through the <code>ethernet-1/49.0</code> subinterface. On this subnet, <code>spine1</code> has the IPv4 address <code>192.168.11.2</code>.     In this minimal configuration example, the only required configuration for the neighbor is its association with the group <code>eBGP-underlay</code> that was previously created.     New neighbors are administratively enabled by default.     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# neighbor 192.168.11.2 peer-group eBGP-underlay\n</code></pre></p> </li> <li> <p>Commit configuration     It seems like the EBGP config has been sorted out. Let's see what we have in our candidate datastore so far.     Regardless of which context you are currently in, you can see the diff against the baseline config by doing <code>diff /</code> <pre><code>--{ * candidate shared default }--[ network-instance default protocols bgp group eBGP-underlay ]--\nA:leaf1# diff /                                                                                   \n    network-instance default {\n        protocols {\n+             bgp {\n+                 autonomous-system 101\n+                 router-id 10.0.0.1\n+                 group eBGP-underlay {\n+                     export-policy all\n+                     import-policy all\n+                     peer-as 201\n+                 }\n+                 ipv4-unicast {\n+                     admin-state enable\n+                 }\n+                 neighbor 192.168.11.2 {\n+                     peer-group eBGP-underlay\n+                 }\n+             }\n        }\n    }\n+     routing-policy {\n+         policy all {\n+             default-action {\n+                 accept {\n+                 }\n+             }\n+         }\n+     }\n</code></pre>     That is what we've added in all those steps above, everything looks OK, so we are good to commit the configuration.     <pre><code>--{ +* candidate shared default }--[ network-instance default protocols bgp ]--\nA:leaf1# commit now\n</code></pre></p> </li> </ol> <p>EBGP configuration on <code>leaf2</code> and <code>spine1</code> is almost a twin of the one we did for <code>leaf1</code>. Here is a copy-paste-able3 config snippets for all of the nodes:</p> leaf1leaf2spine1   <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 101\n            router-id 10.0.0.1\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n                peer-as 201\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.11.2 {\n                peer-group eBGP-underlay\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n}\n</code></pre>   <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 102\n            router-id 10.0.0.2\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n                peer-as 201\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.12.2 {\n                peer-group eBGP-underlay\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n}\n</code></pre>   <p>Spine configuration is a bit different, in a way that <code>peer-as</code> is specified under the neighbor context, and not the group one. <pre><code>network-instance default {\n    protocols {\n        bgp {\n            autonomous-system 201\n            router-id 10.0.1.1\n            group eBGP-underlay {\n                export-policy all\n                import-policy all\n            }\n            ipv4-unicast {\n                admin-state enable\n            }\n            neighbor 192.168.11.1 {\n                peer-group eBGP-underlay\n                peer-as 101\n            }\n            neighbor 192.168.12.1 {\n                peer-group eBGP-underlay\n                peer-as 102\n            }\n        }\n    }\n}\nrouting-policy {\n    policy all {\n        default-action {\n            accept {\n            }\n        }\n    }\n}\n</code></pre></p>","title":"EBGP"},{"location":"tutorials/l2evpn/fabric/#loopbacks","text":"<p>As we will create a IBGP based EVPN control plane at a later stage, we need to configure loopback addresses for our leaf devices so that they can build an IBGP peering over those interfaces.</p> <p>In the context of the VXLAN data plane, a special kind of a loopback needs to be created - <code>system0</code> interface.</p>  <p>Info</p> <p>The <code>system0.0</code> interface hosts the loopback address used to originate and typically terminate VXLAN packets. This address is also used by default as the next-hop of all EVPN routes.</p>  <p>Configuration of the <code>system0</code> interface is exactly the same as for the regular interfaces. The IPv4 addresses we assign to <code>system0</code> interfaces will match the Router-ID of a given BGP speaker.</p> leaf1leaf2spine1   <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.0.1/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre>   <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.0.2/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre>   <pre><code>/interface system0 {\n    admin-state enable\n    subinterface 0 {\n        ipv4 {\n            address 10.0.1.1/32 {\n            }\n        }\n    }\n}\n/network-instance default {\n    interface system0.0 {\n    }\n}\n</code></pre>","title":"Loopbacks"},{"location":"tutorials/l2evpn/fabric/#verification","text":"<p>As stated in the beginning of this section, the VXLAN VTEPs need to be advertised throughout the DC fabric. The <code>system0</code> interfaces we just configured are the VTEPs and they should be advertised via EBGP peering established before. The following verification commands can help ensure that.</p>","title":"Verification"},{"location":"tutorials/l2evpn/fabric/#bgp-status","text":"<p>The first thing worth verifying is that BGP protocol is enabled and operational on all devices. Below is an example of a BGP summary command issued on <code>leaf1</code>:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp summary\n-------------------------------------------------------------\nBGP is enabled and up in network-instance \"default\"\nGlobal AS number  : 101\nBGP identifier    : 10.0.0.1\n-------------------------------------------------------------\n  Total paths               : 3\n  Received routes           : 3\n  Received and active routes: None\n  Total UP peers            : 1\n  Configured peers          : 1, 0 are disabled\n  Dynamic peers             : None\n-------------------------------------------------------------\nDefault preferences\n  BGP Local Preference attribute: 100\n  EBGP route-table preference   : 170\n  IBGP route-table preference   : 170\n-------------------------------------------------------------\nWait for FIB install to advertise: True\nSend rapid withdrawals           : disabled\n-------------------------------------------------------------\nIpv4-unicast AFI/SAFI\n    Received routes               : 3\n    Received and active routes    : None\n    Max number of multipaths      : 1, 1\n    Multipath can transit multi AS: True\n-------------------------------------------------------------\nIpv6-unicast AFI/SAFI\n    Received routes               : None\n    Received and active routes    : None\n    Max number of multipaths      : 1,1\n    Multipath can transit multi AS: True\n-------------------------------------------------------------\nEVPN-unicast AFI/SAFI\n    Received routes               : None\n    Received and active routes    : None\n    Max number of multipaths      : N/A\n    Multipath can transit multi AS: N/A\n-------------------------------------------------------------\n</code></pre>","title":"BGP status"},{"location":"tutorials/l2evpn/fabric/#bgp-neighbor-status","text":"<p>Equally important is the neighbor summary status that we can observe with the following:</p> <pre><code>--{ + running }--[  ]--\nA:spine1# show network-instance default protocols bgp neighbor\n----------------------------------------------------------------------------------------------------------------------------------------------\nBGP neighbor summary for network-instance \"default\"\nFlags: S static, D dynamic, L discovered by LLDP, B BFD enabled, - disabled, * slow\n----------------------------------------------------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------------------------------------------------\n+----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+\n|    Net-Inst    |         Peer          |     Group      | Flag | Peer-AS |    State    |   Uptime    | AFI/SAFI  |    [Rx/Active/Tx]     |\n|                |                       |                |  s   |         |             |             |           |                       |\n+================+=======================+================+======+=========+=============+=============+===========+=======================+\n| default        | 192.168.11.1          | eBGP-underlay  | S    | 101     | established | 0d:18h:20m: | ipv4-unic | [2/1/4]               |\n|                |                       |                |      |         |             | 49s         | ast       |                       |\n| default        | 192.168.12.1          | eBGP-underlay  | S    | 102     | established | 0d:18h:20m: | ipv4-unic | [2/1/4]               |\n|                |                       |                |      |         |             | 9s          | ast       |                       |\n+----------------+-----------------------+----------------+------+---------+-------------+-------------+-----------+-----------------------+\n----------------------------------------------------------------------------------------------------------------------------------------------\nSummary:\n2 configured neighbors, 2 configured sessions are established,0 disabled peers\n0 dynamic peers\n</code></pre> <p>With this command we can ensure that the ipv4-unicast routes are exchanged between the BGP peers and all the sessions are in established state.</p>","title":"BGP neighbor status"},{"location":"tutorials/l2evpn/fabric/#receivedadvertised-routes","text":"<p>The reason we configured EBGP in the fabric's the underlay is to advertise the VXLAN tunnel endpoints - <code>system0</code> interfaces. In the below output we verify that <code>leaf1</code> advertises the prefix of <code>system0</code> (<code>10.0.0.1/32</code>) interface towards its EBGP <code>spine1</code> peer: <pre><code>--{ + running }--[  ]--\nA:leaf1# show network-instance default protocols bgp neighbor 192.168.11.2 advertised-rou\ntes ipv4\n-----------------------------------------------------------------------------------------\nPeer        : 192.168.11.2, remote AS: 201, local AS: 101\nType        : static\nDescription : None\nGroup       : eBGP-underlay\n-----------------------------------------------------------------------------------------\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-------------------------------------------------------------------------------------+\n|    Network        Next Hop       MED     LocPref           AsPath           Origin  |\n+=====================================================================================+\n| 10.0.0.1/32      192.168.11.      -        100     [101]                       i    |\n|                  1                                                                  |\n| 192.168.11.0/3   192.168.11.      -        100     [101]                       i    |\n| 0                1                                                                  |\n+-------------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------\n2 advertised BGP routes\n-----------------------------------------------------------------------------------------\n</code></pre></p> <p>On the far end of the fabric, <code>leaf2</code> receives both the <code>leaf1</code> and <code>spine1</code> system interface prefixes:</p> <pre><code>--{ + running }--[  ]--\nA:leaf2# show network-instance default protocols bgp neighbor 192.168.12.2 received-route\ns ipv4\n-----------------------------------------------------------------------------------------\nPeer        : 192.168.12.2, remote AS: 201, local AS: 102\nType        : static\nDescription : None\nGroup       : eBGP-underlay\n-----------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+-----------------------------------------------------------------------------------+\n|  Status      Network    Next Hop       MED       LocPref     AsPath      Origin   |\n+===================================================================================+\n|    u*&gt;      10.0.0.1/   192.168.1       -          100      [201,           i     |\n|             32          2.2                                 101]                  |\n|    u*&gt;      10.0.1.1/   192.168.1       -          100      [201]           i     |\n|             32          2.2                                                       |\n|    u*&gt;      192.168.1   192.168.1       -          100      [201]           i     |\n|             1.0/30      2.2                                                       |\n|     *       192.168.1   192.168.1       -          100      [201]           i     |\n|             2.0/30      2.2                                                       |\n+-----------------------------------------------------------------------------------+\n-----------------------------------------------------------------------------------------\n4 received BGP routes : 3 used 4 valid\n-----------------------------------------------------------------------------------------\n</code></pre>","title":"Received/Advertised routes"},{"location":"tutorials/l2evpn/fabric/#route-table","text":"<p>The last stop in the control plane verification ride would be to check if the remote loopback prefixes were installed in the <code>default</code> network-instance where we expect them to be:</p> <pre><code>--{ running }--[  ]--\nA:leaf1# show network-instance default route-table ipv4-unicast summary\n-----------------------------------------------------------------------------------------------------------------------------------\nIPv4 Unicast route table of network instance default\n-----------------------------------------------------------------------------------------------------------------------------------\n+-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+\n|     Prefix      |  ID   | Route Type |     Route Owner      |      Best/Fib-       |  Metric  |  Pref   | Next-hop  | Next-hop  |\n|                 |       |            |                      |     status(slot)     |          |         |  (Type)   | Interface |\n+=================+=======+============+======================+======================+==========+=========+===========+===========+\n| 10.0.0.1/32     | 3     | host       | net_inst_mgr         | True/success         | 0        | 0       | None      | None      |\n|                 |       |            |                      |                      |          |         | (extract) |           |\n| 10.0.0.2/32     | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n| 10.0.1.1/32     | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n| 192.168.11.0/30 | 1     | local      | net_inst_mgr         | True/success         | 0        | 0       | 192.168.1 | ethernet- |\n|                 |       |            |                      |                      |          |         | 1.1       | 1/49.0    |\n|                 |       |            |                      |                      |          |         | (direct)  |           |\n| 192.168.11.1/32 | 1     | host       | net_inst_mgr         | True/success         | 0        | 0       | None      | None      |\n|                 |       |            |                      |                      |          |         | (extract) |           |\n| 192.168.11.3/32 | 1     | host       | net_inst_mgr         | True/success         | 0        | 0       | None (bro | None      |\n|                 |       |            |                      |                      |          |         | adcast)   |           |\n| 192.168.12.0/30 | 0     | bgp        | bgp_mgr              | True/success         | 0        | 170     | 192.168.1 | None      |\n|                 |       |            |                      |                      |          |         | 1.2 (indi |           |\n|                 |       |            |                      |                      |          |         | rect)     |           |\n+-----------------+-------+------------+----------------------+----------------------+----------+---------+-----------+-----------+\n-----------------------------------------------------------------------------------------------------------------------------------\n7 IPv4 routes total\n7 IPv4 prefixes with active routes\n0 IPv4 prefixes with active ECMP routes\n-----------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Both <code>leaf2</code> and <code>spine1</code> prefixes are found in the route table of network-instance <code>default</code> and the <code>bgp_mgr</code> is the owner of those prefixes, which means that they have been added to the route-table by the BGP app.</p>","title":"Route table"},{"location":"tutorials/l2evpn/fabric/#dataplane","text":"<p>To finish the verification process let's ensure that the datapath is indeed working, and the VTEPs on both leafs can reach each other via the routed fabric underlay.</p> <p>For that we will use the <code>ping</code> command with src/dst set to loopback addresses:</p> <pre><code>--{ running }--[  ]--\nA:leaf1# ping -I 10.0.0.1 network-instance default 10.0.0.2\nUsing network instance default\nPING 10.0.0.2 (10.0.0.2) from 10.0.0.1 : 56(84) bytes of data.\n64 bytes from 10.0.0.2: icmp_seq=1 ttl=63 time=17.5 ms\n64 bytes from 10.0.0.2: icmp_seq=2 ttl=63 time=12.2 ms\n</code></pre> <p>Perfect, the VTEPs are reachable and the fabric underlay is properly configured. We can proceed with EVPN service configuration!</p>","title":"Dataplane"},{"location":"tutorials/l2evpn/fabric/#resulting-configs","text":"<p>Below you will find aggregated configuration snippets which contain the entire fabric configuration we did in the steps above. Those snippets are in the flat format and were extracted with <code>info flat</code> command.</p>  <p>Note</p> <p><code>enter candidate</code> and <code>commit now</code> commands are part of the snippets, so it is possible to paste them right after you logged into the devices as well as the changes will get committed to running config.</p>  leaf1leaf2spine1   <pre><code>enter candidate\n\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/49\nset / interface ethernet-1/49 subinterface 0\nset / interface ethernet-1/49 subinterface 0 ipv4\nset / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.11.1/30\n\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.1/32\n\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/49.0\nset / network-instance default interface system0.0\n\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action accept\n\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 101\nset / network-instance default protocols bgp router-id 10.0.0.1\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp group eBGP-underlay peer-as 201\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.11.2\nset / network-instance default protocols bgp neighbor 192.168.11.2 peer-group eBGP-underlay\n\ncommit now\n</code></pre>   <pre><code>enter candidate\n\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/49\nset / interface ethernet-1/49 subinterface 0\nset / interface ethernet-1/49 subinterface 0 ipv4\nset / interface ethernet-1/49 subinterface 0 ipv4 address 192.168.12.1/30\n\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.2/32\n\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/49.0\nset / network-instance default interface system0.0\n\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action accept\n\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 102\nset / network-instance default protocols bgp router-id 10.0.0.2\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp group eBGP-underlay peer-as 201\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.12.2\nset / network-instance default protocols bgp neighbor 192.168.12.2 peer-group eBGP-underlay\n\ncommit now\n</code></pre>   <pre><code>enter candidate\n\n# configuration of the physical interface and its subinterface\nset / interface ethernet-1/1\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.11.2/30\nset / interface ethernet-1/2\nset / interface ethernet-1/2 subinterface 0\nset / interface ethernet-1/2 subinterface 0 ipv4\nset / interface ethernet-1/2 subinterface 0 ipv4 address 192.168.12.2/30\n\n# system interface configuration\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.1.1/32\n\n# associating interfaces with net-ins default\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface ethernet-1/2.0\nset / network-instance default interface system0.0\n\n# routing policy\nset / routing-policy\nset / routing-policy policy all\nset / routing-policy policy all default-action\nset / routing-policy policy all default-action accept\n\n# BGP configuration\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp autonomous-system 201\nset / network-instance default protocols bgp router-id 10.0.1.1\nset / network-instance default protocols bgp group eBGP-underlay\nset / network-instance default protocols bgp group eBGP-underlay export-policy all\nset / network-instance default protocols bgp group eBGP-underlay import-policy all\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.11.1\nset / network-instance default protocols bgp neighbor 192.168.11.1 peer-as 101\nset / network-instance default protocols bgp neighbor 192.168.11.1 peer-group eBGP-underlay\nset / network-instance default protocols bgp neighbor 192.168.12.1\nset / network-instance default protocols bgp neighbor 192.168.12.1 peer-as 102\nset / network-instance default protocols bgp neighbor 192.168.12.1 peer-group eBGP-underlay\n\ncommit now\n</code></pre>      <ol> <li> <p>default SR Linux credentials are <code>admin:admin</code>.\u00a0\u21a9</p> </li> <li> <p>the snippets were extracted with <code>info interface ethernet-1/x</code> command issued in running mode.\u00a0\u21a9</p> </li> <li> <p>you can paste those snippets right after you do <code>enter candidate</code> \u21a9</p> </li> <li> <p>a more practical import/export policy would only export/import the loopback prefixes from leaf nodes. The spine nodes would export/import only the bgp-owned routes, as services are not typically present on the spines.\u00a0\u21a9</p> </li> </ol>","title":"Resulting configs"},{"location":"tutorials/l2evpn/intro/","text":"Tutorial name L2 EVPN-VXLAN with SR Linux   Lab components 3 SR Linux nodes   Resource requirements  2vCPU  4 GB   Containerlab topology file evpn01.clab.yml   Lab name evpn01   Packet captures EVPN IMET routes exchange, RT2 routes exchange with ICMP in datapath   Main ref documents RFC 7432 - BGP MPLS-Based Ethernet VPNRFC 8365 - A Network Virtualization Overlay Solution Using Ethernet VPN (EVPN)Nokia 7220 SR Linux Advanced Solutions GuideNokia 7220 SR Linux EVPN-VXLAN Guide   Version information1 <code>containerlab:0.15.4</code>, <code>srlinux:21.6.1-250</code>, <code>docker-ce:20.10.2</code>    <p>Ethernet Virtual Private Network (EVPN) is a standard technology in multi-tenant Data Centers (DCs) and provides a control plane framework for many functions. In this tutorial we will configure a VXLAN based Layer 2 EVPN service3 in a tiny CLOS fabric and at the same get to know SR Linux better!</p> <p>The DC fabric that we will build for this tutorial consists of the two leaf switches (acting as Top-Of-Rack) and a single spine:</p>  <p>The two servers are connected to the leafs via an L2 interface. Service-wise the servers will appear to be on the same L2 network by means of the deployed EVPN Layer 2 service.</p>  <p>The tutorial will consist of the following major parts:</p> <ul> <li>Fabric configuration - here we will configure the routing protocol in the underlay of a fabric to advertise the Virtual Tunnel Endpoints (VTEP) of the leaf switches.</li> <li>EVPN configuration - this chapter is dedicated to the EVPN service configuration and validation.</li> </ul>","title":"Introduction"},{"location":"tutorials/l2evpn/intro/#lab-deployment","text":"<p>To let you follow along the configuration steps of this tutorial we created a lab that you can deploy on any Linux VM:</p> <p>The containerlab file that describes the lab topology is referenced below in full:</p> <pre><code>name: evpn01\n\ntopology:\n  kinds:\n    srl:\n      image: ghcr.io/nokia/srlinux\n    linux:\n      image: ghcr.io/hellt/network-multitool\n\n  nodes:\n    leaf1:\n      kind: srl\n      type: ixrd2\n    leaf2:\n      kind: srl\n      type: ixrd2\n    spine1:\n      kind: srl\n      type: ixrd3\n    srv1:\n      kind: linux\n    srv2:\n      kind: linux\n\n  links:\n    # inter-switch links\n    - endpoints: [\"leaf1:e1-49\", \"spine1:e1-1\"]\n    - endpoints: [\"leaf2:e1-49\", \"spine1:e1-2\"]\n    # server links\n    - endpoints: [\"srv1:eth1\", \"leaf1:e1-1\"]\n    - endpoints: [\"srv2:eth1\", \"leaf2:e1-1\"]\n</code></pre> <p>Save2 the contents of this file under <code>evpn01.clab.yml</code> name and you are ready to deploy: <pre><code>$ containerlab deploy -t evpn01.clab.yml\nINFO[0000] Parsing &amp; checking topology file: evpn01.clab.yml \nINFO[0000] Creating lab directory: /root/learn.srlinux.dev/clab-evpn01 \nINFO[0000] Creating root CA                             \nINFO[0001] Creating container: srv2                  \nINFO[0001] Creating container: srv1                  \nINFO[0001] Creating container: leaf2                    \nINFO[0001] Creating container: spine1                   \nINFO[0001] Creating container: leaf1                    \nINFO[0002] Creating virtual wire: leaf1:e1-49 &lt;--&gt; spine1:e1-1 \nINFO[0002] Creating virtual wire: srv2:eth1 &lt;--&gt; leaf2:e1-1 \nINFO[0002] Creating virtual wire: leaf2:e1-49 &lt;--&gt; spine1:e1-2 \nINFO[0002] Creating virtual wire: srv1:eth1 &lt;--&gt; leaf1:e1-1 \nINFO[0003] Writing /etc/hosts file                      \n\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n| # |        Name        | Container ID |              Image              | Kind  | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n| 1 | clab-evpn01-leaf1  | 4b81c65af558 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.7/24 | 2001:172:20:20::7/64 |\n| 2 | clab-evpn01-leaf2  | de000e791dd6 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.8/24 | 2001:172:20:20::8/64 |\n| 3 | clab-evpn01-spine1 | 231fd97d7e33 | ghcr.io/nokia/srlinux           | srl   |       | running | 172.20.20.6/24 | 2001:172:20:20::6/64 |\n| 4 | clab-evpn01-srv1   | 3a2fa1e6e9f5 | ghcr.io/hellt/network-multitool | linux |       | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 5 | clab-evpn01-srv2   | fb722453d715 | ghcr.io/hellt/network-multitool | linux |       | running | 172.20.20.5/24 | 2001:172:20:20::5/64 |\n+---+--------------------+--------------+---------------------------------+-------+-------+---------+----------------+----------------------+\n</code></pre></p> <p>A few seconds later containerlab finishes the deployment with providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers and those names can be used to reach the nodes, for example to connect to the SSH of <code>leaf1</code>:</p> <pre><code># default credentials admin:admin\nssh admin@clab-evpn01-leaf1\n</code></pre> <p>With the lab deployed we are ready to embark on our learn-by-doing EVPN configuration journey!</p>  <p>Note</p> <p>We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough4 details to survive in the configuration waters we are about to get.</p>    <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work, but if they don't, please pin the version to the mentioned ones.\u00a0\u21a9</p> </li> <li> <p>Or download it with <code>curl -LO https://github.com/srl-labs/learn-srlinux/blob/master/labs/evpn01.clab.yml</code> \u21a9</p> </li> <li> <p>Per RFC 8365 &amp; RFC 7432 \u21a9</p> </li> <li> <p>For a complete documentation coverage don't hesitate to visit our documentation portal.\u00a0\u21a9</p> </li> </ol>","title":"Lab deployment"},{"location":"tutorials/l2evpn/summary/","text":"<p>Layer 2 EVPN services with VXLAN dataplane are very common in multi-tenant data centers. In this tutorial we walked through every step that is needed to configure a basic Layer 2 EVPN with VXLAN dataplane service deployed on SR Linux switches:</p> <ul> <li>IP fabric config using eBGP in the underlay</li> <li>EVPN service config on leaf switches with the control and data plane verification</li> </ul> <p>The highly detailed configuration &amp; verification steps helped us achieve the goal of creating an overlay Layer 2 broadcast domain for the two servers in our topology. So that the high level service diagram transformed into a detailed map of configuration constructs and instances.</p>  <p>During the verification phases we collected the following packet captures to prove the control/data plane behavior:</p> <ul> <li>Exchange of the IMET/RT3 EVPN routes. IMET/RT3 routes are the starting point in the L2 EVPN-VXLAN services, as they are used to dynamically discover the VXLAN VTEPs participating in the same EVI.</li> <li>Exchange of MAC-IP/RT2 EVPN routes which convey the MAC information of the attached servers. These routes are used to create unicast tunnel destinations that the dataplane frames will use.</li> </ul>  <p>Info</p> <p>The more advanced EVPN topics listed below will be covered in separate tutorials:</p> <ul> <li>EVPN L2 multi-homing</li> <li>MAC mobility</li> <li>MAC duplication and loop protection</li> </ul>","title":"Summary"},{"location":"tutorials/mpls/mpls-ldp/intro/","text":"Tutorial name LDP-based MPLS core   Lab components 3 Nokia SR Linux nodes   Resource requirements  2vCPU  4 GB   Containerlab topology file mpls-ldp.clab.yml   Packet captures \u00b7 LDP neighborship\u00b7 MPLS encapsulation   Main ref documents \u00b7 RFC 5036 - LDP Specification\u00b7 Nokia SR Linux MPLS Guide   Version information1 <code>containerlab:0.24.1</code>, <code>srlinux:21.11.2</code>, <code>docker-ce:20.10.2</code>    <p>Multiprotocol Label Switching (MPLS) is a label switching technology that provides the ability to set up connection-oriented paths over a connection-less IP network. MPLS facilitates network traffic flow and provides a mechanism to engineer network traffic patterns independently from routing tables. MPLS sets up a specific path for a sequence of packets. The packets are identified by a label stack inserted into each packet.</p> <p>This short tutorial will guide you through the steps required to build an LDP-based MPLS core consisting of three SR Linux routers. LDP-based MPLS tunnels are commonly used to enable BGP-free core network.</p> <p>The topology we will use for this interactive tutorial is dead simple - three routers connected in a point-to-point fashion:</p>  <p>The MPLS-enabled core will be formed with <code>srl1</code> and <code>srl3</code> acting as Label Edge Routers (LER) and <code>srl2</code> as Label Switch Router (LSR). The loopback <code>lo0</code> interfaces configured on LERs will emulate clients talking to each other via an established MPLS tunnel.</p>  <p>The tutorial will consist of the following configuration parts:</p> <ul> <li>Core routing - configuring interfaces, network instances and IS-IS IGP protocol.</li> <li>LDP-based MPLS - configuring LDP and running control plane and data plane verification steps.</li> </ul>","title":"Introduction"},{"location":"tutorials/mpls/mpls-ldp/intro/#lab-deployment","text":"<p>The tutorial is augmented with the containerlab-based lab so that you can perform all the steps we do here. The clab file describing the topology looks like follows:</p> <pre><code>name: mpls-ldp\nprefix: \"\"\n\ntopology:\n  defaults:\n    kind: srl\n  kinds:\n    srl:\n      image: ghcr.io/nokia/srlinux:21.11.3\n      type: ixr6 # (1)!\n  nodes:\n    srl1:\n    srl2:\n    srl3:\n\n  links:\n    - endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n    - endpoints: [\"srl2:e1-2\", \"srl3:e1-1\"]\n</code></pre> <ol> <li>Pay attention to the HW type we specify in the clab file. MPLS is only available on ixr6 and ixr10 platforms at the time of this writing. IXR-6/10 chassis will require a license since 22.3 release of SR Linux.</li> </ol> <p>Save2 the contents of this file under <code>mpls-ldp.clab.yml</code> name, and you are ready to deploy: <pre><code>$ containerlab deploy -t clab-ldp.clab.yml\n\n# output omitted for brevity\n\n+---+------+--------------+-----------------------+------+---------+----------------+----------------------+\n| # | Name | Container ID |         Image         | Kind |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+------+--------------+-----------------------+------+---------+----------------+----------------------+\n| 1 | srl1 | 7ba522099fd9 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n| 2 | srl2 | a86c7ce3db59 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n| 3 | srl3 | e87ffaf33111 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n+---+------+--------------+-----------------------+------+---------+----------------+----------------------+\n</code></pre></p> <p>A few seconds later, containerlab finishes the deployment by providing a summary table that outlines connection details of the deployed nodes. In the \"Name\" column we have the names of the deployed containers which can be used to reach the nodes. For example to connect to the SSH server of <code>srl1</code>:</p> <pre><code># default credentials admin:admin\nssh admin@srl1\n</code></pre> <p>With the lab deployed, we are ready to embark on our learn-by-doing LDP-based MPLS configuration journey!</p>  <p>Note</p> <p>We advise the newcomers not to skip the SR Linux basic concepts chapter as it provides just enough3 details to survive in the configuration waters we are about to get.</p>    <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> <li> <p>Or download it with <code>curl -LO https://github.com/srl-labs/learn-srlinux/blob/main/labs/mpls-ldp/mpls-ldp.clab.yml</code> \u21a9</p> </li> <li> <p>For complete documentation coverage, check the official documentation.\u00a0\u21a9</p> </li> </ol>","title":"Lab deployment"},{"location":"tutorials/mpls/mpls-ldp/ldp/","text":"<p>LDP is a protocol defined for distributing labels. It is the set of procedures and messages by which Label Switched Routers (LSRs) establish Label Switched Paths (LSPs) through a network by mapping network-layer routing information directly to data-link layer switched paths. These LSPs may have an endpoint at a directly attached neighbor (comparable to IP hop-by-hop forwarding), or may have an endpoint at a network egress node, enabling switching via all intermediary nodes.</p> <p>This chapter focuses on LDP configuration with verification steps to ensure that LSPs are set up and traffic is properly encapsulated.</p>","title":"LDP"},{"location":"tutorials/mpls/mpls-ldp/ldp/#mpls-label-manager","text":"<p>SR Linux features an MPLS label manager process that shares the MPLS label space among client applications that require MPLS labels; these applications include static MPLS forwarding and LDP.</p> <p>LDP must be configured with a reference to a predefined range of labels, called a label block. A label block configuration includes a start-label value and an end-label value. LDP requires a dynamic, non-shared label block.</p> <p>Although it is absolutely fine to configure the same label block on all the nodes, we will configure each device with a distinctive range for readability.</p> srl1srl2srl3   <pre><code>enter candidate\n\nset / system mpls\nset / system mpls label-ranges\nset / system mpls label-ranges dynamic D1\nset / system mpls label-ranges dynamic D1 start-label 100\nset / system mpls label-ranges dynamic D1 end-label 199\n\ncommit save\n</code></pre>   <pre><code>enter candidate\n\nset / system mpls\nset / system mpls label-ranges\nset / system mpls label-ranges dynamic D1\nset / system mpls label-ranges dynamic D1 start-label 200\nset / system mpls label-ranges dynamic D1 end-label 299\n\ncommit save\n</code></pre>   <pre><code>enter candidate\n\nset / system mpls\nset / system mpls label-ranges\nset / system mpls label-ranges dynamic D1\nset / system mpls label-ranges dynamic D1 start-label 300\nset / system mpls label-ranges dynamic D1 end-label 399\n\ncommit save\n</code></pre>","title":"MPLS label manager"},{"location":"tutorials/mpls/mpls-ldp/ldp/#ldp-neighbor-discovery","text":"<p>LDP neighbor discovery allows SR Linux to discover and connect to LDP peers without manually specifying the peers. SR Linux supports basic LDP discovery for discovering LDP peers, using multicast UDP hello messages.</p> <p>At a minimum, you should enable the LDP process in the network-instance and specify LDP-enabled interfaces.</p> srl1srl2srl3   <pre><code>enter candidate\n\nset / network-instance default protocols ldp\nset / network-instance default protocols ldp admin-state enable\nset / network-instance default protocols ldp dynamic-label-block D1\n\nset / network-instance default protocols ldp discovery\nset / network-instance default protocols ldp discovery interfaces\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4 admin-state enable\n\ncommit save\n</code></pre>   <pre><code>enter candidate\n\nset / network-instance default protocols\nset / network-instance default protocols ldp\nset / network-instance default protocols ldp admin-state enable\nset / network-instance default protocols ldp dynamic-label-block D1\nset / network-instance default protocols ldp discovery\n\nset / network-instance default protocols ldp discovery interfaces\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4 admin-state enable\n\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/2.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/2.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/2.0 ipv4 admin-state enable\n\ncommit save\n</code></pre>   <pre><code>enter candidate\n\nset / network-instance default protocols ldp\nset / network-instance default protocols ldp admin-state enable\nset / network-instance default protocols ldp dynamic-label-block D1\n\nset / network-instance default protocols ldp discovery\nset / network-instance default protocols ldp discovery interfaces\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4\nset / network-instance default protocols ldp discovery interfaces interface ethernet-1/1.0 ipv4 admin-state enable\n\ncommit save\n</code></pre>    <p>Once enabled, LDP neighborship will establish over the specified interfaces.</p> neighborssessions   <pre><code>--{ running }--[  ]--                                                                            \nA:srl2# show network-instance default protocols ldp neighbor                                     \n=================================================================================================\nNet-Inst default LDP neighbors\n-------------------------------------------------------------------------------------------------\n+------------------------------------------------------------------------------------------+\n| Interface    Peer LDP     Nbr          Local        Proposed     Negotiated   Remaining  |\n|              ID           Address      Address      Holdtime     Holdtime     Holdtime   |\n+==========================================================================================+\n| ethernet-1   10.0.0.1:0   10.1.2.1     10.1.2.2     15           15           11         |\n| /1.0                                                                                     |\n| ethernet-1   10.0.0.3:0   10.2.3.2     10.2.3.1     15           15           14         |\n| /2.0                                                                                     |\n+------------------------------------------------------------------------------------------+\n=================================================================================================\n</code></pre>   <pre><code>A:srl2# /show network-instance default protocols ldp session                                    \n================================================================================================\nNet-Inst default LDP Sessions\n------------------------------------------------------------------------------------------------\n+-------------------------------------------------------------------------------------------+\n| Peer LDP ID                State           Msg Sent   Msg Recv   Last Oper State Change   |\n+===========================================================================================+\n| 10.0.0.1:0                 operational     24         24         2022-03-14T08:08:28.000Z |\n| 10.0.0.3:0                 operational     24         24         2022-03-14T08:08:24.000Z |\n+-------------------------------------------------------------------------------------------+\nNo. of sessions: 2\n</code></pre>     <p>Tip</p> <p>This packet capture sniffed on srl2's <code>e1-1</code> shows the LDP multicast hello messages as well as the subsequent Initialization and Notification messages.</p>","title":"LDP neighbor discovery"},{"location":"tutorials/mpls/mpls-ldp/ldp/#fec","text":"<p>It is necessary to precisely specify which packets may be mapped to each LSP. This is done by providing a FEC specification for each LSP. The FEC identifies the set of IP packets that may be mapped to that LSP.</p> <p>Each FEC is specified as a set of one or more FEC elements. Each FEC element identifies a set of packets that may be mapped to the corresponding LSP.</p> <p>By default, SR Linux supports /32 IPv4 FEC resolution using IGP routes. For example, on <code>srl2</code> we see four FECs have been received, and four FECs have been advertised. These FECs were created for <code>system0</code> interface IP addresses advertised via IGP.</p> <pre><code>A:srl2# /show network-instance default protocols ldp ipv4 fec                                     \n==================================================================================================\nNet-Inst default LDP IPv4: All FEC prefixes table\n==================================================================================================\nReceived FEC prefixes\n--------------------------------------------------------------------------------------------------\n+--------------------------------------------------------------------------------------------+\n| FEC prefix           Peer LDP ID                 Label                Ingress   Used in    |\n|                                                                       LSR       Forwarding |\n+============================================================================================+\n| 10.0.0.1/32          10.0.0.1:0                  100                  true      true       |\n| 10.0.0.1/32          10.0.0.3:0                  304                  true      false      |\n| 10.0.0.3/32          10.0.0.1:0                  109                  true      false      |\n| 10.0.0.3/32          10.0.0.3:0                  300                  true      true       |\n+--------------------------------------------------------------------------------------------+\n--------------------------------------------------------------------------------------------------\nAdvertised FEC prefixes\n--------------------------------------------------------------------------------------------------\n+--------------------------------------------------------------------------------------------+\n| FEC prefix           Peer LDP ID                 Label                Label        Egress  |\n|                                                                       Status       LSR     |\n+============================================================================================+\n| 10.0.0.1/32          10.0.0.3:0                  206                               false   |\n| 10.0.0.2/32          10.0.0.1:0                  204                               true    |\n| 10.0.0.2/32          10.0.0.3:0                  204                               true    |\n| 10.0.0.3/32          10.0.0.1:0                  205                               false   |\n+--------------------------------------------------------------------------------------------+\n--------------------------------------------------------------------------------------------------\nTotal received FEC prefixes  : 4 (2 used in forwarding)\nTotal advertised FEC prefixes: 4\n</code></pre> <p>The successful labels exchange leads to a populated tunnel table on each MPLS-enabled router. For instance, the tunnel table on <code>srl1</code> lists two tunnels for remote loopbacks of <code>srl2</code> and <code>srl3</code>:</p> <pre><code>--{ running }--[  ]--                                                                                                   \nA:srl1# show network-instance default tunnel-table all                                                                  \n------------------------------------------------------------------------------------------------------------------------\nIPv4 tunnel table of network-instance \"default\"\n------------------------------------------------------------------------------------------------------------------------\n+-------------+------------+------------+-----------+-----+--------+------------+------------+------------+------------+\n| IPv4 Prefix |   Encaps   |   Tunnel   | Tunnel ID | FIB | Metric | Preference |    Last    |  Next-hop  |  Next-hop  |\n|             |    Type    |    Type    |           |     |        |            |   Update   |   (Type)   |            |\n+=============+============+============+===========+=====+========+============+============+============+============+\n| 10.0.0.2/32 | mpls       | ldp        | 65548     | Y   | 10     | 9          | 2022-03-14 | 10.1.2.2   | ethernet-1 |\n|             |            |            |           |     |        |            | T08:08:29. | (mpls)     | /1.0       |\n|             |            |            |           |     |        |            | 207Z       |            |            |\n| 10.0.0.3/32 | mpls       | ldp        | 65549     | Y   | 20     | 9          | 2022-03-14 | 10.1.2.2   | ethernet-1 |\n|             |            |            |           |     |        |            | T08:08:29. | (mpls)     | /1.0       |\n|             |            |            |           |     |        |            | 219Z       |            |            |\n+-------------+------------+------------+-----------+-----+--------+------------+------------+------------+------------+\n------------------------------------------------------------------------------------------------------------------------\n2 LDP tunnels, 2 active, 0 inactive\n------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>The label operations can be seen in the mpls route-table report. For example, our LSR <code>srl2</code> performs swap operations for labels it assigned for remote FECs and it pops a label <code>204</code> as it was assigned to its own <code>10.0.0.2</code> FEC.</p> <pre><code>A:srl2# /show network-instance default route-table mpls\n+---------+-----------+-------------+-----------------+------------------------+----------------------+------------------+\n| Label   | Operation | Type        | Next Net-Inst   | Next-hop IP (Type)     | Next-hop             | Next-hop MPLS    |\n|         |           |             |                 |                        | Subinterface         | labels           |\n+=========+===========+=============+=================+========================+======================+==================+\n| 204     | POP       | ldp         | default         |                        |                      |                  |\n| 205     | SWAP      | ldp         | N/A             | 10.2.3.2 (mpls)        | ethernet-1/2.0       | 300              |\n| 206     | SWAP      | ldp         | N/A             | 10.1.2.1 (mpls)        | ethernet-1/1.0       | 100              |\n+---------+-----------+-------------+-----------------+------------------------+----------------------+------------------+\n</code></pre>","title":"FEC"},{"location":"tutorials/mpls/mpls-ldp/ldp/#testing-mpls-dataplane","text":"<p>The tunnels established for <code>system0</code> loopback FECs cannot be tested as is because they resolve to the existing IGP routes, and thus plain IPv4 transport is used. To test the MPLS dataplane we would need to create another pair of loopbacks on <code>srl1</code>/<code>srl3</code> nodes and create an iBGP session exchanging these loopbacks; only this time, we will leverage a specific BGP knob asking to resolve the nexthops for these prefixes via LDP tunnel only.</p>  <p>In the following snippets we configure <code>lo0</code> loopbacks following with iBGP peering setup to advertise them.</p> srl1srl3   <pre><code>enter candidate\n\n# configuring loopback interface\nset / interface lo0\nset / interface lo0 admin-state enable\nset / interface lo0 subinterface 0\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4\nset / interface lo0 subinterface 0 ipv4 address 192.168.99.1/32\n\nset / network-instance default interface lo0.0\n\n# configuring export policy to advertise loopbacks via BGP\nset / routing-policy\nset / routing-policy prefix-set LOOPBACK\nset / routing-policy prefix-set LOOPBACK prefix 192.168.99.1/32 mask-length-range exact\nset / routing-policy policy EXPORT_LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match family ipv4-unicast\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match prefix-set LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action accept\n\n# configuring iBGP\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp router-id 10.0.0.1\nset / network-instance default protocols bgp group IBGP\nset / network-instance default protocols bgp group IBGP export-policy EXPORT_LOOPBACK\nset / network-instance default protocols bgp group IBGP ipv4-unicast\nset / network-instance default protocols bgp group IBGP ipv4-unicast admin-state enable\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution mode require\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution allowed-tunnel-types [ ldp ]\nset / network-instance default protocols bgp neighbor 10.0.0.3\nset / network-instance default protocols bgp neighbor 10.0.0.3 admin-state enable\nset / network-instance default protocols bgp neighbor 10.0.0.3 peer-as 65001\nset / network-instance default protocols bgp neighbor 10.0.0.3 peer-group IBGP\n\ncommit save\n</code></pre>   <pre><code>enter candidate\n\n# configuring loopback interface\nset / interface lo0\nset / interface lo0 admin-state enable\nset / interface lo0 subinterface 0\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4\nset / interface lo0 subinterface 0 ipv4 address 192.168.99.3/32\n\nset / network-instance default interface lo0.0\n\n# configuring export policy to advertise loopbacks via BGP\nset / routing-policy\nset / routing-policy prefix-set LOOPBACK\nset / routing-policy prefix-set LOOPBACK prefix 192.168.99.3/32 mask-length-range exact\nset / routing-policy policy EXPORT_LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match family ipv4-unicast\nset / routing-policy policy EXPORT_LOOPBACK statement 10 match prefix-set LOOPBACK\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action\nset / routing-policy policy EXPORT_LOOPBACK statement 10 action accept\n\n# configuring iBGP\nset / network-instance default protocols\nset / network-instance default protocols bgp\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp router-id 10.0.0.3\nset / network-instance default protocols bgp group IBGP\nset / network-instance default protocols bgp group IBGP export-policy EXPORT_LOOPBACK\nset / network-instance default protocols bgp group IBGP ipv4-unicast\nset / network-instance default protocols bgp group IBGP ipv4-unicast admin-state enable\nset / network-instance default protocols bgp ipv4-unicast\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution mode require\nset / network-instance default protocols bgp ipv4-unicast next-hop-resolution ipv4-next-hops tunnel-resolution allowed-tunnel-types [ ldp ]\nset / network-instance default protocols bgp neighbor 10.0.0.1\nset / network-instance default protocols bgp neighbor 10.0.0.1 admin-state enable\nset / network-instance default protocols bgp neighbor 10.0.0.1 peer-as 65001\nset / network-instance default protocols bgp neighbor 10.0.0.1 peer-group IBGP\n\ncommit save\n</code></pre>    <p>The iBGP peering should establish and both <code>srl1</code> and <code>srl3</code> nodes should receive loopback prefixes and install them in the routing table. From <code>srl1</code> point of view it received the remote loopback over BGP:</p> <pre><code>A:srl1# /show network-instance default protocols bgp neighbor 10.0.0.3 received-routes ipv4                                                        \n---------------------------------------------------------------------------------------------------------------------------------------------------\nPeer        : 10.0.0.3, remote AS: 65001, local AS: 65001\nType        : static\nDescription : None\nGroup       : IBGP\n---------------------------------------------------------------------------------------------------------------------------------------------------\nStatus codes: u=used, *=valid, &gt;=best, x=stale\nOrigin codes: i=IGP, e=EGP, ?=incomplete\n+----------------------------------------------------------------------------------------------------------------------------------------------+\n| Status         Network                Next Hop             MED          LocPref                       AsPath                       Origin    |\n+==============================================================================================================================================+\n|  u*&gt;     192.168.99.3/32        10.0.0.3                    -             100                                                         i      |\n+----------------------------------------------------------------------------------------------------------------------------------------------+\n---------------------------------------------------------------------------------------------------------------------------------------------------\n1 received BGP routes : 1 used 1 valid\n</code></pre> <p>And installed it in the routing table. The notable difference here is that the nexthop (<code>10.0.0.3</code>) is indirect, as it is being resolved via mpls/ldp tunnel. We can even see which label will be pushed on the stack1.</p> <pre><code>A:srl1# /show network-instance default route-table ipv4-unicast prefix 192.168.99.3/32 detail                                                      \n---------------------------------------------------------------------------------------------------------------------------------------------------\nIPv4 unicast route table of network instance default\n---------------------------------------------------------------------------------------------------------------------------------------------------\nDestination   : 192.168.99.3/32\nID            : 0\nRoute Type    : bgp\nRoute Owner   : bgp_mgr\nMetric        : 0\nPreference    : 170\nActive        : true\nLast change   : 2022-03-14T09:05:46.076Z\nResilient hash: false\n---------------------------------------------------------------------------------------------------------------------------------------------------\nNext hops: 1 entries\n10.0.0.3 (indirect) resolved by tunnel to 10.0.0.3/32 (ldp)\n  via 10.1.2.2 (mpls) via [ethernet-1/1.0]\n      pushed MPLS labels : [205]\n---------------------------------------------------------------------------------------------------------------------------------------------------\n</code></pre> <p>Now it is time to test the datapath with a ping between newly created loopbacks2.</p> <pre><code>--{ + running }--[  ]--                                                                                                                            \nA:srl1# ping network-instance default 192.168.99.3 -I 192.168.99.1                                                                                 \nUsing network instance default\nPING 192.168.99.3 (192.168.99.3) from 192.168.99.1 : 56(84) bytes of data.\n64 bytes from 192.168.99.3: icmp_seq=1 ttl=64 time=15.3 ms\n64 bytes from 192.168.99.3: icmp_seq=2 ttl=64 time=9.32 ms\n64 bytes from 192.168.99.3: icmp_seq=3 ttl=64 time=16.7 ms\n64 bytes from 192.168.99.3: icmp_seq=4 ttl=64 time=14.9 ms\n^C\n--- 192.168.99.3 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3004ms\nrtt min/avg/max/mdev = 9.316/14.063/16.727/2.823 ms\n</code></pre> <p>Yay! It works. Now let's see if we indeed had MPLS encapsulation used for that packet exchange. To quickly test this you can run a tcpdump on any node of the topology filtering mpls packets. For instance, let's connect to <code>srl1</code> shell via <code>docker exec</code> command and start listening for mpls packets on e1-1 interface:</p> <pre><code>docker exec -it srl1 bash\n[root@srl1 /]# tcpdump -nnvi e1-1 mpls\n\ntcpdump: listening on e1-1, link-type EN10MB (Ethernet), snapshot length 262144 bytes\n09:54:03.700378 MPLS (label 205, exp 0, [S], ttl 64) # (1)!\n        IP (tos 0x0, ttl 64, id 50746, offset 0, flags [DF], proto ICMP (1), length 84)\n    192.168.99.1 &gt; 192.168.99.3: ICMP echo request, id 52902, seq 1, length 64\n09:54:03.707775 MPLS (label 100, exp 0, [S], ttl 63) # (2)!\n        IP (tos 0x0, ttl 64, id 63961, offset 0, flags [none], proto ICMP (1), length 84)\n    192.168.99.3 &gt; 192.168.99.1: ICMP echo reply, id 52902, seq 1, length 64\n09:54:04.701840 MPLS (label 205, exp 0, [S], ttl 64)\n        IP (tos 0x0, ttl 64, id 50797, offset 0, flags [DF], proto ICMP (1), length 84)\n    192.168.99.1 &gt; 192.168.99.3: ICMP echo request, id 52902, seq 2, length 64\n09:54:04.707592 MPLS (label 100, exp 0, [S], ttl 63)\n        IP (tos 0x0, ttl 64, id 64000, offset 0, flags [none], proto ICMP (1), length 84)\n    192.168.99.3 &gt; 192.168.99.1: ICMP echo reply, id 52902, seq 2, length 64\n</code></pre> <ol> <li>MPLS frame with label <code>205</code> encapsulates ICMP echo request sourced from srl1</li> <li>MPLS frame with label <code>100</code> encapsulates ICMP echo reply coming from srl2</li> </ol> <p>This evidence clearly shows the MPLS encapsulation in play. In addition to that, we have captured a pcap on <code>srl2:e-1</code> interface for ICMP packets in case you would like to look at the entire packet encapsulation and framing.</p> <p></p>","title":"Testing MPLS dataplane"},{"location":"tutorials/mpls/mpls-ldp/ldp/#complete-lab","text":"<p>It is great to follow the tutorial doing all the steps yourself. But maybe not every single time  For those who just want to get a looksee at the LDP-in-action we created complete config snippets for the nodes so that they can boot with everything pre-provisioned and ready.</p> <p>You can fetch the config snippets with <code>curl</code>: <pre><code>curl -LO https://raw.githubusercontent.com/srl-labs/learn-srlinux/main/labs/mpls-ldp/srl1.cfg\ncurl -LO https://raw.githubusercontent.com/srl-labs/learn-srlinux/main/labs/mpls-ldp/srl2.cfg\ncurl -LO https://raw.githubusercontent.com/srl-labs/learn-srlinux/main/labs/mpls-ldp/srl3.cfg\n</code></pre></p> <p>Put the downloaded config files next to the topology file and make sure to set up startup-config for each node:</p> <pre><code>name: mpls-ldp\nprefix: \"\"\n\ntopology:\n  defaults:\n    kind: srl\n  kinds:\n    srl:\n      image: ghcr.io/nokia/srlinux:21.11.3\n      type: ixr6\n  nodes:\n\n    srl1:\n      startup-config: srl1.cfg\n    srl2:\n      startup-config: srl2.cfg\n    srl3:\n      startup-config: srl3.cfg\n\n  links:\n    - endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n    - endpoints: [\"srl2:e1-2\", \"srl3:e1-1\"]\n</code></pre> <p>Deploy the lab as usual, and you should have everything ready once the lab is deployed.</p>   <ol> <li> <p>the label number (205) indicates that this label comes from <code>srl2</code>, as we configured 200-299 label range block on srl2 device.\u00a0\u21a9</p> </li> <li> <p>one could also introduce linux clients to the topology, connect them to srl\u2153 nodes and test the connectivity that way.\u00a0\u21a9</p> </li> </ol>","title":"Complete lab"},{"location":"tutorials/mpls/mpls-ldp/routing/","text":"<p>Prior to any MPLS configuration, we need to set up routing in the network core. Configuration of interfaces and IGP is the core task explained in this section.</p>","title":"Routing"},{"location":"tutorials/mpls/mpls-ldp/routing/#interfaces","text":"<p>Let's start with basic interfaces configuration following this diagram:</p>  <p>The below config snippets configure regular <code>Ethernet-1/1</code>, <code>Ethernet-1/2</code> and a special loopback <code>system0</code> interfaces.</p> srl1srl2srl3   <pre><code>enter candidate # (1)!\n\nset / interface ethernet-1/1\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 10.1.2.1/30\n\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 admin-state enable\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.1/32\n\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface system0.0\n\ncommit save\n</code></pre> <ol> <li>config snippets contain <code>enter candidate</code> command to switch to configuration context. At the bottom of the snippet <code>commit save</code> command will perform a <code>commit</code> operation followed by saving the running config to a startup config file.</li> </ol>   <pre><code>enter candidate\n\nset / interface ethernet-1/1\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 10.1.2.2/30\n\nset / interface ethernet-1/2\nset / interface ethernet-1/2 admin-state enable\nset / interface ethernet-1/2 subinterface 0\nset / interface ethernet-1/2 subinterface 0 admin-state enable\nset / interface ethernet-1/2 subinterface 0 ipv4\nset / interface ethernet-1/2 subinterface 0 ipv4 address 10.2.3.1/30\n\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 admin-state enable\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.2/32\n\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface ethernet-1/2.0\nset / network-instance default interface system0.0\n\ncommit save\n</code></pre>   <pre><code>enter candidate\n\nset / interface ethernet-1/1\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4\nset / interface ethernet-1/1 subinterface 0 ipv4 address 10.2.3.2/30\n\nset / interface system0\nset / interface system0 admin-state enable\nset / interface system0 subinterface 0\nset / interface system0 subinterface 0 admin-state enable\nset / interface system0 subinterface 0 ipv4\nset / interface system0 subinterface 0 ipv4 address 10.0.0.3/32\n\nset / network-instance default\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface system0.0\n\ncommit save\n</code></pre>    <p>When the interface config is committed1, routers should be able to ping each neighbor's interface address.</p> srl1 pings srl2srl2 pings srl3   <pre><code>--{ running }--[  ]--\nA:srl1# ping network-instance default 10.1.2.2\nUsing network instance default\nPING 10.1.2.2 (10.1.2.2) 56(84) bytes of data.\n64 bytes from 10.1.2.2: icmp_seq=1 ttl=64 time=49.7 ms\n</code></pre>   <pre><code>--{ running }--[  ]--\nA:srl2# ping network-instance default 10.2.3.2\nUsing network instance default\nPING 10.2.3.2 (10.2.3.2) 56(84) bytes of data.\n64 bytes from 10.2.3.2: icmp_seq=1 ttl=64 time=0.033 ms\n</code></pre>","title":"Interfaces"},{"location":"tutorials/mpls/mpls-ldp/routing/#igp","text":"<p>With interfaces config done, proceed with configuring an IGP protocol to redistribute the loopback address information among all routers. In this tutorial, we will use IS-IS routing protocol to achieve this goal.</p>  srl1srl2srl3   <pre><code>enter candidate\n\nset / network-instance default protocols isis\nset / network-instance default protocols isis instance ISIS\nset / network-instance default protocols isis instance ISIS admin-state enable\nset / network-instance default protocols isis instance ISIS level-capability L2\nset / network-instance default protocols isis instance ISIS net [ 49.0001.0000.0000.0001.00 ]\nset / network-instance default protocols isis instance ISIS ipv4-unicast\nset / network-instance default protocols isis instance ISIS ipv4-unicast admin-state enable\n\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 level 2\n\nset / network-instance default protocols isis instance ISIS interface system0.0\nset / network-instance default protocols isis instance ISIS interface system0.0 admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 passive true\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 level 2\n\ncommit save\n</code></pre>   <pre><code>set / network-instance default protocols isis\nset / network-instance default protocols isis instance ISIS\nset / network-instance default protocols isis instance ISIS admin-state enable\nset / network-instance default protocols isis instance ISIS level-capability L2\nset / network-instance default protocols isis instance ISIS net [ 49.0001.0000.0000.0002.00 ]\nset / network-instance default protocols isis instance ISIS ipv4-unicast\nset / network-instance default protocols isis instance ISIS ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 level 2\n\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/2.0 level 2\n\nset / network-instance default protocols isis instance ISIS interface system0.0\nset / network-instance default protocols isis instance ISIS interface system0.0 admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 passive true\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 level 2\n\ncommit save\n</code></pre>   <pre><code>set / network-instance default protocols isis\nset / network-instance default protocols isis instance ISIS\nset / network-instance default protocols isis instance ISIS admin-state enable\nset / network-instance default protocols isis instance ISIS level-capability L2\nset / network-instance default protocols isis instance ISIS net [ 49.0001.0000.0000.0003.00 ]\nset / network-instance default protocols isis instance ISIS ipv4-unicast\nset / network-instance default protocols isis instance ISIS ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface ethernet-1/1.0 level 2\n\nset / network-instance default protocols isis instance ISIS interface system0.0\nset / network-instance default protocols isis instance ISIS interface system0.0 admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 passive true\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast\nset / network-instance default protocols isis instance ISIS interface system0.0 ipv4-unicast admin-state enable\nset / network-instance default protocols isis instance ISIS interface system0.0 level 2\n\ncommit save\n</code></pre>    <p>All routers now should have enabled IS-IS adjacency with their respective neighbors, and the routing table should contain respective <code>system0.0</code> loopback addresses. A view from <code>srl2</code> side:</p> AdjacencyRouting table   <pre><code>--{ running }--[  ]--                                                                                                  \nA:srl2# show  /network-instance default protocols isis adjacency                                                       \n-----------------------------------------------------------------------------------------------------------------------\nNetwork Instance: default\nInstance        : ISIS\n+----------------+----------------+---------------+------------+--------------+-------+---------------+---------------+\n| Interface Name |    Neighbor    |   Adjacency   | Ip Address | Ipv6 Address | State |     Last      |   Remaining   |\n|                |   System Id    |     Level     |            |              |       |  transition   |   holdtime    |\n+================+================+===============+============+==============+=======+===============+===============+\n| ethernet-1/1.0 | 0000.0000.0001 | L2            | 10.1.2.1   | ::           | up    | 2022-03-13T14 | 23            |\n|                |                |               |            |              |       | :15:57.500Z   |               |\n| ethernet-1/2.0 | 0000.0000.0003 | L2            | 10.2.3.2   | ::           | up    | 2022-03-13T14 | 21            |\n|                |                |               |            |              |       | :25:50.100Z   |               |\n+----------------+----------------+---------------+------------+--------------+-------+---------------+---------------+\nAdjacency Count: 2\n-----------------------------------------------------------------------------------------------------------------------\n</code></pre>   <p>The below output verifies that <code>srl2</code> has successfully received loopbacks prefixes from <code>srl1/3</code> nodes. <pre><code>--{ running }--[  ]--                                                                                                  \nA:srl2# /show network-instance default route-table all | grep isis                                                     \n| 10.0.0.1/32 | 0    | isis      | isis_mgr            | True/success        | 10      | 18     | 10.1.2 | ethern |\n| 10.0.0.3/32 | 0    | isis      | isis_mgr            | True/success        | 10      | 18     | 10.2.3 | ethern |\n</code></pre></p>    <p>With IGP setup is done, we can proceed with LDP configuration.</p>   <ol> <li> <p>for instance, with <code>commit save</code> command executed from within configuration context.\u00a0\u21a9</p> </li> </ol>","title":"IGP"},{"location":"tutorials/programmability/event-handler/oper-group/lab/","text":"<p>As always, this tutorial will be backed up by a lab that readers can effortlessly deploy on their machine and follow along. Oper-group lab is contained within srl-labs/oper-group-lab repository and features:</p> <ol> <li>A Clos based fabric with 4 leaves and 2 spines, forming the fabric</li> <li>Two dual-homed clients emulated with linux containers and running <code>iperf</code> software to generate traffic</li> <li>L2 EVPN service1 configured across the leaves of the fabric</li> <li>A telemetry stack to demonstrate oper-group operations in action.</li> </ol>","title":"Lab setup"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#physical-topology","text":"<p>On a physical layer topology interconnections are layed down as follows:</p>  <p>Each client is dual-homed to corresponding leaves; To achieve that, interfaces <code>eth1</code> and <code>eth2</code> are formed into a <code>bond0</code> interface. On the leaves side, the access interface `Ethernet-1/1`` is part of a LAG interface that is \"stretched\" between a pair of leaves, forming a logical construct similar to MC-LAG.</p>","title":"Physical topology"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#fabric-underlay","text":"<p>In the underlay of a fabric leaves and spines run eBGP protocol to enable leaves to exchange reachability information for their <code>system0</code> interfaces.</p>  <p>eBGP peerings are formed between each leaf and spine pair.</p>","title":"Fabric underlay"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#fabric-overlay","text":"<p>To support BGP EVPN service, in the overlay iBGP peerings with EVPN address family are established from each leaf to each spine, with spines acting as route reflectors.</p>  <p>From the EVPN service standpoint, the mac-vrf instance named <code>vrf-1</code> is created on leaves and <code>ES-1</code> ethernet segment is formed from a LAG interface.</p>  <p>Ethernet segments are configured to be in an all-active mode to make sure that every access link is utilized in the fabric.</p>","title":"Fabric overlay"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#telemetry-stack","text":"<p>We have enhanced the lab with a telemetry stack featuring gnmic, prometheus, and grafana - our famous GPG stack. Nothing beats real-time visualization, especially when we want to correlate events happening in the network.</p>    Element Address     Grafana https://localhost:3000   Prometheus https://localhost:9090","title":"Telemetry stack"},{"location":"tutorials/programmability/event-handler/oper-group/lab/#lab-deployment","text":"<p>Start with cloning lab's repository</p> <pre><code>git clone https://github.com/srl-labs/opergroup-lab.git &amp;&amp; cd opergroup-lab\n</code></pre> <p>Lab repository contains startup configuration files for the fabric nodes, as well as necessary files for the telemetry stack to come up online operational. To deploy the lab:</p> <pre><code>containerlab deploy -t opergroup.clab.yml\n</code></pre> <p>This will stand up a lab with an already pre-configured fabric using startup configs contained within <code>configs</code> directory.</p>  <p>The deployed lab starts up in a pre-provisioned step, where underlay/overlay configuration has already been done. We proceed with oper-group use case exploration in the next chapter of this tutorial.</p>   <ol> <li> <p>Check L2 EVPN tutorial to get the basics of L2 EVPN service configuration.\u00a0\u21a9</p> </li> </ol>","title":"Lab deployment"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/","text":"<p>Now that we are aware of a potential traffic blackholing that may happen in the all-active EVPN-based fabrics it is time to meet one of the remediation tactics.</p> <p>What would have helped to prevent traffic to get blackholed is to not let it be forwarded to a leaf that has no active uplinks in the first place. This may be achieved by disabling links connected to workloads as soon as uplinks become operationally disabled. This is what oper-group can do and what is depicted below.</p>  <p>In this section, we will look into how a particular flavor of the oper-group feature is realized using the Event Handler framework.</p>","title":"Configuring oper-group"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#event-handler-based-oper-group","text":"<p>Starting with the 22.6.1 release SR Linux comes equipped with the Event Handler framework that allows users to write custom Python scripts and has these scripts be called in the event state paths change the value. Event Handler enables SR Linux operators to add programmable logic to handle events that happen in a system.</p> <p>The following sequence1 captures the core logic of the Event-Handler framework:</p> <ol> <li>A user configures the Event Handler instance with a set of objects to monitor. The objects are referenced by their path provided in a CLI notation.</li> <li>In addition to the paths, a user may configure arbitrary static options that will parametrize a script. script</li> <li>Whenever there is a state change for any of the monitored paths, Event Handler executes a script with a single argument - a JSON string that consists of:<ul> <li>the current value of the monitored paths</li> <li>options provided by a user</li> <li>persistent data if it was set by a script</li> </ul> </li> </ol> <p>One of the first features that leverage Event Handler capability is Oper Group. As was explained in the introduction section, oper-group feature allows changing the operational status of selected ports based on the operational status of another, related, group of ports.</p> <p>Event Handler is supported in SR Linux by the <code>event_mgr</code> process which exposes configuration and state via a container at <code>.system.event-handler</code>. Within this container, a list of event handling instances can be configured at <code>.system.event-handler.instance{}</code> with a user-defined name.</p> <pre><code>--{ * candidate shared default }--[ system event-handler ]--\nA:leaf1# info\n    instance opergroup { #(1)!\n    }\n</code></pre> <ol> <li>creation of <code>opergroup</code> Event Handler instance</li> </ol> <p>In this tutorial we will touch upon the most crucial configuration options:</p> <ul> <li><code>paths</code> - to select paths for monitoring state changes</li> <li><code>options</code> - to provide optional parameters to a script</li> <li><code>upython-script</code> - a path to a MicroPython script that contains the automation logic</li> </ul>","title":"Event Handler-based Oper Group"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#monitored-paths","text":"<p>The oper-group feature requires users to define a set of uplinks that are crucial for a working service. By monitoring the state of these selected uplinks oper-group decides if the downlinks' operational state should be changed.</p> <p>In the context of this tutorial, on <code>leaf1</code> two uplink interfaces <code>ethernet-1/49</code> and <code>ethernet-1/50</code> should be put under monitoring to avoid blackholing of traffic in case their oper-state will change to a down state.</p>  <p>Event Handler configuration contains <code>paths</code> leaf-list for users to select objects for monitoring. Paths should be provided in a CLI format.</p> <p>To monitor the operational state of a given interface the <code>interface &lt;interface-name&gt; oper-state</code> leaf shall be used; in our case, this requirement will translate to the following configuration:</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info\n    paths [\n        \"interface ethernet-1/{49..50} oper-state\" #(1)!\n    ]\n</code></pre> <ol> <li>Paths can use range expansion and wildcards</li> </ol> <p>With this configuration Event Handler will subscribe to state changes for <code>oper-state</code> leaf of the two interfaces.</p>","title":"Monitored paths"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#options","text":"<p>By just monitoring the operational state of certain uplinks we don't gain much. There needs to be a coupling between the monitored uplinks and the downlinks.</p> <ul> <li>Which access links should react to state changes of the uplinks?</li> <li>How many uplinks must be \"healthy\" before we bring down access links?</li> </ul> <p>To answer these questions we need to provide additional parameters to the Event Handler and this is done via <code>options</code>.</p> <p>Options are a user-defined set of parameters that will be passed to a script along with the state of the monitored paths. For the oper-group feature we are going to define two options, that help us parametrize </p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info options\n    options {\n        object down-links {\n            values [\n                ethernet-1/1\n            ]\n        }\n        object required-up-uplinks {\n            value 1\n        }\n    }\n</code></pre> <p>To define which links should follow the state of the uplinks we provide the <code>down-links</code> option. This option is defined as a list of values to accommodate for potential support of many access links, but since our lab only has single access lint, the list.</p>  <p>Note</p> <p>Values defined in options are free-formed strings and may or may not follow any particular syntax. For <code>down-links</code> option, we choose to use a CLI-compatible value of an interface since this will make it easier to create an action in the script body. But we could use any other form of the interface name.</p>  <p>The second option - <code>required-up-uplinks</code> - conveys the number of uplinks we want to have in operation before we put access links down. When a leaf has more than 1 uplink, we may want to tolerate it losing a single uplink. In this tutorial, we pass a value of <code>1</code> which means that at a minimum we want to have at least one uplink to be up. In the script body, we will implement the logic of calculating the number of uplinks in an operational state, and the option is needed to provide the required boundary.</p> <p>We will also add a third option that will indicate to our script that it should print the value of certain script variables as explained later in the debug section. This option will help us explain script operations when we reach Oper group in action chapter.</p>","title":"Options"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#script","text":"<p>Event-Handler is a programmable framework that doesn't enforce any particular logic when it comes to handling events occurring in a system. Instead, users are free to create their scripts and thus program the handling of events.</p> <p>As part of the event handler instance configuration, users have to provide a path to a MicroPython script:</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info\n    admin-state enable\n    upython-script opergroup.py # (1)!\n    --snip--\n</code></pre> <ol> <li>A file named <code>opergroup.py</code> will be looked up in the following directories:<ul> <li><code>/etc/opt/srlinux/eventmgr/</code> for user-provided scripts</li> <li><code>/opt/srlinux/eventmgr</code> for Nokia-provided scripts.</li> </ul> </li> </ol> <p>This script will be called each time a state of any monitored paths changes.</p>","title":"Script"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-cfg/#resulting-configuration","text":"<p>When paths, options and script location are put together the Event Handler instance config takes the following shape:</p> <pre><code>--{ * candidate shared default }--[ system event-handler instance opergroup ]--\nA:leaf1# info\n    admin-state enable\n    upython-script opergroup.py #(4)!\n    paths [\n        \"interface ethernet-1/{49..50} oper-state\" #(1)!\n    ]\n    options {\n        object debug { #(5)!\n            value true\n        }\n        object down-links { #(2)!\n            values [\n                ethernet-1/1\n            ]\n        }\n        object required-up-uplinks { #(3)!\n            value 1\n        }\n    }\n</code></pre> <ol> <li>Monitor the operational state of these uplinks.</li> <li>The following links we consider \"access\" links, their operational state will depend on the state of the uplinks when processed by a script.</li> <li>Required number of uplinks to be in the oper-up state before putting down downlinks. </li> <li>Path to the script file which defines the logic of the event-handling routine using the state changes of the monitored paths and provided options.</li> <li>Debug option to indicate to a scrip that it should print additional debugging information.</li> </ol> <p>Now when the configuration is done, it is time to dive into the MicroPython code itself; at the end of the day, it is the core component of the framework.</p>   <ol> <li> <p>see the sequence diagram for additional details.\u00a0\u21a9</p> </li> </ol>","title":"Resulting configuration"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-intro/","text":"","title":"Introduction"},{"location":"tutorials/programmability/event-handler/oper-group/oper-group-intro/#introduction-to-oper-group","text":"Tutorial name Oper Groups with Event Handler   Lab components 6 Nokia SR Linux nodes, 2 Linux nodes   Resource requirements  4 vCPU  6 GB   Lab srl-labs/opergroup-lab   Main ref documents    Version information1 <code>containerlab:0.26.1</code>, <code>srlinux:22.3.2</code>, <code>docker-ce:20.10.2</code>    <p>One of the most common use cases that can be covered with the Event Handler framework is known as \"Operational group\" or \"Oper-group\" for short. An oper-group feature covers several use cases, but in essence, it creates a relationship between logical elements of a network node so that they become aware of each other - forming a logical group.</p> <p>In the data center space oper-group feature can tackle the problem of traffic black-holing when leaves lose all connectivity to the spine layer. Consider the following simplified Clos topology where clients are multi-homed to leaves:</p>  <p>With EVPN all-active multihoming enabled in fabric traffic from <code>client1</code> is load-balanced over the links attached to the upstream leaves and propagates via fabric to its destination.</p> <p>Since all links of a client' bond interface are active, traffic is hashed to each of the constituent links and thus utilizes all available bandwidth. A problem occurs when a leaf looses connectivity to all upstream spines, as illustrated below:</p>  <p>When <code>leaf1</code> loses its uplinks, traffic from <code>client1</code> still gets sent to it since the client is not aware of any link loss problems happening on the leaf. This results in traffic blackholing on <code>leaf1</code>.</p> <p>To remedy this particular failure scenario an oper-group can be used. The idea here is to make a logical grouping between certain uplink and downlink interfaces on the leaves so that downlinks would share fate with uplink status. In our example, oper-group can be configured in such a way that leaves will shutdown their downlink interfaces should they detect that uplinks went down. This operational group's workflow depicted below:</p>  <p>When a leaf loses its uplinks, the oper-group gets notified about that fact and reacts accordingly by operationally disabling the access link towards the client. Once the leaf's downlink transitions to a <code>down</code> state, the client's bond interface stops using that particular interface for hashing, and traffic moves over to healthy links. In our example, the client stops sending to <code>leaf1</code> and everything gets sent over to <code>leaf2</code>.</p> <p>In this tutorial, we will see how SR Linux's Event Handler framework enables oper-group capability.</p>   <ol> <li> <p>the following versions have been used to create this tutorial. The newer versions might work; please pin the version to the mentioned ones if they don't.\u00a0\u21a9</p> </li> </ol>","title":"Introduction to oper group"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/","text":"","title":"Oper group in action"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#start-up","text":"<p>When the Event Handler instance is configured and administratively enabled an initial sync of the monitored paths state is done. As a result of that initial sync, Event Handler immediately attempts to execute a script as it receives the state for the monitored paths.</p> <p>Users can check the status of a particular event handler instance by querying the state datastore:</p> <pre><code>--{ + running }--[  ]--\nA:leaf1# info from state /system event-handler instance opergroup\n    system {\n        event-handler {\n            instance opergroup {\n                admin-state enable\n                upython-script opergroup.py\n                oper-state up\n                last-input \"{\\\"paths\\\":[{\\\"path\\\":\\\"interface ethernet-1/49 oper-state\\\",\\\"value\\\":\\\"up\\\"},{\\\"path\\\":\\\"interface ethernet-1/50 oper-state\\\",\\\"value\\\":\\\"up\\\"}],\\\"options\\\":{\\\"down-links\\\":[\\\"ethernet-1/1\\\"],\\\"required-up-uplinks\\\":\\\"1\\\",\\\"required-up-uplins\\\":\\\"1\\\"}}\"\n                last-output \"{\\\"actions\\\": [{\\\"set-ephemeral-path\\\": {\\\"path\\\": \\\"interface ethernet-1/1 oper-state\\\", \\\"value\\\": \\\"up\\\"}}]}\"\n                last-stdout-stderr \"\"\n                path [\n                    \"interface ethernet-1/{49..50} oper-state\"\n                ]\n                options {\n                    object down-links {\n                        values [\n                            ethernet-1/1\n                        ]\n                    }\n                    object required-up-uplinks {\n                        value 1\n                    }\n                }\n                statistics {\n                    execution-duration 0\n                    last-execution \"20 minutes ago\"\n                    total-execution-duration 0\n                    execution-count 2\n                    execution-successes 2\n                }\n            }\n        }\n    }\n</code></pre> <p>Notable leaves in the state definition of the instance:</p> <ul> <li><code>oper-state</code> - the operational state of the instance. In case of any errors in the script and/or configuration, the state will be <code>down</code>.</li> <li><code>oper-reason</code> and <code>oper-reason-detail</code> - these leaves will contain info on the reasoning behind the event handler instance to be rendered operationally down.</li> <li><code>last-input</code> - input json string that was used during the last execution.</li> <li><code>last-stdout-stderr</code> - here you will find outputs from your script such as print statements and log messages.</li> <li><code>last-output</code> - output json string that was produced by a script during the last execution.</li> <li><code>statistics</code> - statistical information about the execution process.</li> </ul> <p>The state dump above captures the state of the <code>opergroup</code> event handler instance after the second successful run.</p>","title":"Start up"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#running-mode","text":"<p>Let's get back to our running fabric and once again verify that we have opergroup instance configured and running before we start manipulating the uplink's state.</p> checking opergroup config   <p>```js A:leaf1# info from running /system event-handler instance opergroup     system {         event-handler {             instance opergroup {                 admin-state enable                 upython-script opergroup.py                 path [                     \"interface ethernet-1/{49..50} oper-state\"                 ]                 options {                     object down-links {                         values [                             ethernet-1/1                         ]                     }                     object required-up-uplinks {                         value 1                     }                 }             }         }     }</p>    <p>```</p> ensuring opergroup is running   <p>```js A:leaf1# info  from state /system event-handler instance opergroup oper-state     system {         event-handler {             instance opergroup {                 oper-state up             }         }     }</p>    <p>```</p>","title":"Running mode"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#disabling-one-uplink","text":"<p>Let's start first putting down a single uplink with leaving the other one operational. Our oper-group is configured in such a way that unless we lose both uplinks nothing should happen to the downstream ethernet-1/1 interface. Time to put this to test.</p> <ol> <li> <p>Starting with the four streams 200 kbps each running for 60 seconds</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 60\n</code></pre> </li> <li> <p>At ~T=45s disable <code>ethernet-1/49</code> uplink interface by putting it administratively down with the following command</p> <pre><code>bash set-uplinks.sh leaf1 49 disable\n</code></pre> </li> <li> <p>Observe traffic distribution with grafana charts</p> </li> </ol> <p>What you should see has to resemble the following picture:</p> <p></p> <p>Initially, traffic is nicely balanced between two leaves and then even more through each leaf's uplinks. When we disable <code>ethernet-1/49</code> interface a single stream that was flowing through it got rerouted to <code>ethernet-1/50</code> and nothing impacted our streams. See how steady that line is on both access interfaces of our leaves.</p> <p>As for the Event Handler, it should've been executed a script once again, because, remember, the script runs every time any of the monitored objects change. Fetch the state of our opergroup instance and see for yourself:</p> <pre><code>A:leaf1# info from state /system event-handler instance opergroup\n    system {\n        event-handler {\n            instance opergroup {\n                admin-state enable\n                upython-script opergroup.py\n                oper-state up\n                last-input \"{\\\"paths\\\":[{\\\"path\\\":\\\"interface ethernet-1/49 oper-state\\\",\\\"value\\\":\\\"down\\\"},{\\\"path\\\":\\\"interface ethernet-1/50 oper-state\\\",\\\"value\\\":\\\"up\\\"}],\\\"options\\\":{\\\"debug\\\":\\\"true\\\",\\\"down-links\\\":[\\\"ethernet-1/1\\\"],\\\"required-up-uplinks\\\":\\\"1\\\",\\\"required-up-uplins\\\":\\\"1\\\"}}\"\n                last-output \"{\\\"actions\\\": [{\\\"set-ephemeral-path\\\": {\\\"path\\\": \\\"interface ethernet-1/1 oper-state\\\", \\\"value\\\": \\\"up\\\"}}]}\"\n                last-stdout-stderr \"num of required up uplinks = 1\ndetected num of up uplinks = 1\ndownlinks new state = up\n\"\n--snip--\n</code></pre> <p>Note, that <code>last-input</code> leaf has a <code>down</code> value for the oper-state of <code>ethernet-1/49</code>. At the same time, the <code>last-output</code> leaf that contains the output structure passed by our script indicates the <code>up</code> state for the access <code>ethernet-1/1</code> interface. That is because we have met our condition of having at least one uplink operational before putting down access links.</p> <p>The <code>last-stdout-stderr</code> leaf will show the debug statements we print out in our script to help us see which variables had which values during the script execution.</p> <ul> <li><code>num of required up uplinks = 1</code>: this value we configured via options is a constant.</li> <li><code>detected num of up uplinks = 1</code>: this is a calculated number of operational uplinks that our script performs using the input JSON string passed by Event Handler. Since one of the interfaces was down, the number of operational ones is <code>1</code>.</li> <li><code>downlinks new state = up</code>: since we met our condition and the number of operational interfaces is not less than the configured number of required active uplinks, the access interface must be operational.</li> </ul>","title":"Disabling one uplink"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#disabling-all-uplinks","text":"<p>So far so good, now let's have a look at a case where a <code>leaf1</code> loses its second uplink. This is where we expect Event Handler to enforce and put the access interface down to prevent traffic blackholing.</p> <ol> <li> <p>Starting with the four streams 200 kbps each running for 60 seconds</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 60\n</code></pre> </li> <li> <p>At ~T=30s disable <code>ethernet-1/50</code> uplink interface by putting it administratively down with the following command</p> <pre><code>bash set-uplinks.sh leaf1 50 disable\n</code></pre> </li> <li> <p>Observe traffic distribution with grafana charts</p> </li> </ol> <p></p> <p>That is Event Handler-based oper-group feature in action! As the annotations explain, the event of <code>ethernet-1/50</code> going down gets noticed by the Event Handler and it disables <code>leaf1</code> access link to prevent traffic from blackholing.</p> <p>All the streams that were served by <code>leaf1</code> moves to <code>leaf2</code> and no disruption is made to the TCP sessions. Iperf client reports that there were a few retransmits for the two streams that switched to the <code>leaf2</code> mid-flight, but that's it:</p> <pre><code>[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    0             sender\n[  5]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[  7]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    0             sender\n[  7]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[  9]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    3             sender\n[  9]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[ 11]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec    1             sender\n[ 11]   0.00-60.00  sec  1.48 MBytes   207 Kbits/sec                  receiver\n[SUM]   0.00-60.00  sec  5.92 MBytes   828 Kbits/sec    4             sender\n[SUM]   0.00-60.00  sec  5.92 MBytes   828 Kbits/sec                  receiver\n\niperf Done.\n</code></pre> <p>On the Event Handler site we will see the following picture:</p> <pre><code>A:leaf1# info from state /system event-handler instance opergroup\n    system {\n        event-handler {\n            instance opergroup {\n                admin-state enable\n                upython-script opergroup.py\n                oper-state up\n                last-input \"{\\\"paths\\\":[{\\\"path\\\":\\\"interface ethernet-1/49 oper-state\\\",\\\"value\\\":\\\"down\\\"},{\\\"path\\\":\\\"interface ethernet-1/50 oper-state\\\",\\\"value\\\":\\\"down\\\"}],\\\"options\\\":{\\\"debug\\\":\\\"true\\\",\\\"down-links\\\":[\\\"ethernet-1/1\\\"],\\\"required-up-link\\\":\\\"1\\\",\\\"required-up-uplinks\\\":\\\"1\\\"}}\"\n                last-output \"{\\\"actions\\\": [{\\\"set-ephemeral-path\\\": {\\\"path\\\": \\\"interface ethernet-1/1 oper-state\\\", \\\"value\\\": \\\"down\\\"}}]}\"\n                last-stdout-stderr \"num of required up uplinks = 1\ndetected num of up uplinks = 0\ndownlinks new state = down\n\"\n</code></pre> <p>First, in the <code>last-input</code> we see that Event Handler rightfully passes the current state of both uplinks, which is <code>down</code>. Next, in the <code>last-stdout-stderr</code> field we see that the script correctly calculated that no uplinks are operational and the desired state for the downlinks is <code>down</code>. Finally, the <code>last-output</code> now lists <code>set-ephemeral-path</code> with <code>down</code> value for the access interface. This will effectively get processed by the Event Handler and put down the <code>ethernet-1/1</code> interface.</p>","title":"Disabling all uplinks"},{"location":"tutorials/programmability/event-handler/oper-group/opergroup-operation/#enabling-interfaces","text":"<p>In the reverse order, let's bring both uplinks up and see what happens.</p> <ol> <li> <p>Starting with the four streams 200 kbps each running for 100 seconds</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 100\n</code></pre> </li> <li> <p>At ~T=30s bring both uplinks up</p> <pre><code>bash set-uplinks.sh leaf1 \"{49..50}\" enable\n</code></pre> </li> <li> <p>Observe traffic distribution with grafana charts</p> </li> </ol> <p></p> <p>We started with all streams taking <code>leaf2</code> route, granted that <code>leaf1</code> access interface was operationally <code>down</code> as a result of Event Handler operation.</p> <p>Then when we brought uplinks up, Event Handler enabled access interface <code>ethernet-1/1</code> on <code>leaf1</code> and strange things happened. Instead of seeing traffic moving over to <code>leaf1</code>, we see how it moves away from <code>leaf2</code>, but doesn't pass through <code>leaf1</code>.</p> <p>The reason is that <code>leaf1</code> although got its uplinks back in an operational state, wasn't able to establish iBGP sessions and get its EVPN routes yet. Thus, traffic was getting stuck. Then iBGP sessions came up, but at this point, TCP sessions were in backoff retry mode, so they were not immediately passing through <code>leaf1</code>.</p> <p>Eventually, closer to the end of the test we see how TCP streams managed to get back in shape and spiked in bitrate to meet the bitrate goal.</p> <p>This is quite an interesting observation, because it is evident that it might not be optimal to bring the access interface up when uplinks get operational, instead, we may want to improve our oper-group script to enable the access interface only when iBGP sessions are ready, or even EVPN routes are received and installed.</p>","title":"Enabling interfaces"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/","text":"<p>Before we meet the Event Handler framework of SR Linux and leverage it to configure oper-group feature, it is crucial to understand the problem at hand. As was mentioned in the introduction, without oper-group feature traffic loss can occur should any leaf lose all its uplinks. Let's lab a couple of scenarios that highlight a problem that oper-group is set to remedy.</p>","title":"Problem statement"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/#healthy-fabric-scenario","text":"<p>The startup configuration that our lab is equipped with gets our fabric to a state where traffic can be exchanged between clients. Users can verify that by running a simple iperf-based traffic test.</p> <p>In our lab, <code>client2</code> runs iperf3 server, while <code>client1</code> acts as a client. With the following command we can run a single stream of TCP data with a bitrate of 200 Kbps:</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K\n</code></pre> <p>Once invoked, <code>client1</code> starts to send data towards <code>client2</code> for 10 seconds, providing a report by the end of a test.</p> <p><pre><code>Connecting to host 192.168.100.2, port 5201\n[  5] local 192.168.100.1 port 55166 connected to 192.168.100.2 port 5201\n[ ID] Interval           Transfer     Bitrate         Retr  Cwnd\n[  5]   0.00-1.00   sec   107 KBytes   880 Kbits/sec    0   26.9 KBytes       \n[  5]   1.00-2.00   sec  0.00 Bytes  0.00 bits/sec    0   26.9 KBytes       \n[  5]   2.00-3.00   sec  0.00 Bytes  0.00 bits/sec    0   26.9 KBytes       \n[  5]   3.00-4.00   sec  0.00 Bytes  0.00 bits/sec    0   26.9 KBytes       \n[  5]   4.00-5.00   sec   128 KBytes  1.05 Mbits/sec    0   31.1 KBytes       \n[  5]   5.00-6.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   6.00-7.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   7.00-8.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   8.00-9.00   sec  0.00 Bytes  0.00 bits/sec    0   31.1 KBytes       \n[  5]   9.00-10.00  sec   128 KBytes  1.05 Mbits/sec    0   35.4 KBytes       \n- - - - - - - - - - - - - - - - - - - - - - - - -\n[ ID] Interval           Transfer     Bitrate         Retr\n[  5]   0.00-10.00  sec   363 KBytes   298 Kbits/sec    0             sender\n[  5]   0.00-10.00  sec   363 KBytes   298 Kbits/sec                  receiver\n</code></pre> In addition to iperf results, users can monitor the throughput of `leaf\u00bd`` links using grafana dashboard: </p> <p>This visualization tells us that <code>client1</code> hashed its single stream1 over <code>client1:eth2</code> interface that connects to <code>leaf2:e1-1</code>. On the \"Leaf2 e1-1 throughput\" panel in the bottom right we see incoming traffic that indicates data is flowing in via this interface.</p> <p>Next, we see that <code>leaf2</code> used its <code>e1-50</code> interface to send data over to a spine layer, through which it reaches <code>client2</code> side2.</p>","title":"Healthy fabric scenario"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/#load-balancing-on-the-client-side","text":"<p>Next, it is interesting to verify that client can utilize both links in its <code>bond0</code> interface since our L2 EVPN service uses an all-active multihoming mode for the ethernet segment. To test that we need to tell iperf to use at least two parallel streams; that is what <code>-P</code> flag is for.</p> <p>With the following command we start two parallel streams, 200 Kbps bitrate each, and this time for 20 seconds.</p> <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P2 -t 20\n</code></pre> <p>Our telemetry visualization makes it clear that client-side load balancing is indeed happening as both leaves receive traffic on their <code>e-1/1</code> interface.</p> <p></p> <p><code>leaf1</code> and <code>leaf2</code> both chose to use their <code>e1-49</code> interface to send the traffic to the spine layer.</p>  Load balancing in the fabric? <p>You may have noticed that when we sent two parallel streams client hashed two streams over two links in its bond interface. But then leaves used a single uplink interface towards the fabric. This is due to the fact that each leaf got a single \"stream\" and thus a single uplink interface was utilized.</p> <p>We can see ECMP in the fabric happening if we send more streams, for example, eight of them: <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 8 -t 20\n</code></pre></p> <p>That way leaves will have more streams to handle and they will load balance the streams nicely as shown in this picture.</p>","title":"Load balancing on the client side"},{"location":"tutorials/programmability/event-handler/oper-group/problem-statement/#traffic-loss-scenario","text":"<p>Now to the interesting part. What happens if one of the leaves suddenly loses all its uplinks while traffic is mid-flight? Will traffic be re-routed to healthy leaf? Will it be dropped? Let's lab it out.</p> <p>We will send 4 streams for 40 seconds long and somewhere in the middle we will execute <code>set-uplinks.sh</code> script which administratively disables uplinks on a given leaf:</p> <ol> <li>Start the traffic generators     <pre><code>docker exec -it client1 iperf3 -c 192.168.100.2 -b 200K -P 4 -t 40\n</code></pre></li> <li>Wait ~20s for graphs to form shape</li> <li>Put down both uplinks on <code>leaf1</code> <pre><code>bash set-uplinks.sh leaf1 \"{49..50}\" disable\n</code></pre></li> <li>Monitor the traffic distribution</li> </ol> <p>Here is a video demonstrating this workflow:</p>  <p>Let's see what exactly is happening there. </p> <ul> <li>[00:00 - 00:15] We started four streams 200Kbps bitrate each, summing up to 800Kbps. Those for streams were evenly distributed over the two links of a bond interface of our <code>client1</code>.     Both leaves report 400 Kbps of traffic detected on their <code>e1-1</code> interface, so each leaf handles two streams each.     Leaves then load balance these two streams over their two uplinks. We see that both <code>e1-49</code> and <code>e1-50</code> report outgoing bitrate to be ~200Kbps, which is a bitrate of a single stream we configured. That way every uplink on our leaves is utilized and handling a stream of data.</li> <li>[00:34 - 01:00] At this very moment, we execute <code>bash set-uplinks.sh leaf1 disable</code> putting uplinks on <code>leaf1</code> administratively down. The bottom left panel immediately indicates that the operational status of both uplinks went down.     But pay close attention to what is happening with traffic throughput. Traffic rate on <code>leaf1</code> access interface drops immediately, as TCP sessions of the streams it was handling stopped to receive ACKs.     At the same time, <code>leaf2</code> didn't attract any new streams, it has been handling its two streams summing up to 400Kbps all way long. This means, that traffic that was passing through <code>leaf1</code> was \"blackholed\" as <code>client1</code> was not notified in any way that one of the links in its bond interface must not be used.</li> </ul> <p>This scenario opens the stage for oper-group, as this feature provides means to make sure that a client won't use a link that is connected to a leaf that has no means to forward traffic to the fabric.</p>   <ol> <li> <p>iperf3 sends data as a single stream, until <code>-P</code> flag is set.\u00a0\u21a9</p> </li> <li> <p>when you start traffic for the first time, you might wonder why a leaf that is not used for traffic forwarding gets some traffic on its uplink interface for a brief moment as shown here. Check out this link to see why is this happening.\u00a0\u21a9</p> </li> </ol>","title":"Traffic loss scenario"},{"location":"tutorials/programmability/event-handler/oper-group/script/","text":"<p>A MicroPython script is a central piece of the framework. It allows users to create1 programmable logic to handle events and thus presents a flexible interface for adding custom functionality to the Nokia SR Linux platform.</p>","title":"Script"},{"location":"tutorials/programmability/event-handler/oper-group/script/#input","text":"<p>As explained in the KB article Event Handler expects to find and execute a specific function - <code>event_handler_main(in_json_str)</code> - which takes in a json string as its single argument. For the oper-group use case, the input JSON string will consist of the current state of the two uplinks and the provided options. For example, the following JSON is expected to be passed to a function when <code>ethernet-1/49</code> operational state goes to <code>down</code>:</p> <pre><code>{\n    \"paths\": [\n        {\n            \"path\": \"interface ethernet-1/49 oper-state\",\n            \"value\": \"down\"\n        },\n        {\n            \"path\": \"interface ethernet-1/50 oper-state\",\n            \"value\": \"up\"\n        }\n    ],\n    \"options\": {\n        \"required-up-uplinks\": \"1\",\n        \"down-links\": [\n            \"ethernet-1/1\"\n        ]\n    }\n}\n</code></pre>","title":"Input"},{"location":"tutorials/programmability/event-handler/oper-group/script/#script-walkthrough","text":"<p>Given the input JSON, let's have a look the script that implements the oper-group feature in its entirety.</p> <pre><code>import sys\nimport json\n\n# count_up_uplinks returns the number of monitored uplinks that have oper-state=up\ndef count_up_uplinks(paths):\n    up_cnt = 0\n    for path in paths:\n        if path.get(\"value\", \"down\") == \"up\":\n            up_cnt = up_cnt + 1\n    return up_cnt\n\n\n# required_up_uplinks returns the value of the `required-up-uplinks` option\ndef required_up_uplinks(options):\n    return int(options.get(\"required-up-uplinks\", 1))\n\n\n# main entry function for event handler\ndef event_handler_main(in_json_str):\n    # parse input json string passed by event handler\n    in_json = json.loads(in_json_str)\n    paths = in_json[\"paths\"]\n    options = in_json[\"options\"]\n\n    num_up_uplinks = count_up_uplinks(paths)\n    downlinks_new_state = (\n        \"down\" if num_up_uplinks &lt; required_up_uplinks(options) else \"up\"\n    )\n\n    # add `debug=\"true\"` option to event-handler configuration to output parsed parameters\n    if options.get(\"debug\") == \"true\":\n        print(\n            f\"num of required up uplinks = {required_up_uplinks(options)}\\n\\\ndetected num of up uplinks = {num_up_uplinks}\\n\\\ndownlinks new state = {downlinks_new_state}\"\n        )\n\n    response_actions = []\n\n    for downlink in options.get(\"down-links\", []):\n        response_actions.append(\n            {\n                \"set-ephemeral-path\": {\n                    \"path\": f\"interface {downlink} oper-state\",\n                    \"value\": downlinks_new_state,\n                }\n            }\n        )\n\n    response = {\"actions\": response_actions}\n    return json.dumps(response)\n</code></pre>","title":"Script walkthrough"},{"location":"tutorials/programmability/event-handler/oper-group/script/#parsing-input-json","text":"<p>Starting with the <code>event_handler_main</code> func we parse the incoming JSON string and extracting the relevant portions:</p> <p><pre><code>in_json = json.loads(in_json_str)\npaths = in_json[\"paths\"]\noptions = in_json[\"options\"]\n</code></pre> Paths and Options are the only objects in the incoming JSON, which we respectfully save in the like-named variables.</p>","title":"Parsing input JSON"},{"location":"tutorials/programmability/event-handler/oper-group/script/#evaluating-the-desired-state-of-downlinks","text":"<p>With the input parsed, we enter the central piece of the script where we make a decision on what state should the access links be in, given the inputs we received.</p> <p><pre><code>num_up_uplinks = count_up_uplinks(paths)\ndownlinks_new_state = (\n    \"down\" if num_up_uplinks &lt; required_up_uplinks(options) else \"up\"\n)\n</code></pre> First, we count the number of uplinks in oper-state up, this is done with <code>count_up_uplinks()</code> function which simply walks through the current state of the uplinks passed into the script by the Event Handler.</p> <pre><code># count_up_uplinks returns the number of monitored uplinks that have oper-state=up\ndef count_up_uplinks(paths):\n    up_cnt = 0\n    for path in paths:\n        if path.get(\"value\", \"down\") == \"up\":\n            up_cnt = up_cnt + 1\n    return up_cnt\n</code></pre> <p>When we calculated how many uplinks are operationally up, we can decide what state should the downlinks be in. To rule that decision we compare the number of operational uplinks with the required number of uplinks passed via options:</p> <pre><code>downlinks_new_state = (\n    \"down\" if num_up_uplinks &lt; required_up_uplinks(options) else \"up\"\n)\n</code></pre> <p>If the required number of operational uplinks is less than the required number of them, we should put down downlinks to prevent traffic blackholing. On the other hand, if the number of operational uplinks is &gt;= the required number of uplinks, we should bring the access links up.</p> <p>The desired state of the downlinks is saved in <code>downlinks_new_state</code> variable.</p>","title":"Evaluating the desired state of downlinks"},{"location":"tutorials/programmability/event-handler/oper-group/script/#debugging","text":"<p>It is useful to take a pause here and embed some debugging log outputs for the key variables of a script. In our case, we've added a print statement that dumps important variables of our script.</p> <pre><code># add `debug=\"true\"` option to event-handler configuration to output parsed parameters\nif options.get(\"debug\") == \"true\":\n    print(\n        f\"num of required up uplinks = {required_up_uplinks(options)}\\n\\\ndetected num of up uplinks = {num_up_uplinks}\\n\\\ndownlinks new state = {downlinks_new_state}\"\n    )\n</code></pre> <p>The debug log will only be present if the <code>debug</code> option will be set to <code>\"true\"</code> in the Event Handler instance config. You will be able to find this log output by using this CLI command:</p> <pre><code>info from state /system event-handler instance opergroup last-stdout-stderr\n</code></pre>","title":"Debugging"},{"location":"tutorials/programmability/event-handler/oper-group/script/#composing-output","text":"<p>At this point, our script is able to define the desired state of the downlinks, based on the state of the user-defined uplinks and the required number of healthy uplinks. For the Event Handler to take any action, the script needs to output a JSON string following the expected format.</p> <pre><code>response_actions = []\n\nfor downlink in options.get(\"down-links\", []):\n    response_actions.append(\n        {\n            \"set-ephemeral-path\": {\n                \"path\": f\"interface {downlink} oper-state\",\n                \"value\": downlinks_new_state,\n            }\n        }\n    )\n\nresponse = {\"actions\": response_actions}\nreturn json.dumps(response)\n</code></pre> <p>This code snippet shows the way to create an output JSON, using the calculated <code>downlinks_new_state</code> and the list of downlinks provided via <code>down-links</code> option. We range over the down-links option to append a structure that Event Handler expects to see in output JSON and using <code>set-ephemeral-path</code> action that will set oper state of the downlinks to the desired value (up or down).</p> <p>The output is provided via <code>response</code> dictionary, that we marshal to JSON encoding at the end before returning from the function. This routine will provide a JSON back to the Event Handler and since it is formed in a well-known way, Event Handler will process and execute the actions passed to it.</p> <p>Consequently, by receiving back a list of actions from the script, Event Handler will implement the oper-group feature when a state of a group of downlinks is derived from the state of a group of uplinks.</p>","title":"Composing output"},{"location":"tutorials/programmability/event-handler/oper-group/script/#summary","text":"<p>Let's take a few input examples and see which outputs will be generated by the script to better understand the logic of the automation.</p> <p>We start in a healthy state with both uplinks in operation and oper-group event handler configured as per the previous steps.</p> <p>In the event of a single uplink interface going operationally down:</p> Input JSONCalculated parametersOutput JSON   <pre><code>{\n    \"paths\": [\n        {\n            \"path\": \"interface ethernet-1/49 oper-state\",\n            \"value\": \"down\"\n        },\n        {\n            \"path\": \"interface ethernet-1/50 oper-state\",\n            \"value\": \"up\"\n        }\n    ],\n    \"options\": {\n        \"required-up-uplinks\": \"1\",\n        \"down-links\": [\n            \"ethernet-1/1\"\n        ]\n    }\n}\n</code></pre>   <ul> <li>Number of required uplinks doesn't change as it is an option provided as user input. It is always <code>1</code> in our case.</li> <li>Detected number of uplinks in operational state equals <code>1</code>, as we range through the <code>paths</code> in the incoming JSON and count paths which have <code>up</code> value for the <code>interface ethernet-* oper-state</code> leaf.</li> <li>Downlinks' new state should be <code>\"up\"</code>, since we still have a minimum number of operational uplinks = <code>1</code>.</li> </ul>   <pre><code>{\n    \"actions\": [\n        {\n            \"set-ephemeral-path\": {\n                \"path\": \"/interface ethernet-1/1 oper-state\",\n                \"value\": \"up\",\n            }\n        }\n    ]\n}\n</code></pre>    <p>Then let's see what happens if the second uplink goes down.</p> Input JSONCalculated parametersOutput JSON   <pre><code>{\n    \"paths\": [\n        {\n            \"path\": \"interface ethernet-1/49 oper-state\",\n            \"value\": \"down\"\n        },\n        {\n            \"path\": \"interface ethernet-1/50 oper-state\",\n            \"value\": \"down\"\n        }\n    ],\n    \"options\": {\n        \"required-up-uplinks\": \"1\",\n        \"down-links\": [\n            \"ethernet-1/1\"\n        ]\n    }\n}\n</code></pre>   <ul> <li>Number of required uplinks doesn't change as it is an option provided as user input. It is always <code>1</code> in our case.</li> <li>Detected number of uplinks in operational state equals <code>0</code>, as we range through the <code>paths</code> in the incoming JSON and count paths which have <code>up</code> value for the <code>interface ethernet-* oper-state</code> leaf.</li> <li>Downlinks' new state should be <code>\"down\"</code>, since the number of operational uplinks (<code>0</code>) is less than the required number of operational uplinks.</li> </ul>   <pre><code>{\n    \"actions\": [\n        {\n            \"set-ephemeral-path\": {\n                \"path\": \"/interface ethernet-1/1 oper-state\",\n                \"value\": \"down\",\n            }\n        }\n    ]\n}\n</code></pre>","title":"Summary"},{"location":"tutorials/programmability/event-handler/oper-group/script/#off-box-testing","text":"<p>Although it is absolutely possible to test Event Handler scripts using containerized SR Linux image, it makes a lot of sense to test the script off-box.</p> <p>Since scripts are provided with a known input JSON structure, we can pass it to a script's <code>main()</code> function as if it was provided by the Event Manager itself. Consider the following code snippet that is part of the opergroup.py script we just walked through:</p> <pre><code>def main():\n    example_in_json_str = \"\"\"\n{\n    \"paths\": [\n        {\n            \"path\":\"interface ethernet-1/49 oper-status\",\n            \"value\":\"down\"\n        },\n        {\n            \"path\":\"interface ethernet-1/50 oper-status\",\n            \"value\":\"down\"\n        }\n    ],\n    \"options\": {\n        \"required-up-uplinks\":1,\n        \"down-links\": [\n            \"Ethernet-1/1\",\n            \"Ethernet-1/2\"\n        ],\n        \"debug\": \"true\"\n    }\n}\n\"\"\"\n    json_response = event_handler_main(example_in_json_str)\n    print(f\"Response JSON:\\n{json_response}\")\n\n\nif __name__ == \"__main__\":\n    sys.exit(main())\n</code></pre> <p>Since Event Handler's entrypoint is <code>event_handler_main()</code> func, we can create a <code>main()</code> function that will a variable with a JSON-encoded string that follows the schema of the input argument. In essence, we are mocking the Event Handler and provide a hand-crafted input JSON to the <code>event_handler_main()</code> function.</p> <p>Now, we can test our script on any system that has Python/MicroPython installed, for example:</p> Testing with PythonTesting with MicroPyton   <p>Given your script doesn't use any non supported by MicroPython libraries, you may use Python3 installed on any system to test your script. For example: <pre><code>\u276f python3 opergroup.py\nnum of required up uplinks = 1\ndetected num of up uplinks = 0\ndownlinks new state = down\nResponse JSON:\n{\"actions\": [{\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/1 oper-state\", \"value\": \"down\"}}, {\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/2 oper-state\", \"value\": \"down\"}}]}\n</code></pre></p>   <p>Testing with MicroPython is advised, as this will guarantee that the code will work on SR Linux. Feel free to install Unix port of MicroPython or leverage <code>srl-labs/upy:1.18</code> container image:</p> <p><pre><code>docker run -it  -v $(pwd):/workdir ghcr.io/srl-labs/upy:1.18 micropython opergroup.py\nnum of required up uplinks = 1\ndetected num of up uplinks = 0\ndownlinks new state = down\nResponse JSON:\n{\"actions\": [{\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/1 oper-state\", \"value\": \"down\"}}, {\"set-ephemeral-path\": {\"path\": \"interface Ethernet-1/2 oper-state\", \"value\": \"down\"}}]}\n</code></pre> To pretty print the output, use <code>jq</code>: <pre><code>docker exec -it dc6ded4ed7ff bash -c \"micropython opergroup.py | tail -1 | jq .\"\n</code></pre> <pre><code>{\n    \"actions\": [\n        {\n            \"set-ephemeral-path\": {\n                \"path\": \"interface Ethernet-1/1 oper-state\",\n                \"value\": \"down\"\n            }\n        },\n        {\n            \"set-ephemeral-path\": {\n                \"path\": \"interface Ethernet-1/2 oper-state\",\n                \"value\": \"down\"\n            }\n        }\n    ]\n}\n</code></pre></p>","title":"Off-box testing"},{"location":"tutorials/programmability/event-handler/oper-group/script/#script-delivery","text":"<p>Scripts created by users must be delivered to the SR Linux nodes and available by the well-known location. Any file transfer technique can be used to deliver the source files/packages.</p> <p>When using containerlab, users may take advantage of the <code>binds</code> option of a node and bind mount the script to its location. This is exactly how we do it in the opergroup-lab:</p> <pre><code>name: opergroup\n\ntopology:\n  nodes:\n    leaf1:\n      binds:\n        - opergroup.py:/etc/opt/srlinux/eventmgr/opergroup.py\n</code></pre>   <ol> <li> <p>Check this article for ways of writing code for MicroPython\u00a0\u21a9</p> </li> </ol>","title":"Script delivery"},{"location":"tutorials/programmability/event-handler/oper-group/summary/","text":"<p>Event-driven automation is a popular paradigm in the networks field. One practical implementation of that paradigm is Nokia SR Linux Event Handler framework that allows users to programmatically react to events happening in a network OS.</p> <p>This tutorial covers Event Handler concepts by explaining how they can be used to implement Operational Group feature.</p> <p>Theoretical data is backed by a containerlab-based lab that we exclusively use throughout the tutorial. Readers can therefore repeat every step in their own time.</p> <p>Before explaining how to configure an event handler-based oper-group instance, we first explain what problem oper-group is set to fix.</p> <p>Once the problem statement is set, we proceed with configuration steps for the event handler instance.</p> <p>A key piece of the Event Handler framework is the script that is getting executed every time an event to which users subscribed happens. In the Script chapter we explain how oper-group script is composed.</p> <p>Finally, it is time to see how the Event Handler instance powered by the oper-group script works. We follow through with the various scenarios and capture the behavior of the fabric.</p>","title":"Summary"},{"location":"yang/browser/","text":"<p>YANG data models are the map one should use when looking for their way to configure or retrieve any data on SR Linux system. A central role that is given to YANG in SR Linux demands a convenient interface to browse, search through, and process these data models.</p> <p>To answer these demands, we created a web portal - yang.srlinux.dev - it offers:</p> <ul> <li>Fast Path Browser to effectively search through thousands of available YANG paths</li> <li>Beautiful Tree Browser to navigate the tree representation of the entire YANG data model of SR Linux</li> <li>Source <code>.yang</code> files neatly stored in <code>nokia/srlinux-yang-models</code> repository for programmatic access and code generation</li> </ul>  <p></p>  <p>The web portal's front page aggregates links to individual releases of YANG models. Select the needed version to open the web view of the YANG tools we offer.</p>  <p></p>  <p>The main stage of the YANG Browser view is dedicated to the Path Browser , as it is the most efficient way to search through the model. Additional tools are located in the upper right corner . Let's cover them one by one.</p>","title":"SR Linux YANG Browser"},{"location":"yang/browser/#path-browser","text":"<p>As was discussed before, SR Linux is a fully modeled system with its configuration and state data entirely covered with YANG models. Consequently, to access any data for configuration or state, one needs to follow the YANG model. Effectively searching for those YANG-based access paths is key to rapid development and operations. For example, how to tell which one to use to get ipv4 statistics of an interface?</p> <p>With Path Browser, it is possible to search through the entire SR Linux YANG model and extract the paths to the leaves of interest. The Path Browser area is composed of three main elements:</p> <ul> <li>search input for entering the query </li> <li>Config/State selector </li> <li>table with results for a given search input </li> </ul>  <p> </p> Path Browser elements  <p>A user types in a search query, and the result is rendered immediately in the table with the matched words highlighted. The Config/State selector allows users to select if they want the table to show config, state, or all leaves. The state leaf is a leaf that has <code>config false</code> statement2.</p>","title":"Path Browser"},{"location":"yang/browser/#path-structure","text":"<p>The table contains the flattened XPATH-like paths3 for every leaf of a model sorted alphabetically.</p> <ul> <li>Each path is denoted with a State attribute in the first column of a table. Leaves, which represent the state data, will have the <code>true</code> value in the first column2.</li> <li>List elements are represented in the paths as <code>list-element[key-name=*]</code> - a format suitable for gNMI subscriptions.</li> <li>Each leaf is provided with the type information.</li> </ul>","title":"Path structure"},{"location":"yang/browser/#search-capabilities","text":"<p>Snappy search features of the Path Browser make it a joy to use when exploring the model or looking for a specific leaf of interest.</p> <p>Let's imagine we need to solve the simple task of subscribing to interface traffic statistics. How would we know which gNMI path corresponds to the traffic statistics counters? Should we try reading source YANG files? But it is challenging as models have lots of imports and quite some augmentations. A few moments and - you're lost. What about the tree representation of a model generated with <code>pyang</code>? Searching through something like pyang's tree output is impractical since searching the tree representation can't include more than one search parameter. The search becomes a burden on operators' eyes.</p> <p>Path Browser to the rescue. Its ability to return search requests instantaneously makes interrogating the model a walk in the park. The animation below demos a leaf-searching exercise where a user searches for a state leaf responsible for traffic statistics.  </p> <p>First, a user tries a logical search query <code>interface byte</code>, which yields some results, but it is easy to spot that they are not related to the task at hand. Thanks to the embedded highlighting capabilities, the search inputs are detectable in the resulting paths.</p> <p>Next, they try to use <code>interface octets</code> search query hoping that it will yield the right results, and so it does!</p>   <p>Tip</p> <p>Every table row denotes a leaf, and when a user hovers a mouse over a row, the popup appears with a description of the leaf.</p>","title":"Search capabilities"},{"location":"yang/browser/#tree-browser","text":"<p>The Path Browser is great to search through the entire model, but because it works on flattened paths, it hides the \"tree\" view of the model. Sometimes the tree representation is the best side to look at the models with a naked eye, as the hierarchy becomes very clear.</p> <p>To not strip our users of the beloved tree view mode, we enhanced the <code>pyang -f jstree</code> output and named this view Tree Browser.</p>  <p> </p> Access Tree Browser  <p>The tree view of the model offers a step-by-step exploration of the SR Linux model going from the top-level modules all the way down to the leaves. The tree view displays the node's type (leaf/container/etc) as well as the leaf type and the read-only status of a leaf.</p>  <p> </p> Tree Browser view   <p>Tip</p> <p>Every element of a tree has a description that becomes visible if you hover over the element with a mouse. </p>","title":"Tree Browser"},{"location":"yang/browser/#tree-and-paths","text":"<p>If you feel like everything in the world better be in ASCII, then Tree and Paths menu elements will satisfy the urge. These are the ASCII tree of the SR Linux model1 and the text flattened paths that are used in the Path Browser.</p>  <p> </p> Text version of tree and paths  <p>The textual paths can be, for example, fetched with curl and users can <code>sed</code> themselves out doing comprehensive searches or path manipulations.</p>   <ol> <li> <p>extracted with <code>pyang -f tree</code> \u21a9</p> </li> <li> <p>refer to https://datatracker.ietf.org/doc/html/rfc6020#section-4.2.3 \u21a9\u21a9</p> </li> <li> <p>paths are generated from the YANG model with gnmic \u21a9</p> </li> </ol>","title":"Tree and Paths"},{"location":"yang/yang/","text":"<p>Model-driven (MD) interfaces are becoming essential for robust and modern Network OSes. The changes required to create fully model-driven interfaces can not happen overnight - it is a long and tedious process that requires substantial R&amp;D effort. Traditional Network OSes often had to take an evolutionary route with adding MD interfaces on top of the existing internal infrastructure.</p>  <p> </p> SR Linux ground-up support for YANG  <p>Unfortunately, bolting on model-driven interfaces while keeping the legacy internal infrastructure layer couldn't fully deliver on the promises of MD interfaces. In reality, those new interfaces had visibility discrepancies1, which often led to a situation where users needed to mix and match different interfaces to achieve some configuration goal. Apparently, without adopting a fully modeled universal API, it is impossible to make a uniform set of interfaces offering the same visibility level into the NOS.</p> <p>Nokia SR Linux was ground-up designed with YANG2 data modeling taking a central role. SR Linux makes extensive use of structured data models with each application regardless if it's being provided by Nokia or written by a user has a YANG model that defines its configuration and state.</p>  <p> </p> Both Nokia and customer's apps are modeled in YANG  <p>SR Linux exposes the YANG models to the supported management APIs. For example, the command tree in the CLI is derived from the SR Linux YANG models loaded into the system, and a gNMI client uses RPCs to configure an application based on its YANG model. When a configuration is committed, the SR Linux management server validates the YANG models and translates them into protocol buffers for the impart database (IDB).</p> <p>With this design, there is no way around YANG; the data model is defined first for any application SR Linux has, then the CLI, APIs, and show output formats derived from it.</p>","title":"SR Linux & YANG"},{"location":"yang/yang/#sr-linux-yang-models","text":"<p>As YANG models play a central role in SR Linux NOS, it is critical to have unobstructed access. With that in mind, we offer SR Linux users many ways to get ahold of SR Linux YANG models:</p> <ol> <li>Download modules from SR Linux NOS itself.     The models can be found at <code>/opt/srlinux/models/*</code> location.</li> <li>Fetch modules from <code>nokia/srlinux-yang-models</code> repo.</li> <li>Use SR Linux YANG Browser to consume modules in a human-friendly way</li> </ol> <p>SR Linux employs a uniform mapping between a YANG module name and the CLI context, making it easy to correlate modules with CLI contexts.</p>   YANG modules and CLI aligned  <p>The structure of the Nokia SR Linux native models may look familiar to the OpenConfig standard, where different high-level domains are contained in their modules.</p> <p>Source <code>.yang</code> files are great for YANG-based automation tools such as ygot but are not so easy for a human's eye. For living creatures, we offer a YANG Browser portal. We suggest people use it when they want to consume the models in a non-programmable way.</p>   <ol> <li> <p>indicated by the blue color on the diagram and explained in detail in NFD25 talk.\u00a0\u21a9</p> </li> <li> <p>RFC 6020 and RFC 7950 \u21a9</p> </li> </ol>","title":"SR Linux YANG Models"}]}